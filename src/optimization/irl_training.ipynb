{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e0fd24",
   "metadata": {},
   "source": [
    "## Reward Model Training Example Given Policy Model has Already been Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a8224cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "# get the current script's directory\n",
    "current_directory = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in locals() else os.getcwd()\n",
    "# get the parent directory\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "# add the parent directory to the sys.path\n",
    "sys.path.append(parent_directory)\n",
    "\n",
    "from utils import constants\n",
    "from utils.dataset_loader import PolicyDatasetLoader\n",
    "\n",
    "from optimization.updater import Updater\n",
    "from optimization.functions import setup_config, get_directories, load_policy\n",
    "from optimization.functions import find_indices_of_trajectory_changes, get_estimated_rewards\n",
    "\n",
    "from models.policy_model import RobotPolicy\n",
    "from models.reward_model import RewardFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba05b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",\n",
    "              None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae49fa9e",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "250108f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Device:  cpu\n",
      "Current Time:  Feb_06_2024-10_59_08\n"
     ]
    }
   ],
   "source": [
    "# available evaluating machine\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Evaluating Device: \", device)\n",
    "\n",
    "# setup hyperparameters\n",
    "configs = setup_config(device=device)\n",
    "\n",
    "# create and return preliminary base paths\n",
    "json_paths, results_path = get_directories(parent_directory=parent_directory,\n",
    "                                           data_folder_name=constants.DEMO_COLLECTION_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a53b910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================== Policy Dataset Loader ==================\n",
      "\n",
      "Number of Trajectories:  43\n",
      "Each Trajectory Length:  30\n",
      "Full Demo Dataset Size:  1379\n"
     ]
    }
   ],
   "source": [
    "# load train-validation dataset of demonstrations\n",
    "all_data = PolicyDatasetLoader(demo_data_json_paths=json_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d3318d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_network = RobotPolicy(state_size=configs.state_size,\n",
    "                             hidden_size=configs.hidden_size,\n",
    "                             out_size=configs.action_size,\n",
    "                             log_std_min=configs.policy_log_std_min,\n",
    "                             log_std_max=configs.policy_log_std_max,\n",
    "                             log_std_init=configs.policy_log_std_init,\n",
    "                             device=configs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f04f5710",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_network = RewardFunction(state_action_size=configs.state_action_size,\n",
    "                                hidden_size=configs.hidden_size,\n",
    "                                out_size=configs.reward_size,\n",
    "                                device=configs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15029e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "updater = Updater(configs=configs,\n",
    "                  policy_network=policy_network,\n",
    "                  reward_network=reward_network)\n",
    "updater.initialize_optimizers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ebe979",
   "metadata": {},
   "source": [
    "# Load Policy Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4a418a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder name where parameters are located (\"results / policy_network_params / loading_folder_name\")\n",
    "policy_loading_folder_name = \"Feb_05_2024-16_45_05\"\n",
    "policy_params_name = \"policy_network_epoch_100_loss_0_30367.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5f131e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the trained model parameters (make sure that the folder exists where model is trained priorly)\n",
    "policy_model_folder_path = os.path.join(results_path,\n",
    "                                        \"policy_network_params\",\n",
    "                                        policy_loading_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7479c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model_path = os.path.join(policy_model_folder_path,\n",
    "                                 policy_params_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd0fbf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set trained parameters to neural network\n",
    "policy_network = load_policy(policy_network=policy_network,\n",
    "                             model_path=policy_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74bdc37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model to evaluation mode\n",
    "for param in policy_network.parameters():\n",
    "    param.requires_grad = False\n",
    "policy_network = policy_network.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fbec74",
   "metadata": {},
   "source": [
    "# Training Reward Model with Each Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4545e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all indice numbers where the new trajectory is initialized in the dataset\n",
    "trajectory_indices = find_indices_of_trajectory_changes(dataset=all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64d8c707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index location where the each trajectory starts in the dataframe\n",
    "traj_start_index = 0\n",
    "\n",
    "traj_df, reward_values_demo_data, reward_values_estim_data, logprob_action_estim_avg = get_estimated_rewards(configs=configs,\n",
    "                                                                                                             updater_obj=updater,\n",
    "                                                                                                             data_loader=all_data,\n",
    "                                                                                                             policy_network=policy_network,\n",
    "                                                                                                             reward_network=reward_network,\n",
    "                                                                                                             trajectory_indices=trajectory_indices,\n",
    "                                                                                                             traj_start_index=traj_start_index,\n",
    "                                                                                                             is_inference_reward=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94779803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: currently nu weight is zero; will be updated later\n",
    "nu_factor = torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f586c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate irl loss function value of the particilar trajectories\n",
    "irl_train_loss = updater.calculate_irl_loss(demo_traj_reward=reward_values_demo_data,\n",
    "                                            robot_traj_reward=reward_values_estim_data,\n",
    "                                            log_probability=logprob_action_estim_avg,\n",
    "                                            nu_factor=nu_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6243c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass and optimization\n",
    "updater.run_reward_optimizer(irl_loss=irl_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "423270f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3826]], dtype=torch.float64, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irl_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e73ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
