{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e0fd24",
   "metadata": {},
   "source": [
    "## Reward Model Training Example Given Policy Model has Already been Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a8224cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "# get the current script's directory\n",
    "current_directory = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in locals() else os.getcwd()\n",
    "# get the parent directory\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "# add the parent directory to the sys.path\n",
    "sys.path.append(parent_directory)\n",
    "\n",
    "from utils import constants\n",
    "from utils.dataset_loader import PolicyDatasetLoader\n",
    "\n",
    "from optimization.updater import Updater\n",
    "from optimization.functions import setup_config, get_directories, load_policy, trajectory_generation\n",
    "from optimization.functions import find_indices_of_trajectory_changes, get_estimated_rewards\n",
    "\n",
    "from models.policy_model import RobotPolicy\n",
    "from models.reward_model import RewardFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba05b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",\n",
    "              None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae49fa9e",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "250108f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Device:  cpu\n",
      "Current Time:  Feb_06_2024-20_56_12\n"
     ]
    }
   ],
   "source": [
    "# available evaluating machine\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Evaluating Device: \", device)\n",
    "\n",
    "# setup hyperparameters\n",
    "configs = setup_config(device=device)\n",
    "\n",
    "# create and return preliminary base paths\n",
    "json_paths, results_path = get_directories(parent_directory=parent_directory,\n",
    "                                           data_folder_name=constants.DEMO_COLLECTION_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a53b910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================== Policy Dataset Loader ==================\n",
      "\n",
      "Number of Trajectories:  43\n",
      "Each Trajectory Length:  30\n",
      "Full Demo Dataset Size:  1379\n"
     ]
    }
   ],
   "source": [
    "# load train-validation dataset of demonstrations\n",
    "all_data = PolicyDatasetLoader(demo_data_json_paths=json_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d3318d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_network = RobotPolicy(state_size=configs.state_size,\n",
    "                             hidden_size=configs.hidden_size,\n",
    "                             out_size=configs.action_size,\n",
    "                             log_std_min=configs.policy_log_std_min,\n",
    "                             log_std_max=configs.policy_log_std_max,\n",
    "                             log_std_init=configs.policy_log_std_init,\n",
    "                             device=configs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f04f5710",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_network = RewardFunction(state_action_size=configs.state_action_size,\n",
    "                                hidden_size=configs.hidden_size,\n",
    "                                out_size=configs.reward_size,\n",
    "                                device=configs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15029e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "updater_obj = Updater(configs=configs,\n",
    "                      policy_network=policy_network,\n",
    "                      reward_network=reward_network)\n",
    "updater_obj.initialize_optimizers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ebe979",
   "metadata": {},
   "source": [
    "# Load Policy Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4a418a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder name where parameters are located (\"results / policy_network_params / loading_folder_name\")\n",
    "policy_loading_folder_name = \"Feb_05_2024-16_45_05\"\n",
    "policy_params_name = \"policy_network_epoch_100_loss_0_30367.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5f131e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the trained model parameters (make sure that the folder exists where model is trained priorly)\n",
    "policy_model_folder_path = os.path.join(results_path,\n",
    "                                        \"policy_network_params\",\n",
    "                                        policy_loading_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7479c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model_path = os.path.join(policy_model_folder_path,\n",
    "                                 policy_params_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd0fbf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set trained parameters to neural network\n",
    "policy_network = load_policy(policy_network=policy_network,\n",
    "                             model_path=policy_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74bdc37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model to evaluation mode\n",
    "for param in policy_network.parameters():\n",
    "    param.requires_grad = False\n",
    "policy_network = policy_network.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fbec74",
   "metadata": {},
   "source": [
    "# Training Reward Model with Each Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4545e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all indice numbers where the new trajectory is initialized in the dataset\n",
    "trajectory_indices = find_indices_of_trajectory_changes(dataset=all_data)\n",
    "\n",
    "# number of human demonstrations (check GCL formula)\n",
    "N = len(trajectory_indices)\n",
    "M = N\n",
    "\n",
    "# TODO: currently nu weight is zero; will be updated later\n",
    "nu_factor = torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64d8c707",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_reward, samp_reward = 0.0, 0.0\n",
    "\n",
    "# loop through each separate trajectory inside the dataset\n",
    "for traj_start_index in range(N):\n",
    "    \n",
    "    # return demonstrator trajectories and expert reward predictions under currently training reward model\n",
    "    traj_df, reward_values_demo_data, reward_values_estim_data, logprob_action_estim_avg = get_estimated_rewards(configs=configs,\n",
    "                                                                                                                 updater_obj=updater_obj,\n",
    "                                                                                                                 data_loader=all_data,\n",
    "                                                                                                                 policy_network=policy_network,\n",
    "                                                                                                                 reward_network=reward_network,\n",
    "                                                                                                                 trajectory_indices=trajectory_indices,\n",
    "                                                                                                                 traj_start_index=traj_start_index,\n",
    "                                                                                                                 is_inference_reward=False)\n",
    "    \n",
    "    # first denormalized state vector and initial end-effector position of corresponding state vector\n",
    "    average_initial_state_denorm = traj_df[[\n",
    "        constants.STATE_DENORMALIZED_LABEL_NAME + f\"_{i+1}\" for i in range(len(all_data.state_norms))]].values[0]\n",
    "    initial_state_location = traj_df[[\n",
    "        constants.ACTION_DENORMALIZED_LABEL_NAME + f\"_{i+1}\" for i in range(len(all_data.action_norms))]].values[0]\n",
    "    \n",
    "    # trajectory generation given the learned policy within initial state randomness\n",
    "    sample_traj_df, sample_action_logprobs, sample_reward_values = trajectory_generation(configs=configs,\n",
    "                                                                                         state_norms=all_data.state_norms,\n",
    "                                                                                         action_norms=all_data.action_norms,\n",
    "                                                                                         policy_network=policy_network,\n",
    "                                                                                         reward_network=reward_network,                                              \n",
    "                                                                                         average_initial_state_denorm=average_initial_state_denorm,\n",
    "                                                                                         initial_state_location=initial_state_location)\n",
    "    \n",
    "    demo_reward += torch.mean(reward_values_demo_data)\n",
    "    samp_reward += updater_obj.calculate_sample_traj_loss(nu_factor=nu_factor,\n",
    "                                                          robot_traj_reward=sample_reward_values,\n",
    "                                                          log_probability=sample_action_logprobs)\n",
    "\n",
    "avg_demo_reward = demo_reward / N\n",
    "avg_samp_reward = samp_reward / M\n",
    "\n",
    "IRL_loss = -avg_demo_reward + avg_samp_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "423270f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "updater_obj.run_reward_optimizer(irl_loss=IRL_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e73ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
