{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74bdfaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get the current script's directory\n",
    "current_directory = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in locals() else os.getcwd()\n",
    "# get the parent directory\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "# add the parent directory to the sys.path\n",
    "sys.path.append(parent_directory)\n",
    "\n",
    "from optimization import functions\n",
    "from optimization.updater import Updater\n",
    "\n",
    "from utils import constants, common\n",
    "from utils.config import Config\n",
    "from utils.dataset_loader import PolicyDatasetLoader\n",
    "\n",
    "from models.policy_model import RobotPolicy\n",
    "from models.reward_model import RewardFunction\n",
    "\n",
    "from environment.environment import RobotEnvironment\n",
    "from environment.buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c33833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_loss_reward, mean_loss_policy = [], []\n",
    "\n",
    "EPISODES_TO_PLAY = 10\n",
    "REWARD_FUNCTION_UPDATE = 5\n",
    "DEMO_BATCH = 2 # 256\n",
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc005d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "current_path = os.getcwd()\n",
    "parent_path = os.path.dirname(current_path)\n",
    "grand_parent_path = os.path.dirname(parent_path)\n",
    "\n",
    "results_path = os.path.join(grand_parent_path, \"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b5721d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time:  Feb_15_2024-18_57_14\n",
      "Training Device:  cpu\n",
      "Current Time:  Feb_15_2024-18_57_14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2a1b4a80810>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = Config()\n",
    "# call the parameters method to set the parameters\n",
    "configs.parameters()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Training Device: \", device)\n",
    "configs.device = device\n",
    "\n",
    "configs = functions.setup_config(device=device)\n",
    "\n",
    "random.seed(configs.seed)\n",
    "np.random.seed(configs.seed)\n",
    "torch.manual_seed(configs.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a65313e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================== Policy Dataset Loader ==================\n",
      "\n",
      "Number of Trajectories:  43\n",
      "Each Trajectory Length:  20\n",
      "Full Demo Dataset Size:  879\n"
     ]
    }
   ],
   "source": [
    "policy_saving_path, reward_saving_path = functions.create_directories(configs=configs,\n",
    "                                                                      results_path=results_path,\n",
    "                                                                      saving_policy=False,\n",
    "                                                                      saving_reward=False)\n",
    "\n",
    "json_paths_train, results_path = functions.get_directories(parent_directory=parent_directory,\n",
    "                                                           data_folder_name=constants.DEMO_COLLECTION_DATE) # DEMO_COLLECTION_DATE, TEST_COLLECTION_DATE\n",
    "\n",
    "training_data = PolicyDatasetLoader(demo_data_json_paths=json_paths_train)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(training_data,\n",
    "                                           batch_size=configs.batch_size,\n",
    "                                           shuffle=configs.data_shuffle,\n",
    "                                           num_workers=configs.num_workers)\n",
    "\n",
    "trajectory_indices = functions.find_indices_of_trajectory_changes(dataset=training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "764ff18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_network = RobotPolicy(state_size=configs.state_size,\n",
    "                             hidden_size=configs.hidden_size,\n",
    "                             out_size=configs.action_size,\n",
    "                             log_std_min=configs.policy_log_std_min,\n",
    "                             log_std_max=configs.policy_log_std_max,\n",
    "                             log_std_init=configs.policy_log_std_init,\n",
    "                             device=configs.device)\n",
    "reward_network = RewardFunction(state_action_size=configs.state_action_size,\n",
    "                                hidden_size=configs.hidden_size,\n",
    "                                out_size=configs.reward_size,\n",
    "                                device=configs.device)\n",
    "\n",
    "updater_obj = Updater(configs=configs,\n",
    "                      policy_network=policy_network,\n",
    "                      reward_network=reward_network)\n",
    "updater_obj.initialize_optimizers()\n",
    "\n",
    "env = RobotEnvironment()\n",
    "env.set_reward_network(updater_obj.reward_network)\n",
    "\n",
    "env.is_reward_inference = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91434aec",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eff57f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, gamma=0.99):\n",
    "    G = torch.zeros_like(rewards, dtype=torch.float64)\n",
    "\n",
    "    G[-1] = rewards[-1].clone()\n",
    "\n",
    "    for idx in range(len(rewards) - 2, -1, -1):\n",
    "        G[idx] = rewards[idx] + gamma * G[idx + 1]\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00ab7d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_gradient_loss(cumulative_log_probs,\n",
    "                         advantages,\n",
    "                         entropy_weight=1e-2):\n",
    "    \n",
    "    # negative log-likelihood multiplied by rewards)\n",
    "#     weighted_log_probs = torch.sum(cumulative_log_probs, dim=1) * advantages.squeeze()\n",
    "    weighted_log_probs = cumulative_log_probs * advantages\n",
    "    policy_loss = - torch.mean(weighted_log_probs, dim=0).mean()\n",
    "    \n",
    "#     print(\"shape 1: \", torch.sum(cumulative_log_probs, dim=1).shape)\n",
    "#     print(\"shape 2: \", advantages.shape)\n",
    "    \n",
    "#     print(\"sum cumulative_log_probs : \", torch.sum(cumulative_log_probs, dim=1))\n",
    "#     print(\"advantages : \", advantages)\n",
    "#     print(\"summed cumulative_log_probs : \", torch.sum(cumulative_log_probs, dim=1) * advantages.squeeze())\n",
    "    \n",
    "#     print(\"weighted_log_probs\")\n",
    "#     print(weighted_log_probs, weighted_log_probs.shape)\n",
    "#     print(\"torch.mean(weighted_log_probs) : \", torch.mean(weighted_log_probs))\n",
    "    \n",
    "    # entropy regularization\n",
    "    entropy = - torch.sum(torch.exp(cumulative_log_probs) * cumulative_log_probs, dim=1).mean()\n",
    "\n",
    "#     print(\"cumulative_log_probs : \", cumulative_log_probs)\n",
    "    loss = policy_loss + entropy_weight * entropy\n",
    "    \n",
    "    print(\"cumulative_log_probs : \",cumulative_log_probs)\n",
    "    print(\"policy_loss : \", policy_loss)\n",
    "    print(\"entropy: \", entropy)\n",
    "    print(\"total loss : \", loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8216fa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max,\n",
    "                     agent,\n",
    "                     is_policy_inference): \n",
    "    \n",
    "    states, traj_log_probs, actions, rewards = [], [], [], []\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        action, action_log_prob = agent.get_action(state)\n",
    "        \n",
    "#         print(\"state : \", state)\n",
    "#         print(\" action : \", action)\n",
    "#         print(\" action : \", action.squeeze(0))\n",
    "#         print(\" action_log_prob : \", action_log_prob.squeeze(0))\n",
    "        \n",
    "        next_state, reward, done = env.step(state=state,\n",
    "                                            action=torch.tensor(action.squeeze(0)))\n",
    "        \n",
    "        states.append(state.clone())\n",
    "        actions.append(torch.tensor(action.squeeze(0)).clone())\n",
    "        traj_log_probs.append(action_log_prob.squeeze(0).clone())\n",
    "        rewards.append(reward.clone())\n",
    "        \n",
    "        state = next_state.detach()\n",
    "        \n",
    "        if done:  \n",
    "            break\n",
    "    \n",
    "    return states, actions, traj_log_probs, rewards, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ded75a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_traj(traj_list_,\n",
    "                    step_tensor,\n",
    "                    is_Demo=False):\n",
    "    for traj_df in traj_list_:\n",
    "        \n",
    "        if is_Demo:\n",
    "            states = torch.tensor(traj_df[[\"state_label_norm_1\", \"state_label_norm_2\", \"state_label_norm_3\"]].values)\n",
    "            actions = torch.tensor(traj_df[[\"action_label_norm_1\", \"action_label_norm_2\", \"action_label_norm_3\"]].values)\n",
    "            log_probs = torch.tensor(np.zeros((actions.shape[0], 1)))\n",
    "        \n",
    "        else:\n",
    "            states = torch.stack(traj_df[0])\n",
    "            actions = torch.stack(traj_df[1])\n",
    "            log_probs = torch.stack(traj_df[3])\n",
    "        \n",
    "        mdp = torch.cat((states, log_probs, actions), dim=1)\n",
    "        step_tensor = torch.cat((step_tensor.clone(), mdp.clone()), dim=0)\n",
    "    \n",
    "    return step_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede29cb4",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2712c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capacity = 100\n",
    "# replay_buffer = ReplayBuffer(capacity)\n",
    "# for i in range(NUM_EPOCHS):\n",
    "#     print(\"Epoch : \", i)\n",
    "#     for _ in range(EPISODES_TO_PLAY):\n",
    "#         samp_trajs = generate_session(t_max=constants.TRAJECTORY_SIZE,\n",
    "#                                       updater_obj=updater_obj,\n",
    "#                                       replay_buffer=replay_buffer,\n",
    "#                                       is_policy_inference=False,\n",
    "#                                       is_policy_update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3afc29b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_states, sampled_actions, sampled_rewards, sampled_next_states, sampled_dones, sampled_probs = \\\n",
    "#     replay_buffer.sample_trajectory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc962bc",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1dc45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim=3, action_dim=3):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.linear1 = nn.Linear(3, 64)\n",
    "        self.linear2 = nn.Linear(64 + 3, 32)\n",
    "        self.linear3 = nn.Linear(32, 32)\n",
    "        self.linear4 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        xa_cat = torch.cat([x,a], 1)\n",
    "        xa = F.relu(self.linear2(xa_cat))\n",
    "        xa = F.relu(self.linear3(xa))\n",
    "        qval = self.linear4(xa)\n",
    "\n",
    "        return qval\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim=3, action_dim=3):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.linear1 = nn.Linear(3, 64)\n",
    "        self.linear2 = nn.Linear(64, 64)\n",
    "        self.linear3 = nn.Linear(64, 3)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = F.relu(self.linear1(obs))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def estimate_action(self, obs):\n",
    "        action_mean = self.forward(obs)\n",
    "        \n",
    "        action_distribution = torch.distributions.Normal(action_mean, torch.ones_like(action_mean))\n",
    "        \n",
    "        sampled_action = action_distribution.rsample()\n",
    "        log_prob = action_distribution.log_prob(sampled_action)\n",
    "        \n",
    "        return torch.tanh(sampled_action), log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37a728d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise(object):\n",
    "    def __init__(self, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = 3\n",
    "        self.low          = -1\n",
    "        self.high         = +1\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        \n",
    "    def evolve_state(self):\n",
    "        x  = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "#     def get_action(self, action, t=0):\n",
    "#         ou_state = self.evolve_state()\n",
    "#         self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
    "#         return np.clip(action + ou_state, self.low, self.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fe4c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DDPGAgent:\n",
    "    \n",
    "    def __init__(self, env, gamma, tau, buffer_maxlen, critic_learning_rate, actor_learning_rate):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.env = env\n",
    "        self.obs_dim = 3\n",
    "        self.action_dim = 3\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        # initialize actor and critic networks\n",
    "        self.critic = Critic(self.obs_dim, self.action_dim).to(self.device)\n",
    "        self.critic_target = Critic(self.obs_dim, self.action_dim).to(self.device)\n",
    "        \n",
    "        self.actor = Actor(self.obs_dim, self.action_dim).to(self.device)\n",
    "        self.actor_target = Actor(self.obs_dim, self.action_dim).to(self.device)\n",
    "    \n",
    "        # Copy critic target parameters\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        # optimizers\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_learning_rate)\n",
    "        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=actor_learning_rate)\n",
    "    \n",
    "        self.replay_buffer = ReplayBuffer(buffer_maxlen)      \n",
    "        self.noise = OUNoise()\n",
    "        \n",
    "    def get_action(self, obs):\n",
    "        state = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
    "#         action = self.actor.forward(state)\n",
    "        action, log_prob = self.actor.estimate_action(state)\n",
    "    \n",
    "        action = action.cpu().detach().numpy()\n",
    "\n",
    "        return action, log_prob\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        \n",
    "        states, actions, rewards, next_states, _, _ = self.replay_buffer.sample_batch(batch_size)\n",
    "        \n",
    "        state_batch, action_batch, reward_batch, next_state_batch, masks, _ = self.replay_buffer.sample_batch(batch_size)\n",
    "        \n",
    "#         print(\"masks : \", masks.int())\n",
    "        \n",
    "        state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
    "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
    "        masks = torch.FloatTensor(masks.float()).to(self.device)\n",
    "   \n",
    "        curr_Q = self.critic.forward(state_batch, action_batch)\n",
    "        next_actions = self.actor_target.forward(next_state_batch)\n",
    "        next_Q = self.critic_target.forward(next_state_batch, next_actions.detach())\n",
    "        expected_Q = reward_batch + self.gamma * next_Q\n",
    "        \n",
    "        # update critic\n",
    "        q_loss = F.mse_loss(curr_Q, expected_Q.detach())\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        q_loss.backward() \n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # update actor\n",
    "#         policy_loss = -self.critic.forward(state_batch, self.actor.forward(state_batch)).mean()\n",
    "        policy_loss = -self.critic.forward(state_batch, self.actor.estimate_action(state_batch)[0]).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # update target networks \n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "       \n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47b9edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_train(env, agent, max_episodes, max_steps, batch_size):\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "#             action = torch.tensor(agent.get_action(state)[0])\n",
    "            print(\"state : \", )\n",
    "            print(\"agent.get_action(state)[0] : \", agent.get_action(state)[0])\n",
    "            next_state, reward, done = env.step(state, torch.tensor(agent.get_action(state)[0].squeeze(0)))\n",
    "#             print(\"action : \", action)\n",
    "#             print(\"next_state : \", next_state)\n",
    "#             print(\"reward : \", reward)\n",
    "#             print(\"done : \", done)\n",
    "            agent.replay_buffer.push(state, torch.tensor(agent.get_action(state)[0].squeeze(0)), reward, next_state, done, torch.tensor([]))\n",
    "            episode_reward += reward\n",
    "\n",
    "            if len(agent.replay_buffer) > batch_size:\n",
    "                agent.update(batch_size)   \n",
    "\n",
    "            if done or step == max_steps-1:\n",
    "                episode_rewards.append(episode_reward)\n",
    "                print(\"Episode \" + str(episode) + \": \" + str(episode_reward))\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "253d3e18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D_demo, D_samp = torch.tensor([]), torch.tensor([])\n",
    "\n",
    "max_episodes = 100\n",
    "max_steps = 500\n",
    "batch_size = 32\n",
    "\n",
    "gamma = 0.99\n",
    "tau = 1e-2\n",
    "buffer_maxlen = 100000\n",
    "critic_lr = 1e-3\n",
    "actor_lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02997be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = RobotEnvironment()\n",
    "env.set_reward_network(reward_network)\n",
    "env.is_reward_inference = False\n",
    "\n",
    "agent = DDPGAgent(env, gamma, tau, buffer_maxlen, critic_lr, actor_lr)\n",
    "\n",
    "# episode_rewards = mini_batch_train(env, agent, max_episodes, max_steps, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b47882f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.1220, 0.4467, 0.2398]])\n",
      "action_mu, action_std :  tensor([[ 0.0338,  0.0361, -0.0228]]) tensor([[1.0837, 1.0412, 1.0278]])\n",
      "action_dist.mean :  tensor([[ 0.0338,  0.0361, -0.0228]])\n",
      "action_dist.stddev :  tensor([[1.0837, 1.0412, 1.0278]])\n",
      "action_sample :  tensor([[ 0.0338,  0.0361, -0.0228]])\n",
      "action_log_prob :  tensor([[-0.9993, -0.9593, -0.9464]])\n",
      "state :  tensor([[0.2116, 0.4693, 0.3819]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1220, 0.4348, 0.2215]])\n",
      "action_mu, action_std :  tensor([[ 0.0330,  0.0359, -0.0228]]) tensor([[1.0825, 1.0398, 1.0273]])\n",
      "action_dist.mean :  tensor([[ 0.0330,  0.0359, -0.0228]])\n",
      "action_dist.stddev :  tensor([[1.0825, 1.0398, 1.0273]])\n",
      "action_sample :  tensor([[ 0.0330,  0.0359, -0.0228]])\n",
      "action_log_prob :  tensor([[-0.9982, -0.9580, -0.9459]])\n",
      "state :  tensor([[0.2112, 0.4692, 0.3819]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1244, 0.4400, 0.2272]])\n",
      "action_mu, action_std :  tensor([[ 0.0332,  0.0359, -0.0226]]) tensor([[1.0829, 1.0403, 1.0275]])\n",
      "action_dist.mean :  tensor([[ 0.0332,  0.0359, -0.0226]])\n",
      "action_dist.stddev :  tensor([[1.0829, 1.0403, 1.0275]])\n",
      "action_sample :  tensor([[ 0.0332,  0.0359, -0.0226]])\n",
      "action_log_prob :  tensor([[-0.9986, -0.9585, -0.9461]])\n",
      "state :  tensor([[0.2113, 0.4693, 0.3820]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1521, 0.4840, 0.2729]])\n",
      "action_mu, action_std :  tensor([[ 0.0343,  0.0359, -0.0207]]) tensor([[1.0868, 1.0441, 1.0288]])\n",
      "action_dist.mean :  tensor([[ 0.0343,  0.0359, -0.0207]])\n",
      "action_dist.stddev :  tensor([[1.0868, 1.0441, 1.0288]])\n",
      "action_sample :  tensor([[ 0.0343,  0.0359, -0.0207]])\n",
      "action_log_prob :  tensor([[-1.0021, -0.9621, -0.9473]])\n",
      "state :  tensor([[0.2123, 0.4701, 0.3826]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0182]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1300, 0.4559, 0.2475]])\n",
      "action_mu, action_std :  tensor([[ 0.0339,  0.0360, -0.0222]]) tensor([[1.0845, 1.0420, 1.0280]])\n",
      "action_dist.mean :  tensor([[ 0.0339,  0.0360, -0.0222]])\n",
      "action_dist.stddev :  tensor([[1.0845, 1.0420, 1.0280]])\n",
      "action_sample :  tensor([[ 0.0339,  0.0360, -0.0222]])\n",
      "action_log_prob :  tensor([[-1.0000, -0.9601, -0.9466]])\n",
      "state :  tensor([[0.2117, 0.4695, 0.3821]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1651, 0.5092, 0.3016]])\n",
      "action_mu, action_std :  tensor([[ 0.0354,  0.0362, -0.0195]]) tensor([[1.0887, 1.0453, 1.0293]])\n",
      "action_dist.mean :  tensor([[ 0.0354,  0.0362, -0.0195]])\n",
      "action_dist.stddev :  tensor([[1.0887, 1.0453, 1.0293]])\n",
      "action_sample :  tensor([[ 0.0354,  0.0362, -0.0195]])\n",
      "action_log_prob :  tensor([[-1.0039, -0.9632, -0.9478]])\n",
      "state :  tensor([[0.2131, 0.4705, 0.3830]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0182]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1505, 0.4846, 0.2741]])\n",
      "action_mu, action_std :  tensor([[ 0.0344,  0.0359, -0.0208]]) tensor([[1.0868, 1.0441, 1.0288]])\n",
      "action_dist.mean :  tensor([[ 0.0344,  0.0359, -0.0208]])\n",
      "action_dist.stddev :  tensor([[1.0868, 1.0441, 1.0288]])\n",
      "action_sample :  tensor([[ 0.0344,  0.0359, -0.0208]])\n",
      "action_log_prob :  tensor([[-1.0022, -0.9621, -0.9474]])\n",
      "state :  tensor([[0.2123, 0.4700, 0.3826]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0182]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1238, 0.4322, 0.2140]])\n",
      "action_mu, action_std :  tensor([[ 0.0327,  0.0357, -0.0225]]) tensor([[1.0820, 1.0393, 1.0272]])\n",
      "action_dist.mean :  tensor([[ 0.0327,  0.0357, -0.0225]])\n",
      "action_dist.stddev :  tensor([[1.0820, 1.0393, 1.0272]])\n",
      "action_sample :  tensor([[ 0.0327,  0.0357, -0.0225]])\n",
      "action_log_prob :  tensor([[-0.9978, -0.9575, -0.9457]])\n",
      "state :  tensor([[0.2111, 0.4693, 0.3820]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1834, 0.5245, 0.3035]])\n",
      "action_mu, action_std :  tensor([[ 0.0354,  0.0354, -0.0179]]) tensor([[1.0896, 1.0455, 1.0294]])\n",
      "action_dist.mean :  tensor([[ 0.0354,  0.0354, -0.0179]])\n",
      "action_dist.stddev :  tensor([[1.0896, 1.0455, 1.0294]])\n",
      "action_sample :  tensor([[ 0.0354,  0.0354, -0.0179]])\n",
      "action_log_prob :  tensor([[-1.0048, -0.9634, -0.9479]])\n",
      "state :  tensor([[0.2134, 0.4712, 0.3835]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0181]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0181]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0181]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2118, 0.4693, 0.3835]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1264, 0.4411, 0.2260]])\n",
      "action_mu, action_std :  tensor([[ 0.0331,  0.0358, -0.0224]]) tensor([[1.0829, 1.0403, 1.0275]])\n",
      "action_dist.mean :  tensor([[ 0.0331,  0.0358, -0.0224]])\n",
      "action_dist.stddev :  tensor([[1.0829, 1.0403, 1.0275]])\n",
      "action_sample :  tensor([[ 0.0331,  0.0358, -0.0224]])\n",
      "action_log_prob :  tensor([[-0.9986, -0.9584, -0.9460]])\n",
      "state :  tensor([[0.2113, 0.4694, 0.3820]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1475, 0.4673, 0.2349]])\n",
      "action_mu, action_std :  tensor([[ 0.0334,  0.0351, -0.0206]]) tensor([[1.0842, 1.0415, 1.0277]])\n",
      "action_dist.mean :  tensor([[ 0.0334,  0.0351, -0.0206]])\n",
      "action_dist.stddev :  tensor([[1.0842, 1.0415, 1.0277]])\n",
      "action_sample :  tensor([[ 0.0334,  0.0351, -0.0206]])\n",
      "action_log_prob :  tensor([[-0.9998, -0.9596, -0.9463]])\n",
      "state :  tensor([[0.2118, 0.4702, 0.3826]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1352, 0.4784, 0.2896]])\n",
      "action_mu, action_std :  tensor([[ 0.0350,  0.0368, -0.0223]]) tensor([[1.0872, 1.0448, 1.0293]])\n",
      "action_dist.mean :  tensor([[ 0.0350,  0.0368, -0.0223]])\n",
      "action_dist.stddev :  tensor([[1.0872, 1.0448, 1.0293]])\n",
      "action_sample :  tensor([[ 0.0350,  0.0368, -0.0223]])\n",
      "action_log_prob :  tensor([[-1.0025, -0.9628, -0.9478]])\n",
      "state :  tensor([[0.2123, 0.4695, 0.3821]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0182]]) tensor([[1.0947, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1361, 0.4719, 0.2708]])\n",
      "action_mu, action_std :  tensor([[ 0.0345,  0.0363, -0.0220]]) tensor([[1.0862, 1.0438, 1.0287]])\n",
      "action_dist.mean :  tensor([[ 0.0345,  0.0363, -0.0220]])\n",
      "action_dist.stddev :  tensor([[1.0862, 1.0438, 1.0287]])\n",
      "action_sample :  tensor([[ 0.0345,  0.0363, -0.0220]])\n",
      "action_log_prob :  tensor([[-1.0016, -0.9618, -0.9472]])\n",
      "state :  tensor([[0.2121, 0.4696, 0.3822]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0182]]) tensor([[1.0947, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1394, 0.4783, 0.2751]])\n",
      "action_mu, action_std :  tensor([[ 0.0347,  0.0362, -0.0217]]) tensor([[1.0865, 1.0441, 1.0289]])\n",
      "action_dist.mean :  tensor([[ 0.0347,  0.0362, -0.0217]])\n",
      "action_dist.stddev :  tensor([[1.0865, 1.0441, 1.0289]])\n",
      "action_sample :  tensor([[ 0.0347,  0.0362, -0.0217]])\n",
      "action_log_prob :  tensor([[-1.0019, -0.9621, -0.9474]])\n",
      "state :  tensor([[0.2123, 0.4698, 0.3823]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0182]]) tensor([[1.0947, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1340, 0.4822, 0.3116]])\n",
      "action_mu, action_std :  tensor([[ 0.0355,  0.0375, -0.0226]]) tensor([[1.0884, 1.0457, 1.0298]])\n",
      "action_dist.mean :  tensor([[ 0.0355,  0.0375, -0.0226]])\n",
      "action_dist.stddev :  tensor([[1.0884, 1.0457, 1.0298]])\n",
      "action_sample :  tensor([[ 0.0355,  0.0375, -0.0226]])\n",
      "action_log_prob :  tensor([[-1.0037, -0.9637, -0.9483]])\n",
      "state :  tensor([[0.2126, 0.4693, 0.3820]])\n",
      "action_mu, action_std :  tensor([[ 0.0315,  0.0386, -0.0182]]) tensor([[1.0947, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0315,  0.0386, -0.0182]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0315,  0.0386, -0.0182]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9658, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1204, 0.4428, 0.2333]])\n",
      "action_mu, action_std :  tensor([[ 0.0336,  0.0361, -0.0229]]) tensor([[1.0832, 1.0407, 1.0276]])\n",
      "action_dist.mean :  tensor([[ 0.0336,  0.0361, -0.0229]])\n",
      "action_dist.stddev :  tensor([[1.0832, 1.0407, 1.0276]])\n",
      "action_sample :  tensor([[ 0.0336,  0.0361, -0.0229]])\n",
      "action_log_prob :  tensor([[-0.9988, -0.9588, -0.9462]])\n",
      "state :  tensor([[0.2115, 0.4693, 0.3819]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1260, 0.4314, 0.2122]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_mu, action_std :  tensor([[ 0.0326,  0.0357, -0.0224]]) tensor([[1.0820, 1.0392, 1.0271]])\n",
      "action_dist.mean :  tensor([[ 0.0326,  0.0357, -0.0224]])\n",
      "action_dist.stddev :  tensor([[1.0820, 1.0392, 1.0271]])\n",
      "action_sample :  tensor([[ 0.0326,  0.0357, -0.0224]])\n",
      "action_log_prob :  tensor([[-0.9978, -0.9574, -0.9457]])\n",
      "state :  tensor([[0.2110, 0.4693, 0.3820]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0946, 1.0480, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0946, 1.0480, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1172, 0.4466, 0.2500]])\n",
      "action_mu, action_std :  tensor([[ 0.0341,  0.0365, -0.0234]]) tensor([[1.0842, 1.0418, 1.0281]])\n",
      "action_dist.mean :  tensor([[ 0.0341,  0.0365, -0.0234]])\n",
      "action_dist.stddev :  tensor([[1.0842, 1.0418, 1.0281]])\n",
      "action_sample :  tensor([[ 0.0341,  0.0365, -0.0234]])\n",
      "action_log_prob :  tensor([[-0.9998, -0.9599, -0.9466]])\n",
      "state :  tensor([[0.2117, 0.4691, 0.3817]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9658, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1392, 0.4837, 0.3045]])\n",
      "action_mu, action_std :  tensor([[ 0.0353,  0.0372, -0.0221]]) tensor([[1.0882, 1.0454, 1.0296]])\n",
      "action_dist.mean :  tensor([[ 0.0353,  0.0372, -0.0221]])\n",
      "action_dist.stddev :  tensor([[1.0882, 1.0454, 1.0296]])\n",
      "action_sample :  tensor([[ 0.0353,  0.0372, -0.0221]])\n",
      "action_log_prob :  tensor([[-1.0034, -0.9634, -0.9481]])\n",
      "state :  tensor([[0.2126, 0.4695, 0.3821]])\n",
      "action_mu, action_std :  tensor([[ 0.0315,  0.0386, -0.0182]]) tensor([[1.0947, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0315,  0.0386, -0.0182]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0315,  0.0386, -0.0182]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1383, 0.4544, 0.2293]])\n",
      "action_mu, action_std :  tensor([[ 0.0332,  0.0354, -0.0214]]) tensor([[1.0835, 1.0408, 1.0276]])\n",
      "action_dist.mean :  tensor([[ 0.0332,  0.0354, -0.0214]])\n",
      "action_dist.stddev :  tensor([[1.0835, 1.0408, 1.0276]])\n",
      "action_sample :  tensor([[ 0.0332,  0.0354, -0.0214]])\n",
      "action_log_prob :  tensor([[-0.9992, -0.9590, -0.9461]])\n",
      "state :  tensor([[0.2116, 0.4698, 0.3824]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1531, 0.5019, 0.3216]])\n",
      "action_mu, action_std :  tensor([[ 0.0358,  0.0372, -0.0208]]) tensor([[1.0895, 1.0462, 1.0298]])\n",
      "action_dist.mean :  tensor([[ 0.0358,  0.0372, -0.0208]])\n",
      "action_dist.stddev :  tensor([[1.0895, 1.0462, 1.0298]])\n",
      "action_sample :  tensor([[ 0.0358,  0.0372, -0.0208]])\n",
      "action_log_prob :  tensor([[-1.0047, -0.9641, -0.9483]])\n",
      "state :  tensor([[0.2131, 0.4699, 0.3826]])\n",
      "action_mu, action_std :  tensor([[ 0.0315,  0.0386, -0.0182]]) tensor([[1.0947, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0315,  0.0386, -0.0182]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0315,  0.0386, -0.0182]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1366, 0.4732, 0.2721]])\n",
      "action_mu, action_std :  tensor([[ 0.0346,  0.0363, -0.0219]]) tensor([[1.0862, 1.0438, 1.0287]])\n",
      "action_dist.mean :  tensor([[ 0.0346,  0.0363, -0.0219]])\n",
      "action_dist.stddev :  tensor([[1.0862, 1.0438, 1.0287]])\n",
      "action_sample :  tensor([[ 0.0346,  0.0363, -0.0219]])\n",
      "action_log_prob :  tensor([[-1.0017, -0.9618, -0.9473]])\n",
      "state :  tensor([[0.2122, 0.4697, 0.3822]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0182]]) tensor([[1.0947, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1385, 0.4840, 0.3006]])\n",
      "action_mu, action_std :  tensor([[ 0.0353,  0.0370, -0.0221]]) tensor([[1.0879, 1.0453, 1.0295]])\n",
      "action_dist.mean :  tensor([[ 0.0353,  0.0370, -0.0221]])\n",
      "action_dist.stddev :  tensor([[1.0879, 1.0453, 1.0295]])\n",
      "action_sample :  tensor([[ 0.0353,  0.0370, -0.0221]])\n",
      "action_log_prob :  tensor([[-1.0032, -0.9632, -0.9480]])\n",
      "state :  tensor([[0.2125, 0.4695, 0.3821]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0182]]) tensor([[1.0947, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1303, 0.4598, 0.2521]])\n",
      "action_mu, action_std :  tensor([[ 0.0341,  0.0360, -0.0222]]) tensor([[1.0848, 1.0424, 1.0281]])\n",
      "action_dist.mean :  tensor([[ 0.0341,  0.0360, -0.0222]])\n",
      "action_dist.stddev :  tensor([[1.0848, 1.0424, 1.0281]])\n",
      "action_sample :  tensor([[ 0.0341,  0.0360, -0.0222]])\n",
      "action_log_prob :  tensor([[-1.0003, -0.9604, -0.9467]])\n",
      "state :  tensor([[0.2119, 0.4696, 0.3821]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1264, 0.4410, 0.2232]])\n",
      "action_mu, action_std :  tensor([[ 0.0331,  0.0357, -0.0224]]) tensor([[1.0827, 1.0401, 1.0274]])\n",
      "action_dist.mean :  tensor([[ 0.0331,  0.0357, -0.0224]])\n",
      "action_dist.stddev :  tensor([[1.0827, 1.0401, 1.0274]])\n",
      "action_sample :  tensor([[ 0.0331,  0.0357, -0.0224]])\n",
      "action_log_prob :  tensor([[-0.9984, -0.9582, -0.9460]])\n",
      "state :  tensor([[0.2113, 0.4694, 0.3820]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1527, 0.4979, 0.3037]])\n",
      "action_mu, action_std :  tensor([[ 0.0354,  0.0367, -0.0207]]) tensor([[1.0885, 1.0454, 1.0295]])\n",
      "action_dist.mean :  tensor([[ 0.0354,  0.0367, -0.0207]])\n",
      "action_dist.stddev :  tensor([[1.0885, 1.0454, 1.0295]])\n",
      "action_sample :  tensor([[ 0.0354,  0.0367, -0.0207]])\n",
      "action_log_prob :  tensor([[-1.0037, -0.9633, -0.9480]])\n",
      "state :  tensor([[0.2129, 0.4700, 0.3826]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0182]]) tensor([[1.0947, 1.0481, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1211, 0.4495, 0.2472]])\n",
      "action_mu, action_std :  tensor([[ 0.0341,  0.0363, -0.0230]]) tensor([[1.0841, 1.0418, 1.0280]])\n",
      "action_dist.mean :  tensor([[ 0.0341,  0.0363, -0.0230]])\n",
      "action_dist.stddev :  tensor([[1.0841, 1.0418, 1.0280]])\n",
      "action_sample :  tensor([[ 0.0341,  0.0363, -0.0230]])\n",
      "action_log_prob :  tensor([[-0.9997, -0.9598, -0.9466]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3818]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9658, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1406, 0.4564, 0.2519]])\n",
      "action_mu, action_std :  tensor([[ 0.0333,  0.0360, -0.0216]]) tensor([[1.0852, 1.0424, 1.0282]])\n",
      "action_dist.mean :  tensor([[ 0.0333,  0.0360, -0.0216]])\n",
      "action_dist.stddev :  tensor([[1.0852, 1.0424, 1.0282]])\n",
      "action_sample :  tensor([[ 0.0333,  0.0360, -0.0216]])\n",
      "action_log_prob :  tensor([[-1.0007, -0.9605, -0.9467]])\n",
      "state :  tensor([[0.2116, 0.4696, 0.3823]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1412, 0.4460, 0.2388]])\n",
      "action_mu, action_std :  tensor([[ 0.0326,  0.0359, -0.0216]]) tensor([[1.0844, 1.0414, 1.0278]])\n",
      "action_dist.mean :  tensor([[ 0.0326,  0.0359, -0.0216]])\n",
      "action_dist.stddev :  tensor([[1.0844, 1.0414, 1.0278]])\n",
      "action_sample :  tensor([[ 0.0326,  0.0359, -0.0216]])\n",
      "action_log_prob :  tensor([[-0.9999, -0.9595, -0.9464]])\n",
      "state :  tensor([[0.2113, 0.4695, 0.3823]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1061, 0.4442, 0.2877]])\n",
      "action_mu, action_std :  tensor([[ 0.0349,  0.0378, -0.0249]]) tensor([[1.0863, 1.0441, 1.0290]])\n",
      "action_dist.mean :  tensor([[ 0.0349,  0.0378, -0.0249]])\n",
      "action_dist.stddev :  tensor([[1.0863, 1.0441, 1.0290]])\n",
      "action_sample :  tensor([[ 0.0349,  0.0378, -0.0249]])\n",
      "action_log_prob :  tensor([[-1.0017, -0.9621, -0.9476]])\n",
      "state :  tensor([[0.2118, 0.4684, 0.3812]])\n",
      "action_mu, action_std :  tensor([[ 0.0315,  0.0386, -0.0183]]) tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0315,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0315,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0093, -0.9658, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1701, 0.5095, 0.2877]])\n",
      "action_mu, action_std :  tensor([[ 0.0350,  0.0356, -0.0189]]) tensor([[1.0881, 1.0447, 1.0290]])\n",
      "action_dist.mean :  tensor([[ 0.0350,  0.0356, -0.0189]])\n",
      "action_dist.stddev :  tensor([[1.0881, 1.0447, 1.0290]])\n",
      "action_sample :  tensor([[ 0.0350,  0.0356, -0.0189]])\n",
      "action_log_prob :  tensor([[-1.0033, -0.9627, -0.9475]])\n",
      "state :  tensor([[0.2130, 0.4708, 0.3832]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0182]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2034, 0.5467, 0.3169]])\n",
      "action_mu, action_std :  tensor([[ 0.0356,  0.0345, -0.0166]]) tensor([[1.0914, 1.0465, 1.0301]])\n",
      "action_dist.mean :  tensor([[ 0.0356,  0.0345, -0.0166]])\n",
      "action_dist.stddev :  tensor([[1.0914, 1.0465, 1.0301]])\n",
      "action_sample :  tensor([[ 0.0356,  0.0345, -0.0166]])\n",
      "action_log_prob :  tensor([[-1.0064, -0.9644, -0.9486]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2137, 0.4719, 0.3840]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0385, -0.0181]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0385, -0.0181]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0385, -0.0181]])\n",
      "action_log_prob :  tensor([[-1.0095, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2118, 0.4694, 0.3835]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1244, 0.4534, 0.2507]])\n",
      "action_mu, action_std :  tensor([[ 0.0341,  0.0362, -0.0227]]) tensor([[1.0845, 1.0421, 1.0281]])\n",
      "action_dist.mean :  tensor([[ 0.0341,  0.0362, -0.0227]])\n",
      "action_dist.stddev :  tensor([[1.0845, 1.0421, 1.0281]])\n",
      "action_sample :  tensor([[ 0.0341,  0.0362, -0.0227]])\n",
      "action_log_prob :  tensor([[-1.0000, -0.9602, -0.9466]])\n",
      "state :  tensor([[0.2118, 0.4694, 0.3819]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0946, 1.0480, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1575, 0.5031, 0.3265]])\n",
      "action_mu, action_std :  tensor([[ 0.0357,  0.0373, -0.0205]]) tensor([[1.0900, 1.0463, 1.0299]])\n",
      "action_dist.mean :  tensor([[ 0.0357,  0.0373, -0.0205]])\n",
      "action_dist.stddev :  tensor([[1.0900, 1.0463, 1.0299]])\n",
      "action_sample :  tensor([[ 0.0357,  0.0373, -0.0205]])\n",
      "action_log_prob :  tensor([[-1.0051, -0.9642, -0.9484]])\n",
      "state :  tensor([[0.2131, 0.4700, 0.3827]])\n",
      "action_mu, action_std :  tensor([[ 0.0315,  0.0386, -0.0182]]) tensor([[1.0947, 1.0481, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0315,  0.0386, -0.0182]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0315,  0.0386, -0.0182]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1998, 0.5418, 0.3106]])\n",
      "action_mu, action_std :  tensor([[ 0.0355,  0.0346, -0.0168]]) tensor([[1.0908, 1.0461, 1.0298]])\n",
      "action_dist.mean :  tensor([[ 0.0355,  0.0346, -0.0168]])\n",
      "action_dist.stddev :  tensor([[1.0908, 1.0461, 1.0298]])\n",
      "action_sample :  tensor([[ 0.0355,  0.0346, -0.0168]])\n",
      "action_log_prob :  tensor([[-1.0059, -0.9640, -0.9483]])\n",
      "state :  tensor([[0.2137, 0.4718, 0.3839]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0385, -0.0181]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0385, -0.0181]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0385, -0.0181]])\n",
      "action_log_prob :  tensor([[-1.0095, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2118, 0.4694, 0.3835]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2188, 0.5488, 0.2828]])\n",
      "action_mu, action_std :  tensor([[ 0.0345,  0.0331, -0.0149]]) tensor([[1.0897, 1.0451, 1.0291]])\n",
      "action_dist.mean :  tensor([[ 0.0345,  0.0331, -0.0149]])\n",
      "action_dist.stddev :  tensor([[1.0897, 1.0451, 1.0291]])\n",
      "action_sample :  tensor([[ 0.0345,  0.0331, -0.0149]])\n",
      "action_log_prob :  tensor([[-1.0049, -0.9630, -0.9476]])\n",
      "state :  tensor([[0.2135, 0.4726, 0.3845]])\n",
      "action_mu, action_std :  tensor([[ 0.0317,  0.0386, -0.0181]]) tensor([[1.0948, 1.0482, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0317,  0.0386, -0.0181]])\n",
      "action_dist.stddev :  tensor([[1.0948, 1.0482, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0317,  0.0386, -0.0181]])\n",
      "action_log_prob :  tensor([[-1.0095, -0.9660, -0.9488]])\n",
      "state :  tensor([[0.2118, 0.4694, 0.3835]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1788, 0.5198, 0.3815]])\n",
      "action_mu, action_std :  tensor([[ 0.0361,  0.0380, -0.0197]]) tensor([[1.0944, 1.0490, 1.0313]])\n",
      "action_dist.mean :  tensor([[ 0.0361,  0.0380, -0.0197]])\n",
      "action_dist.stddev :  tensor([[1.0944, 1.0490, 1.0313]])\n",
      "action_sample :  tensor([[ 0.0361,  0.0380, -0.0197]])\n",
      "action_log_prob :  tensor([[-1.0091, -0.9668, -0.9497]])\n",
      "state :  tensor([[0.2136, 0.4700, 0.3829]])\n",
      "action_mu, action_std :  tensor([[ 0.0315,  0.0385, -0.0182]]) tensor([[1.0947, 1.0481, 1.0302]])\n",
      "action_dist.mean :  tensor([[ 0.0315,  0.0385, -0.0182]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0302]])\n",
      "action_sample :  tensor([[ 0.0315,  0.0385, -0.0182]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9487]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3835]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2186, 0.5275, 0.2364]])\n",
      "action_mu, action_std :  tensor([[ 0.0331,  0.0326, -0.0140]]) tensor([[1.0864, 1.0425, 1.0274]])\n",
      "action_dist.mean :  tensor([[ 0.0331,  0.0326, -0.0140]])\n",
      "action_dist.stddev :  tensor([[1.0864, 1.0425, 1.0274]])\n",
      "action_sample :  tensor([[ 0.0331,  0.0326, -0.0140]])\n",
      "action_log_prob :  tensor([[-1.0018, -0.9605, -0.9460]])\n",
      "state :  tensor([[0.2129, 0.4727, 0.3848]])\n",
      "action_mu, action_std :  tensor([[ 0.0317,  0.0386, -0.0182]]) tensor([[1.0948, 1.0482, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0317,  0.0386, -0.0182]])\n",
      "action_dist.stddev :  tensor([[1.0948, 1.0482, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0317,  0.0386, -0.0182]])\n",
      "action_log_prob :  tensor([[-1.0095, -0.9660, -0.9488]])\n",
      "state :  tensor([[0.2118, 0.4694, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2380, 0.5587, 0.2609]])\n",
      "action_mu, action_std :  tensor([[ 0.0338,  0.0318, -0.0132]]) tensor([[1.0891, 1.0444, 1.0286]])\n",
      "action_dist.mean :  tensor([[ 0.0338,  0.0318, -0.0132]])\n",
      "action_dist.stddev :  tensor([[1.0891, 1.0444, 1.0286]])\n",
      "action_sample :  tensor([[ 0.0338,  0.0318, -0.0132]])\n",
      "action_log_prob :  tensor([[-1.0043, -0.9624, -0.9472]])\n",
      "state :  tensor([[0.2134, 0.4734, 0.3851]])\n",
      "action_mu, action_std :  tensor([[ 0.0317,  0.0386, -0.0181]]) tensor([[1.0948, 1.0482, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0317,  0.0386, -0.0181]])\n",
      "action_dist.stddev :  tensor([[1.0948, 1.0482, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0317,  0.0386, -0.0181]])\n",
      "action_log_prob :  tensor([[-1.0095, -0.9660, -0.9488]])\n",
      "state :  tensor([[0.2118, 0.4694, 0.3835]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.1669, 0.4860, 0.2529]])\n",
      "action_mu, action_std :  tensor([[ 0.0334,  0.0350, -0.0192]]) tensor([[1.0861, 1.0432, 1.0282]])\n",
      "action_dist.mean :  tensor([[ 0.0334,  0.0350, -0.0192]])\n",
      "action_dist.stddev :  tensor([[1.0861, 1.0432, 1.0282]])\n",
      "action_sample :  tensor([[ 0.0334,  0.0350, -0.0192]])\n",
      "action_log_prob :  tensor([[-1.0015, -0.9612, -0.9467]])\n",
      "state :  tensor([[0.2121, 0.4706, 0.3831]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0182]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0182]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2151, 0.5400, 0.2865]])\n",
      "action_mu, action_std :  tensor([[ 0.0342,  0.0337, -0.0153]]) tensor([[1.0897, 1.0451, 1.0290]])\n",
      "action_dist.mean :  tensor([[ 0.0342,  0.0337, -0.0153]])\n",
      "action_dist.stddev :  tensor([[1.0897, 1.0451, 1.0290]])\n",
      "action_sample :  tensor([[ 0.0342,  0.0337, -0.0153]])\n",
      "action_log_prob :  tensor([[-1.0049, -0.9630, -0.9475]])\n",
      "state :  tensor([[0.2133, 0.4722, 0.3844]])\n",
      "action_mu, action_std :  tensor([[ 0.0317,  0.0386, -0.0181]]) tensor([[1.0948, 1.0482, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0317,  0.0386, -0.0181]])\n",
      "action_dist.stddev :  tensor([[1.0948, 1.0482, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0317,  0.0386, -0.0181]])\n",
      "action_log_prob :  tensor([[-1.0095, -0.9660, -0.9488]])\n",
      "state :  tensor([[0.2118, 0.4694, 0.3835]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2286, 0.5587, 0.2921]])\n",
      "action_mu, action_std :  tensor([[ 0.0345,  0.0328, -0.0144]]) tensor([[1.0907, 1.0457, 1.0295]])\n",
      "action_dist.mean :  tensor([[ 0.0345,  0.0328, -0.0144]])\n",
      "action_dist.stddev :  tensor([[1.0907, 1.0457, 1.0295]])\n",
      "action_sample :  tensor([[ 0.0345,  0.0328, -0.0144]])\n",
      "action_log_prob :  tensor([[-1.0058, -0.9636, -0.9480]])\n",
      "state :  tensor([[0.2136, 0.4729, 0.3847]])\n",
      "action_mu, action_std :  tensor([[ 0.0317,  0.0386, -0.0181]]) tensor([[1.0948, 1.0482, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0317,  0.0386, -0.0181]])\n",
      "action_dist.stddev :  tensor([[1.0948, 1.0482, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0317,  0.0386, -0.0181]])\n",
      "action_log_prob :  tensor([[-1.0095, -0.9660, -0.9488]])\n",
      "state :  tensor([[0.2118, 0.4694, 0.3835]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2396, 0.5491, 0.2385]])\n",
      "action_mu, action_std :  tensor([[ 0.0335,  0.0313, -0.0126]]) tensor([[1.0873, 1.0432, 1.0278]])\n",
      "action_dist.mean :  tensor([[ 0.0335,  0.0313, -0.0126]])\n",
      "action_dist.stddev :  tensor([[1.0873, 1.0432, 1.0278]])\n",
      "action_sample :  tensor([[ 0.0335,  0.0313, -0.0126]])\n",
      "action_log_prob :  tensor([[-1.0027, -0.9612, -0.9464]])\n",
      "state :  tensor([[0.2134, 0.4736, 0.3853]])\n",
      "action_mu, action_std :  tensor([[ 0.0317,  0.0386, -0.0181]]) tensor([[1.0948, 1.0482, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0317,  0.0386, -0.0181]])\n",
      "action_dist.stddev :  tensor([[1.0948, 1.0482, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0317,  0.0386, -0.0181]])\n",
      "action_log_prob :  tensor([[-1.0095, -0.9660, -0.9488]])\n",
      "state :  tensor([[0.2118, 0.4694, 0.3835]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n",
      "state :  tensor([[0.2117, 0.4693, 0.3834]])\n",
      "action_mu, action_std :  tensor([[ 0.0316,  0.0386, -0.0183]]) tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_dist.mean :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_dist.stddev :  tensor([[1.0947, 1.0481, 1.0303]])\n",
      "action_sample :  tensor([[ 0.0316,  0.0386, -0.0183]])\n",
      "action_log_prob :  tensor([[-1.0094, -0.9659, -0.9488]])\n"
     ]
    }
   ],
   "source": [
    "demo_traj_list = []\n",
    "\n",
    "for traj_start_index in range(len(trajectory_indices)):\n",
    "    \n",
    "    traj_df, reward_values_demo_data, reward_values_estim_data, logprob_action_estim_avg = \\\n",
    "        functions.get_estimated_rewards(configs=configs,\n",
    "                                        updater_obj=updater_obj,\n",
    "                                        data_loader=training_data,\n",
    "                                        policy_network=updater_obj.policy_network,\n",
    "                                        reward_network=updater_obj.reward_network,\n",
    "                                        trajectory_indices=trajectory_indices,\n",
    "                                        traj_start_index=traj_start_index,\n",
    "                                        is_inference_reward=True,\n",
    "                                        is_inference_policy=True)\n",
    "    demo_traj_list.append(traj_df)\n",
    "    del traj_df\n",
    "\n",
    "D_demo = preprocess_traj(traj_list_=demo_traj_list,\n",
    "                         step_tensor=D_demo,\n",
    "                         is_Demo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7261a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0\n",
      "loss_IOC :  -0.2266782596314631\n",
      "loss_IOC :  -0.22670409902744226\n",
      "loss_IOC :  -0.22665104833641708\n",
      "loss_IOC :  -0.22661380200170111\n",
      "loss_IOC :  -0.22616552736715967\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.49455532 -0.6351016  -0.70126   ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.6234666  0.41434664 0.87598324]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.8159721  0.9538097  0.30330393]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.5497063  -0.45127332  0.43321475]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.84627545  0.02798351 -0.0808639 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.86010545 -0.6220966   0.7851015 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.80227643 -0.3762393   0.75453216]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.91654336  0.85077244  0.441727  ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.91843575  0.9002074   0.55702996]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.63082296 -0.1797663  -0.89138204]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.6029873   0.09780756 -0.85378444]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.9002835  -0.96834624  0.7214973 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.10109525  0.5327855  -0.46447784]]\n",
      "Episode 0: tensor([6.6126])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.9934933  0.42320657 0.8942404 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.34343812 0.9900067  0.7202675 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.891826    0.7659806   0.22000979]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.21094076  0.15080518 -0.76608247]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.61636084 -0.69540274  0.59739786]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.25581026  0.8357605  -0.40024298]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.6561551  -0.23714109 -0.18549119]]\n",
      "Episode 1: tensor([3.5610])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9159793  -0.35775575 -0.86239946]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.975074    0.09068745 -0.62539804]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.81932557  0.43009096 -0.10165072]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.6217295   0.53140265 -0.9525454 ]]\n",
      "Episode 2: tensor([2.0375])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.18975453 -0.5238863  -0.3143395 ]]\n",
      "Episode 3: tensor([0.5132])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.01415226  0.37561223 -0.35810706]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.00830771 -0.9673835  -0.77541804]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.18043995 -0.1936828  -0.08636296]]\n",
      "Episode 4: tensor([1.5451])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.719655   -0.35147932 -0.9866111 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.4325702  -0.5703568   0.44227424]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.44772783 -0.4882477  -0.9281487 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.91912067  0.32282704  0.5283188 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99526024 -0.55880326 -0.07082769]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.24177672 -0.12395516  0.15521684]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.2369306  -0.2895318   0.49808604]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.58022654 -0.42887866  0.82822907]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9103981  -0.05475004 -0.51700324]]\n",
      "Episode 5: tensor([4.5859])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.78269184 -0.7049114  -0.57355416]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.12119788  0.41476792 -0.936776  ]]\n",
      "Episode 6: tensor([1.0185])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.87680244  0.85703784 -0.80907106]]\n",
      "Episode 7: tensor([0.5139])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.90897137  0.80471647  0.9210622 ]]\n",
      "Episode 8: tensor([0.5140])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.21779661 -0.75826645  0.8635858 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.52462703 -0.89972657  0.8745827 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.8799505  -0.8507243   0.44666168]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.24154821  0.32996613 -0.779561  ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.5000137   0.8841887   0.56500864]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.7067268  -0.82593626  0.93994784]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.8479133 -0.6257764 -0.8957732]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.56991744 -0.06321388 -0.00124719]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.69670945 -0.06329009 -0.74920225]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.61036664  0.14030059  0.12286121]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.10154574 -0.7898729  -0.67227507]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.98609567  0.089031   -0.4813634 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.61314934  0.12466709 -0.422695  ]]\n",
      "Episode 9: tensor([6.6454])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.6585474  -0.976092    0.30994016]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.89147663 -0.9003933  -0.6756163 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.18555401 -0.9056621   0.95203006]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.12166091 -0.09215314  0.329576  ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.73870856 0.15683986 0.28932834]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.661857    0.6153946  -0.17526741]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.0291505  -0.51823515 -0.88302284]]\n",
      "Episode 10: tensor([3.5814])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.11453538  0.73960924 -0.64151275]]\n",
      "Episode 11: tensor([0.5138])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.46379244  0.98826003  0.93400043]]\n",
      "Episode 12: tensor([0.5138])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.94855595 -0.35503146 -0.90924186]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.22000906  0.6502653  -0.45284152]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.6504603  -0.10649045 -0.4324254 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.6989988  -0.14451872  0.08338962]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.62974584 -0.44865042 -0.8113629 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.92974585  0.7626895  -0.45876825]]\n",
      "Episode 13: tensor([3.0654])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.58519083 -0.7583962  -0.80441713]]\n",
      "Episode 14: tensor([0.5142])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.4913425   0.46910363 -0.877072  ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.2875859  0.45566356 0.03338782]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.40237874  0.6575542  -0.7972182 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.9748023  -0.21534789 -0.73404765]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.3553737   0.7258807  -0.56961536]]\n",
      "Episode 15: tensor([2.5298])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.04577904 0.9689325  0.8317318 ]]\n",
      "Episode 16: tensor([0.5137])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.5767821  0.7402742 -0.9064222]]\n",
      "Episode 17: tensor([0.5147])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.53524554 -0.83595514  0.50448287]]\n",
      "Episode 18: tensor([0.5141])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.8697671  0.4898342 -0.8016585]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.76885116  0.44344383 -0.9766149 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.2810441  -0.95667094 -0.9715297 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9726056  -0.32576784  0.94958466]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.96026564 -0.82838833 -0.6898248 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.41679445  0.4292493  -0.88703185]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.36769745  0.00609951 -0.9454858 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.8917983  -0.81934774  0.900761  ]]\n",
      "Episode 19: tensor([4.0502])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.50348324  0.6270562  -0.16053568]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.76986796 -0.5977543   0.8532617 ]]\n",
      "Episode 20: tensor([1.0274])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.30357635 -0.49716637  0.9029912 ]]\n",
      "Episode 21: tensor([0.5142])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.18611673 -0.4919732  -0.33227003]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.57951987 -0.48424554  0.3467555 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.5556094 -0.0909882 -0.816758 ]]\n",
      "Episode 22: tensor([1.5286])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.02018644  0.8889269  -0.8985805 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.08425216  0.9360492  -0.5217608 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9222713  -0.6844839  -0.98049146]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.7488822 -0.5784488 -0.7794263]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.492498    0.16360402 -0.58905333]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.98556733 0.815336   0.3384645 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.98770285 -0.39116463 -0.86876297]]\n",
      "Episode 23: tensor([3.5852])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.36765403 -0.83319813  0.15557627]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.4190348  0.6764211  0.4113858]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.44154102  0.15390094 -0.86561793]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.2685699  0.44813243 0.6749188 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.7283199  0.6250674  0.8751404]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.28286892  0.24624431 -0.546418  ]]\n",
      "Episode 24: tensor([3.0810])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.7476136  -0.44505796  0.5378509 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.01277165  0.03411263  0.5959078 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.7685108   0.3144503  -0.20028858]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.8366553   0.05570719  0.3170835 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.83738065 -0.58325005 -0.6306143 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.72084403 -0.35266578  0.0271414 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.34893587 -0.61713564  0.1551776 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.01182713  0.7101529  -0.8027413 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.32465297  0.6331237  -0.62937236]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.7225913  -0.49491465  0.1664567 ]]\n",
      "Episode 25: tensor([5.0972])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.8109671   0.61754614 -0.80142117]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.91878045  0.5354058  -0.74253666]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.8956816   0.04449124 -0.78513545]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.717003    0.19409026 -0.52122414]]\n",
      "Episode 26: tensor([2.0279])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9125255  -0.7826263   0.43439624]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.6680133  -0.4312089  -0.14698401]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.9876127  -0.60771316 -0.68925166]]\n",
      "Episode 27: tensor([1.5391])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.88487804  0.65736854 -0.75598514]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.7209062   0.89417386 -0.57247466]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.42917848  0.28059033 -0.32975677]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.37379146  0.8916508  -0.7735931 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.7865628  0.5675734  0.8705825]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.01975712 0.12928966 0.10004562]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.21566848 -0.6692576  -0.3637269 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.6742022   0.63361496 -0.9729571 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.4487156  -0.46603167 -0.02653681]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.6176264  -0.06711683  0.47738057]]\n",
      "Episode 28: tensor([5.1090])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.7752589 -0.8544935 -0.4885661]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.88730973 -0.37427077 -0.7398918 ]]\n",
      "Episode 29: tensor([1.0227])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.9218493  0.20867573 0.42290077]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.4686208  -0.49591333  0.8216314 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.808481   -0.51968265 -0.8561702 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.2723127 0.6991709 0.2307394]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.36639407  0.3009771  -0.59551454]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.01853304 0.3198017  0.91251314]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.14943431  0.8186443  -0.543312  ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.8314202  -0.15344912  0.9711284 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.10969789 0.5851382  0.83369666]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.15709007 -0.7667664  -0.7823454 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.53244483  0.16857103  0.93401295]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.8252443  -0.77305776  0.6256421 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.8669842 -0.8108938 -0.7873453]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.5309542  -0.90873116 -0.9661803 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.5108838   0.14052969 -0.73478025]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.03347772  0.54818934 -0.9196539 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.60719866  0.32513365 -0.4657665 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.7479582  -0.16186617  0.3703103 ]]\n",
      "Episode 30: tensor([9.1966])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.88269943 -0.9030627   0.1902196 ]]\n",
      "Episode 31: tensor([0.5137])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9783049  -0.3647736   0.75784004]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.5814748 -0.936389  -0.3419835]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.56501746  0.6202943  -0.78273726]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.82808226 -0.9014816   0.9813825 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.6406293  -0.8229641  -0.05245674]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.7280882  -0.30896103  0.27444246]]\n",
      "Episode 32: tensor([3.0699])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.26222885  0.88440794  0.9855776 ]]\n",
      "Episode 33: tensor([0.5142])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.1531474  -0.22357629  0.8185562 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.47945678 -0.71580577 -0.57525605]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.6214455  -0.26292375  0.73100704]]\n",
      "Episode 34: tensor([1.5384])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.89882743  0.39898375  0.7879453 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.24386272  0.9156174  -0.62887263]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.16273278 0.8401572  0.8268385 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.84539443 -0.8743325  -0.08570228]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.5978655  -0.91078883  0.16291934]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.22365373 -0.23653525  0.9220421 ]]\n",
      "Episode 35: tensor([3.0623])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.1860393   0.03991514  0.47890505]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.19554347 -0.9496788   0.39371723]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.09587226 0.40019712 0.86929697]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.6603668   0.06261931 -0.13585924]]\n",
      "Episode 36: tensor([2.0582])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.74091434 0.92936    0.56364894]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.2592355   0.30015522 -0.7488495 ]]\n",
      "Episode 37: tensor([1.0257])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.79824066 -0.28309855  0.9454654 ]]\n",
      "Episode 38: tensor([0.5135])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.3169212  -0.77974534 -0.290515  ]]\n",
      "Episode 39: tensor([0.5141])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.7082536  -0.5496502   0.88586843]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.7224442  -0.03420097  0.28846332]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.20093238  0.13602124  0.5394724 ]]\n",
      "Episode 40: tensor([1.5279])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.44248462 -0.45986688  0.76218283]]\n",
      "Episode 41: tensor([0.5137])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.87703043 -0.9572188  -0.6425803 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.68692696 -0.53936756  0.5268527 ]]\n",
      "Episode 42: tensor([1.0197])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.16035464 0.26843566 0.9203839 ]]\n",
      "Episode 43: tensor([0.5143])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.02436844 -0.34370598  0.12155226]]\n",
      "Episode 44: tensor([0.5140])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.14956202 -0.98668313  0.86932737]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9476647 -0.933235   0.9973372]]\n",
      "Episode 45: tensor([1.0211])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.7917003  -0.13018905  0.961507  ]]\n",
      "Episode 46: tensor([0.5142])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.6308968  -0.8191885   0.98026973]]\n",
      "Episode 47: tensor([0.5136])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.08724547 -0.9192499   0.9807303 ]]\n",
      "Episode 48: tensor([0.5136])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.5797395   0.806718    0.98025733]]\n",
      "Episode 49: tensor([0.5139])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.7408037  -0.9693401   0.81299156]]\n",
      "Episode 50: tensor([0.5136])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.5899982  0.17105936 0.4700546 ]]\n",
      "Episode 51: tensor([0.5133])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.46883395 -0.8169085  -0.37088296]]\n",
      "Episode 52: tensor([0.5138])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[-0.24157755  0.19290406  0.9328401 ]]\n",
      "Episode 53: tensor([0.5145])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9818997 -0.5313434  0.9389978]]\n",
      "Episode 54: tensor([0.5138])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.12405957 -0.9988374   0.85203046]]\n",
      "Episode 55: tensor([0.5142])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[0.87659407 0.06079201 0.97567075]]\n",
      "Episode 56: tensor([0.5137])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.92765445 -0.94828826  0.90844357]]\n",
      "Episode 57: tensor([0.5137])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9270866  -0.94806373  0.96202743]]\n",
      "Episode 58: tensor([0.5137])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.7017396 -0.918005   0.9203424]]\n",
      "Episode 59: tensor([0.5132])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.7346922 -0.6528621  0.9992989]]\n",
      "Episode 60: tensor([0.5140])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.8852566  -0.93930024  0.9599202 ]]\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.6954172  -0.99188393  0.9980204 ]]\n",
      "Episode 61: tensor([1.0184])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.92726487 -0.98670995  0.9648737 ]]\n",
      "Episode 62: tensor([0.5134])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9154043  -0.9227224   0.52096593]]\n",
      "Episode 63: tensor([0.5133])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.98183364 -0.8382291   0.99901587]]\n",
      "Episode 64: tensor([0.5141])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.5384225  -0.87302583  0.99074215]]\n",
      "Episode 65: tensor([0.5141])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.8441292 -0.9981273  0.9533315]]\n",
      "Episode 66: tensor([0.5138])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9785387  -0.99253875  0.972728  ]]\n",
      "Episode 67: tensor([0.5142])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9763717  -0.64992756  0.99984443]]\n",
      "Episode 68: tensor([0.5139])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99612296 -0.9659444   0.99668235]]\n",
      "Episode 69: tensor([0.5140])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9961716 -0.9923168  0.9857892]]\n",
      "Episode 70: tensor([0.5134])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.09445039 -0.42224458  0.9996361 ]]\n",
      "Episode 71: tensor([0.5136])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9767267  -0.99992234  0.9807173 ]]\n",
      "Episode 72: tensor([0.5140])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.93634945 -0.96606314  0.9579505 ]]\n",
      "Episode 73: tensor([0.5140])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.98737085 -0.99863696  0.9995225 ]]\n",
      "Episode 74: tensor([0.5137])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9904587  -0.99795216  0.9990311 ]]\n",
      "Episode 75: tensor([0.5141])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9938337  -0.99934644  0.96066856]]\n",
      "Episode 76: tensor([0.5135])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9969959 -0.9903728  0.9508935]]\n",
      "Episode 77: tensor([0.5138])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9958583 -0.9995409  0.9990088]]\n",
      "Episode 78: tensor([0.5138])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.979068  -0.9937299  0.9951213]]\n",
      "Episode 79: tensor([0.5139])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9482642  -0.9998187   0.99099565]]\n",
      "Episode 80: tensor([0.5143])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.95563906 -0.83817434  0.9991815 ]]\n",
      "Episode 81: tensor([0.5144])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.56263995 -0.78405374  0.9941193 ]]\n",
      "Episode 82: tensor([0.5136])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9350061  -0.9828811   0.99994767]]\n",
      "Episode 83: tensor([0.5136])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.98112774 -0.9998903   0.6528911 ]]\n",
      "Episode 84: tensor([0.5133])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99971265 -0.994184    0.99995166]]\n",
      "Episode 85: tensor([0.5131])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.96159375 -0.97133666  0.9986775 ]]\n",
      "Episode 86: tensor([0.5145])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99601173 -0.6126114   0.9675823 ]]\n",
      "Episode 87: tensor([0.5142])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9899807  -0.97079957  0.99982953]]\n",
      "Episode 88: tensor([0.5138])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9952014  -0.9908516   0.25983936]]\n",
      "Episode 89: tensor([0.5141])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9986973  -0.99745667  0.8978687 ]]\n",
      "Episode 90: tensor([0.5135])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99746996 -0.99841297  0.99845946]]\n",
      "Episode 91: tensor([0.5142])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.98637235 -0.99176466  0.99941015]]\n",
      "Episode 92: tensor([0.5141])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.97646224 -0.9402264   0.9957083 ]]\n",
      "Episode 93: tensor([0.5136])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999257  -0.9998512   0.99893874]]\n",
      "Episode 94: tensor([0.5142])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9989985 -0.9972412  0.9986339]]\n",
      "Episode 95: tensor([0.5143])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99476916 -0.994682    0.97668135]]\n",
      "Episode 96: tensor([0.5137])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99618423 -0.9991032   0.9994459 ]]\n",
      "Episode 97: tensor([0.5142])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.83318233 -0.9999456   0.9997446 ]]\n",
      "Episode 98: tensor([0.5136])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99859387 -0.75823236  0.9996951 ]]\n",
      "Episode 99: tensor([0.5136])\n",
      "Epoch :  1\n",
      "loss_IOC :  -0.22700177438886454\n",
      "loss_IOC :  -0.2271203162014936\n",
      "loss_IOC :  -0.22835603856591397\n",
      "loss_IOC :  -0.2283039285434872\n",
      "loss_IOC :  -0.2276037728190583\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99775875 -0.99957776  0.9938888 ]]\n",
      "Episode 0: tensor([0.5123])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9095113  -0.99958795  0.9993995 ]]\n",
      "Episode 1: tensor([0.5127])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.95475554 -0.999283    0.9995438 ]]\n",
      "Episode 2: tensor([0.5124])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99993414 -0.99992734  0.9974483 ]]\n",
      "Episode 3: tensor([0.5117])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999861 -0.9984032  0.9153188]]\n",
      "Episode 4: tensor([0.5120])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9996499 -0.9998951  0.9999669]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5: tensor([0.5131])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9944286  -0.999712    0.99971646]]\n",
      "Episode 6: tensor([0.5123])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999672  -0.99670464  0.9998429 ]]\n",
      "Episode 7: tensor([0.5125])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9979926 -0.9922857  0.9848746]]\n",
      "Episode 8: tensor([0.5124])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9953716 -0.9953525  0.9998549]]\n",
      "Episode 9: tensor([0.5120])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9998727  -0.99875665  0.99409807]]\n",
      "Episode 10: tensor([0.5128])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99985707 -0.9929304   0.9060419 ]]\n",
      "Episode 11: tensor([0.5128])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9829028  -0.99565136  0.993939  ]]\n",
      "Episode 12: tensor([0.5126])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99940103 -0.9956479   0.9999912 ]]\n",
      "Episode 13: tensor([0.5131])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99904734 -0.99989694  0.9983335 ]]\n",
      "Episode 14: tensor([0.5121])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9981553  -0.9999321   0.99729717]]\n",
      "Episode 15: tensor([0.5128])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9955923 -0.9955813  0.9947633]]\n",
      "Episode 16: tensor([0.5120])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99990547 -0.9998264   0.9990169 ]]\n",
      "Episode 17: tensor([0.5131])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.98983276 -0.99889565  0.9998886 ]]\n",
      "Episode 18: tensor([0.5125])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99733424 -0.99498993  0.9997391 ]]\n",
      "Episode 19: tensor([0.5120])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9998124 -0.9925025  0.999792 ]]\n",
      "Episode 20: tensor([0.5120])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9969477  -0.99979824  0.9971595 ]]\n",
      "Episode 21: tensor([0.5123])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9989198  -0.9998299   0.99998295]]\n",
      "Episode 22: tensor([0.5124])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99980354 -0.9925908   0.9998816 ]]\n",
      "Episode 23: tensor([0.5126])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9932393  -0.93543625  0.9990916 ]]\n",
      "Episode 24: tensor([0.5123])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99976116 -0.9997734   0.99999714]]\n",
      "Episode 25: tensor([0.5125])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9998183  -0.99987495  0.9996619 ]]\n",
      "Episode 26: tensor([0.5128])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.998903   -0.9976348   0.99993956]]\n",
      "Episode 27: tensor([0.5122])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99878615 -0.9996105   0.9979989 ]]\n",
      "Episode 28: tensor([0.5127])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99997705 -0.9998459   0.9996729 ]]\n",
      "Episode 29: tensor([0.5119])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9992346  -0.99958175  0.99803334]]\n",
      "Episode 30: tensor([0.5122])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99995387 -0.9997907   0.9999686 ]]\n",
      "Episode 31: tensor([0.5124])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9998699 -0.9998463  0.9990857]]\n",
      "Episode 32: tensor([0.5125])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99914145 -0.9997556   0.9992129 ]]\n",
      "Episode 33: tensor([0.5128])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9973053 -0.9967923  0.9981379]]\n",
      "Episode 34: tensor([0.5129])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9998973  -0.9980566   0.99925447]]\n",
      "Episode 35: tensor([0.5126])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99997693 -0.99869055  0.999999  ]]\n",
      "Episode 36: tensor([0.5126])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99461055 -0.9999436   0.9999145 ]]\n",
      "Episode 37: tensor([0.5125])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9972222 -0.9934454  0.9989493]]\n",
      "Episode 38: tensor([0.5119])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9972929  -0.99964124  0.99984246]]\n",
      "Episode 39: tensor([0.5127])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99941874 -0.99982953  0.9998706 ]]\n",
      "Episode 40: tensor([0.5123])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99728644 -0.9999665   0.99988073]]\n",
      "Episode 41: tensor([0.5126])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99950975 -0.9996147   0.99952394]]\n",
      "Episode 42: tensor([0.5125])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999569  -0.99958813  0.9999627 ]]\n",
      "Episode 43: tensor([0.5129])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99519414 -0.9999947   0.9999731 ]]\n",
      "Episode 44: tensor([0.5117])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9969963  -0.99994534  0.9999634 ]]\n",
      "Episode 45: tensor([0.5131])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9995833  -0.99775565  0.9999784 ]]\n",
      "Episode 46: tensor([0.5127])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9962764  -0.99038327  0.9992196 ]]\n",
      "Episode 47: tensor([0.5126])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999097  -0.99998516  0.99976647]]\n",
      "Episode 48: tensor([0.5130])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9968057  -0.99767494  0.9997507 ]]\n",
      "Episode 49: tensor([0.5126])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9996715  -0.99995416  0.9996502 ]]\n",
      "Episode 50: tensor([0.5128])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9976567  -0.99988544  0.9997872 ]]\n",
      "Episode 51: tensor([0.5117])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999538 -0.9994157  0.9999266]]\n",
      "Episode 52: tensor([0.5123])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999603  -0.9999831   0.99892884]]\n",
      "Episode 53: tensor([0.5119])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9996685 -0.9994617  0.9999772]]\n",
      "Episode 54: tensor([0.5126])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9998402  -0.9999965   0.99798054]]\n",
      "Episode 55: tensor([0.5123])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9993274 -0.9988928  0.999971 ]]\n",
      "Episode 56: tensor([0.5121])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999924  -0.9973362   0.99964154]]\n",
      "Episode 57: tensor([0.5120])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99994123 -0.99927473  0.9936782 ]]\n",
      "Episode 58: tensor([0.5122])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9865174  -0.99877656  0.9998076 ]]\n",
      "Episode 59: tensor([0.5122])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99990374 -0.99779975  0.9996554 ]]\n",
      "Episode 60: tensor([0.5121])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99896455 -0.9997448   0.9999868 ]]\n",
      "Episode 61: tensor([0.5122])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99958795 -0.99946576  0.9999433 ]]\n",
      "Episode 62: tensor([0.5116])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99958575 -0.999653    0.9999627 ]]\n",
      "Episode 63: tensor([0.5122])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99978817 -0.99928087  0.9994206 ]]\n",
      "Episode 64: tensor([0.5130])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99806386 -0.9980061   0.9980567 ]]\n",
      "Episode 65: tensor([0.5118])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99983776 -0.99998224  0.9996545 ]]\n",
      "Episode 66: tensor([0.5121])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9992689  -0.99991035  0.999205  ]]\n",
      "Episode 67: tensor([0.5120])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9984047 -0.9991436  0.9999882]]\n",
      "Episode 68: tensor([0.5122])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.999297   -0.99771935  0.9997588 ]]\n",
      "Episode 69: tensor([0.5120])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.92184687 -0.999871    0.997864  ]]\n",
      "Episode 70: tensor([0.5124])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9998815  -0.9980566   0.99857676]]\n",
      "Episode 71: tensor([0.5126])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999203 -0.9999968  0.9999823]]\n",
      "Episode 72: tensor([0.5128])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99997467 -0.9998638   0.9919693 ]]\n",
      "Episode 73: tensor([0.5128])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9981611  -0.99998134  0.9999942 ]]\n",
      "Episode 74: tensor([0.5119])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99971646 -0.9980427   0.99967927]]\n",
      "Episode 75: tensor([0.5130])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99988574 -0.9991583   0.9999065 ]]\n",
      "Episode 76: tensor([0.5127])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.999986  -0.9993549  0.9999208]]\n",
      "Episode 77: tensor([0.5124])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99989057 -0.9887018   0.99978876]]\n",
      "Episode 78: tensor([0.5127])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99950993 -0.9987337   0.9999867 ]]\n",
      "Episode 79: tensor([0.5130])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99753475 -0.9998111   0.99982804]]\n",
      "Episode 80: tensor([0.5120])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9991797  -0.99952537  0.9998414 ]]\n",
      "Episode 81: tensor([0.5130])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99978834 -0.99999934  0.99996465]]\n",
      "Episode 82: tensor([0.5122])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99995375 -0.9982539   0.9995432 ]]\n",
      "Episode 83: tensor([0.5122])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9997331  -0.99702096  0.9985463 ]]\n",
      "Episode 84: tensor([0.5120])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999156  -0.999985    0.99982154]]\n",
      "Episode 85: tensor([0.5126])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999255 -0.9998678   0.9999379 ]]\n",
      "Episode 86: tensor([0.5117])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999723  -0.99964833  0.9977831 ]]\n",
      "Episode 87: tensor([0.5123])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999938 -0.9990356  0.9999883]]\n",
      "Episode 88: tensor([0.5127])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99985707 -0.99837834  0.9999151 ]]\n",
      "Episode 89: tensor([0.5124])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9996296  -0.99996656  0.9997149 ]]\n",
      "Episode 90: tensor([0.5128])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99845237 -0.9999806   0.9997565 ]]\n",
      "Episode 91: tensor([0.5127])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9997809  -0.99998415  0.99993604]]\n",
      "Episode 92: tensor([0.5124])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9995281 -0.9981076  0.9999581]]\n",
      "Episode 93: tensor([0.5121])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9996462  -0.98985076  0.99977976]]\n",
      "Episode 94: tensor([0.5130])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9996617 -0.9999298  0.998904 ]]\n",
      "Episode 95: tensor([0.5126])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9997016 -0.999661   0.9999004]]\n",
      "Episode 96: tensor([0.5121])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999436  -0.9981871   0.99998856]]\n",
      "Episode 97: tensor([0.5122])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.98513675 -0.9997582   0.9999779 ]]\n",
      "Episode 98: tensor([0.5128])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999446 -0.99992687  0.9999541 ]]\n",
      "Episode 99: tensor([0.5129])\n",
      "Epoch :  2\n",
      "loss_IOC :  -0.22679940378754737\n",
      "loss_IOC :  -0.2275813457950211\n",
      "loss_IOC :  -0.2266066025173581\n",
      "loss_IOC :  -0.22757342310566964\n",
      "loss_IOC :  -0.2284658340832833\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99991924 -0.99976075  0.9999075 ]]\n",
      "Episode 0: tensor([0.5112])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99882543 -0.9970946   0.9998851 ]]\n",
      "Episode 1: tensor([0.5109])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99856013 -0.9998883   0.99996805]]\n",
      "Episode 2: tensor([0.5109])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999836  -0.9995847   0.99999815]]\n",
      "Episode 3: tensor([0.5111])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9998913  -0.9999951   0.99965733]]\n",
      "Episode 4: tensor([0.5108])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99957675 -0.9989122   0.99961746]]\n",
      "Episode 5: tensor([0.5107])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999874 -0.9998369  0.9999332]]\n",
      "Episode 6: tensor([0.5106])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9987189  -0.99983543  0.99989295]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7: tensor([0.5108])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99979097 -0.99630815  0.9999737 ]]\n",
      "Episode 8: tensor([0.5111])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999291  -0.9999796   0.99953705]]\n",
      "Episode 9: tensor([0.5111])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9993543 -0.999166   0.9984508]]\n",
      "Episode 10: tensor([0.5114])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99988425 -0.9965942   0.9987139 ]]\n",
      "Episode 11: tensor([0.5115])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9998405 -0.9999221  0.999797 ]]\n",
      "Episode 12: tensor([0.5113])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99997205 -0.999866    0.99997646]]\n",
      "Episode 13: tensor([0.5111])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9997829 -0.9995422  0.9999795]]\n",
      "Episode 14: tensor([0.5109])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999957  -0.99999154  0.9997292 ]]\n",
      "Episode 15: tensor([0.5111])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9996643 -0.9999771  0.9998709]]\n",
      "Episode 16: tensor([0.5118])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9816731  -0.99999857  0.99998003]]\n",
      "Episode 17: tensor([0.5108])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.999902   -0.99850696  0.999954  ]]\n",
      "Episode 18: tensor([0.5107])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9969586  -0.9999903   0.99991345]]\n",
      "Episode 19: tensor([0.5108])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99998116 -0.9984758   0.9998806 ]]\n",
      "Episode 20: tensor([0.5106])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99977094 -0.99995416  0.9999612 ]]\n",
      "Episode 21: tensor([0.5115])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999905 -0.9999734  0.9999848]]\n",
      "Episode 22: tensor([0.5110])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99900454 -0.99657786  0.99978024]]\n",
      "Episode 23: tensor([0.5106])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99996436 -0.99995786  0.9998744 ]]\n",
      "Episode 24: tensor([0.5108])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9855807  -0.99999976  0.9998231 ]]\n",
      "Episode 25: tensor([0.5116])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9996953  -0.99997455  0.99999267]]\n",
      "Episode 26: tensor([0.5118])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9990506  -0.95715886  0.99980503]]\n",
      "Episode 27: tensor([0.5110])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99946046 -0.9974259   0.9999826 ]]\n",
      "Episode 28: tensor([0.5107])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9992739  -0.99998766  0.9999232 ]]\n",
      "Episode 29: tensor([0.5110])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99974597 -0.9999923   0.99999374]]\n",
      "Episode 30: tensor([0.5109])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99850595 -0.99997646  0.9999979 ]]\n",
      "Episode 31: tensor([0.5114])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9998737  -0.9999751   0.99995863]]\n",
      "Episode 32: tensor([0.5111])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999114  -0.99902797  0.9999723 ]]\n",
      "Episode 33: tensor([0.5107])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999843  -0.99989295  0.99998987]]\n",
      "Episode 34: tensor([0.5110])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999841  -0.9999858   0.99992245]]\n",
      "Episode 35: tensor([0.5113])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99991107 -0.9996046   0.9999956 ]]\n",
      "Episode 36: tensor([0.5108])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999123  -0.99961925  0.99996203]]\n",
      "Episode 37: tensor([0.5116])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99749154 -0.99991405  0.999995  ]]\n",
      "Episode 38: tensor([0.5107])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99967325 -0.9998913   0.9999926 ]]\n",
      "Episode 39: tensor([0.5110])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99996996 -0.9999733   0.99955213]]\n",
      "Episode 40: tensor([0.5117])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99959946 -0.9999899   0.99990666]]\n",
      "Episode 41: tensor([0.5111])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99997807 -0.9999413   0.9995083 ]]\n",
      "Episode 42: tensor([0.5102])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9992237  -0.9999052   0.99997854]]\n",
      "Episode 43: tensor([0.5112])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99552625 -0.99998635  0.99747986]]\n",
      "Episode 44: tensor([0.5105])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9987478  -0.9999483   0.99999964]]\n",
      "Episode 45: tensor([0.5117])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999889 -0.9999692  0.9999949]]\n",
      "Episode 46: tensor([0.5107])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999226  -0.99983144  0.99991596]]\n",
      "Episode 47: tensor([0.5110])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99969393 -0.9998165   0.9999366 ]]\n",
      "Episode 48: tensor([0.5109])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9995384  -0.99993557  0.9999404 ]]\n",
      "Episode 49: tensor([0.5112])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9998032  -0.99999833  0.99999124]]\n",
      "Episode 50: tensor([0.5116])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99998873 -0.99987406  0.9998435 ]]\n",
      "Episode 51: tensor([0.5108])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999396  -0.9999919   0.99973464]]\n",
      "Episode 52: tensor([0.5112])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.999912  -0.9993828  0.9999996]]\n",
      "Episode 53: tensor([0.5121])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999271 -0.9992842  0.9999882]]\n",
      "Episode 54: tensor([0.5109])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99942255 -0.9997973   0.9999817 ]]\n",
      "Episode 55: tensor([0.5114])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.999907   -0.99998605  0.99824804]]\n",
      "Episode 56: tensor([0.5111])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99958396 -0.99945915  0.9999999 ]]\n",
      "Episode 57: tensor([0.5110])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999509  -0.99984753  0.99996644]]\n",
      "Episode 58: tensor([0.5111])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99995756 -0.99998814  0.9999275 ]]\n",
      "Episode 59: tensor([0.5106])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999719 -0.9999941  0.9992765]]\n",
      "Episode 60: tensor([0.5113])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99996    -0.99998504  0.9999988 ]]\n",
      "Episode 61: tensor([0.5104])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999917 -0.9998258   0.99999386]]\n",
      "Episode 62: tensor([0.5103])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99998915 -0.9997248   0.9999954 ]]\n",
      "Episode 63: tensor([0.5106])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999897 -0.9999437  0.999998 ]]\n",
      "Episode 64: tensor([0.5111])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999963 -0.9991418  0.99999  ]]\n",
      "Episode 65: tensor([0.5110])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999636 -0.99996674  0.9999932 ]]\n",
      "Episode 66: tensor([0.5113])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9997661  -0.99892956  0.9999965 ]]\n",
      "Episode 67: tensor([0.5114])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999392 -0.9980891  0.9999765]]\n",
      "Episode 68: tensor([0.5111])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99995863 -0.99943364  0.99987215]]\n",
      "Episode 69: tensor([0.5108])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9998106  -0.99796927  0.99987954]]\n",
      "Episode 70: tensor([0.5106])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999828  -0.99992996  0.9996338 ]]\n",
      "Episode 71: tensor([0.5102])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9998938  -0.99998564  0.99999183]]\n",
      "Episode 72: tensor([0.5111])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9997393  -0.99985623  0.9998542 ]]\n",
      "Episode 73: tensor([0.5108])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999511 -0.9999967  0.9999978]]\n",
      "Episode 74: tensor([0.5106])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9997542 -0.9999896  0.9999918]]\n",
      "Episode 75: tensor([0.5112])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9986211 -0.9999357  0.9999947]]\n",
      "Episode 76: tensor([0.5102])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999658  -0.99997836  0.9999999 ]]\n",
      "Episode 77: tensor([0.5113])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999267 -0.99995816  0.99999475]]\n",
      "Episode 78: tensor([0.5109])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999866 -0.9979621  0.9999345]]\n",
      "Episode 79: tensor([0.5107])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999615  -0.9999938   0.99997216]]\n",
      "Episode 80: tensor([0.5113])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999925  -0.9999281   0.99992317]]\n",
      "Episode 81: tensor([0.5107])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999917 -0.9999815   0.99999523]]\n",
      "Episode 82: tensor([0.5106])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999957  -0.99996376  0.9999541 ]]\n",
      "Episode 83: tensor([0.5112])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999847  -0.9999367   0.99817044]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 84: tensor([0.5111])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999783 -0.9999758  0.9999976]]\n",
      "Episode 85: tensor([0.5111])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9998762  -0.99999547  0.99999815]]\n",
      "Episode 86: tensor([0.5104])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999739  -0.9997745   0.99999785]]\n",
      "Episode 87: tensor([0.5112])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999791  -0.99999774  0.9999932 ]]\n",
      "Episode 88: tensor([0.5112])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999629 -0.9999984  0.9999541]]\n",
      "Episode 89: tensor([0.5102])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999166 -0.999987    0.9997107 ]]\n",
      "Episode 90: tensor([0.5107])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999917 -0.99997073  0.9999881 ]]\n",
      "Episode 91: tensor([0.5112])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9998788  -0.9999959   0.99998623]]\n",
      "Episode 92: tensor([0.5113])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9996762 -0.9999109  0.9999955]]\n",
      "Episode 93: tensor([0.5113])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999879  -0.9998939   0.99998885]]\n",
      "Episode 94: tensor([0.5110])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.999797   -0.9999957   0.99998444]]\n",
      "Episode 95: tensor([0.5110])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999989 -0.999905   0.9999834]]\n",
      "Episode 96: tensor([0.5109])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999094 -0.99999666  0.9999978 ]]\n",
      "Episode 97: tensor([0.5114])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999957  -0.99989575  0.99999756]]\n",
      "Episode 98: tensor([0.5108])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999931  -0.99999505  0.99999726]]\n",
      "Episode 99: tensor([0.5112])\n",
      "Epoch :  3\n",
      "loss_IOC :  -0.22860907157544347\n",
      "loss_IOC :  -0.22811975646270344\n",
      "loss_IOC :  -0.22738435326709355\n",
      "loss_IOC :  -0.23080583249063658\n",
      "loss_IOC :  -0.22881879870840516\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999134  -0.9999862   0.99999565]]\n",
      "Episode 0: tensor([0.5093])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999058  -0.9999967   0.99998385]]\n",
      "Episode 1: tensor([0.5098])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99998784 -0.99987     0.9999584 ]]\n",
      "Episode 2: tensor([0.5099])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999213 -0.9999986   0.99999535]]\n",
      "Episode 3: tensor([0.5092])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99983394 -0.9999928   0.9999647 ]]\n",
      "Episode 4: tensor([0.5093])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999984  -0.9999674   0.99999917]]\n",
      "Episode 5: tensor([0.5085])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999917 -0.9999937   0.99999887]]\n",
      "Episode 6: tensor([0.5101])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99993926 -0.99952775  0.99999946]]\n",
      "Episode 7: tensor([0.5097])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999981  -0.99981517  0.99997437]]\n",
      "Episode 8: tensor([0.5091])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999935 -0.9999628  0.9999993]]\n",
      "Episode 9: tensor([0.5094])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9998102  -0.99999934  0.99999475]]\n",
      "Episode 10: tensor([0.5099])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999956  -0.99994785  0.99999243]]\n",
      "Episode 11: tensor([0.5094])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999905 -0.99995905  0.9999362 ]]\n",
      "Episode 12: tensor([0.5096])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999972 -0.999984   0.9999336]]\n",
      "Episode 13: tensor([0.5090])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999983 -0.9998575  0.9999985]]\n",
      "Episode 14: tensor([0.5091])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99997056 -0.9999852   0.9999929 ]]\n",
      "Episode 15: tensor([0.5097])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99967575 -0.9999755   0.99990714]]\n",
      "Episode 16: tensor([0.5094])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999833 -0.9999966   0.9999995 ]]\n",
      "Episode 17: tensor([0.5094])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999714 -0.9999934   0.9999972 ]]\n",
      "Episode 18: tensor([0.5096])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999743   0.9999996 ]]\n",
      "Episode 19: tensor([0.5092])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999905 -0.99998087  0.99997693]]\n",
      "Episode 20: tensor([0.5097])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999976 -0.9999988   0.9999997 ]]\n",
      "Episode 21: tensor([0.5096])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99992263 -0.9999853   0.9999898 ]]\n",
      "Episode 22: tensor([0.5094])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999968  -0.99986184  0.9999995 ]]\n",
      "Episode 23: tensor([0.5094])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999819 -0.9999637  0.9999997]]\n",
      "Episode 24: tensor([0.5093])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999983 -0.9999684  0.9999966]]\n",
      "Episode 25: tensor([0.5094])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999996  -0.99965644  0.99950933]]\n",
      "Episode 26: tensor([0.5092])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99998367 -0.99999994  0.99999976]]\n",
      "Episode 27: tensor([0.5094])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999803 -0.99998677  0.99999565]]\n",
      "Episode 28: tensor([0.5092])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99997586 -0.99985945  0.99999577]]\n",
      "Episode 29: tensor([0.5090])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99998426 -0.99994206  0.9999669 ]]\n",
      "Episode 30: tensor([0.5097])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999948  -0.9999815   0.99998254]]\n",
      "Episode 31: tensor([0.5095])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99997514 -0.9999907   0.99998236]]\n",
      "Episode 32: tensor([0.5098])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999875 -0.9998492   0.99999994]]\n",
      "Episode 33: tensor([0.5101])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999917 -0.9999979   0.99998724]]\n",
      "Episode 34: tensor([0.5090])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99998343 -0.999996    0.99999756]]\n",
      "Episode 35: tensor([0.5095])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999716 -0.9999982  0.9999984]]\n",
      "Episode 36: tensor([0.5090])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99995875 -0.9999981   0.99999034]]\n",
      "Episode 37: tensor([0.5093])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999994  -0.99998116  0.9999992 ]]\n",
      "Episode 38: tensor([0.5097])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999555  -0.99974316  0.99981064]]\n",
      "Episode 39: tensor([0.5091])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999756 -0.99989533  0.9999994 ]]\n",
      "Episode 40: tensor([0.5089])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999929  0.9999945]]\n",
      "Episode 41: tensor([0.5087])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999636 -0.9999991   0.9999681 ]]\n",
      "Episode 42: tensor([0.5089])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999756 -0.99997294  0.99999636]]\n",
      "Episode 43: tensor([0.5094])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999817  -0.99999917  0.9999959 ]]\n",
      "Episode 44: tensor([0.5097])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999997 -0.9999699  0.9999905]]\n",
      "Episode 45: tensor([0.5088])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999987  -0.99999225  0.9997569 ]]\n",
      "Episode 46: tensor([0.5101])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999918  -0.99999547  0.9999999 ]]\n",
      "Episode 47: tensor([0.5099])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999297 -0.9999986   0.9999999 ]]\n",
      "Episode 48: tensor([0.5094])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999344 -0.9999782   0.9999926 ]]\n",
      "Episode 49: tensor([0.5092])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999995 -0.9999995  0.9999979]]\n",
      "Episode 50: tensor([0.5095])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999612  -0.99999607  0.9999982 ]]\n",
      "Episode 51: tensor([0.5091])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99998325 -0.999999    0.99999994]]\n",
      "Episode 52: tensor([0.5097])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999958 -0.9999939  0.9999992]]\n",
      "Episode 53: tensor([0.5100])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999998 -0.9999985  0.9999995]]\n",
      "Episode 54: tensor([0.5097])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999995 -0.9999935  0.9999867]]\n",
      "Episode 55: tensor([0.5097])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99995637 -0.9999933   0.99999756]]\n",
      "Episode 56: tensor([0.5096])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999995 -0.9999987  0.9999856]]\n",
      "Episode 57: tensor([0.5102])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999992 -0.9999995  0.999999 ]]\n",
      "Episode 58: tensor([0.5092])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999064 -0.99998784  0.999984  ]]\n",
      "Episode 59: tensor([0.5101])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999774 -0.9999653   0.99999976]]\n",
      "Episode 60: tensor([0.5092])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999917 -0.9999924   0.9999965 ]]\n",
      "Episode 61: tensor([0.5096])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999968  -0.99998975  1.        ]]\n",
      "Episode 62: tensor([0.5086])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99997145 -0.99999905  0.99999994]]\n",
      "Episode 63: tensor([0.5104])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999849  -0.99999964  0.9999999 ]]\n",
      "Episode 64: tensor([0.5101])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999961  -0.9999895   0.99999976]]\n",
      "Episode 65: tensor([0.5092])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999243 -0.999981    0.9999991 ]]\n",
      "Episode 66: tensor([0.5100])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999866 -0.9999944  0.9999917]]\n",
      "Episode 67: tensor([0.5092])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999985  -0.99999756  0.9999933 ]]\n",
      "Episode 68: tensor([0.5093])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999684 -0.99999887  0.9999997 ]]\n",
      "Episode 69: tensor([0.5102])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999803 -0.9999934   0.99999946]]\n",
      "Episode 70: tensor([0.5095])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999969  -0.99997175  0.99993825]]\n",
      "Episode 71: tensor([0.5097])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99972403 -0.9999929   0.99996233]]\n",
      "Episode 72: tensor([0.5098])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999812 -0.9999996  0.9999998]]\n",
      "Episode 73: tensor([0.5095])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999946 -0.9999975   0.9999999 ]]\n",
      "Episode 74: tensor([0.5095])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999  -0.99999905  0.9999984 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 75: tensor([0.5097])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999845  -0.9999989   0.99999636]]\n",
      "Episode 76: tensor([0.5094])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999215  -0.99998915  0.9999949 ]]\n",
      "Episode 77: tensor([0.5098])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99977225  0.9999941 ]]\n",
      "Episode 78: tensor([0.5098])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99998736 -0.9994626   0.9999935 ]]\n",
      "Episode 79: tensor([0.5093])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999924 -0.9999255  0.9999981]]\n",
      "Episode 80: tensor([0.5100])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999975  -0.99999005  0.9999927 ]]\n",
      "Episode 81: tensor([0.5100])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999726  0.9999968 ]]\n",
      "Episode 82: tensor([0.5101])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999936 -0.9999999  0.9999876]]\n",
      "Episode 83: tensor([0.5095])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.999991    0.9999999 ]]\n",
      "Episode 84: tensor([0.5103])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999652   0.99999976]]\n",
      "Episode 85: tensor([0.5097])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999404 -0.99997437  0.99999994]]\n",
      "Episode 86: tensor([0.5093])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999833 -0.9999998   0.99999326]]\n",
      "Episode 87: tensor([0.5097])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999964 -0.999999    0.99999976]]\n",
      "Episode 88: tensor([0.5095])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999887 -0.9999924   0.9999999 ]]\n",
      "Episode 89: tensor([0.5095])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999998 -0.9999911  1.       ]]\n",
      "Episode 90: tensor([0.5094])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999774 -0.99999577  0.9999998 ]]\n",
      "Episode 91: tensor([0.5089])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999  -0.99998564  0.99999994]]\n",
      "Episode 92: tensor([0.5093])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999998   0.99999976]]\n",
      "Episode 93: tensor([0.5101])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999356 -0.999999    0.99999994]]\n",
      "Episode 94: tensor([0.5098])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999905 -0.99998957  0.99999875]]\n",
      "Episode 95: tensor([0.5096])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999998   0.99999964]]\n",
      "Episode 96: tensor([0.5095])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999696  0.9999991 ]]\n",
      "Episode 97: tensor([0.5091])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999997  -0.99998945  0.99999154]]\n",
      "Episode 98: tensor([0.5098])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999  -0.99999833  0.9999962 ]]\n",
      "Episode 99: tensor([0.5093])\n",
      "Epoch :  4\n",
      "loss_IOC :  -0.22702895959179475\n",
      "loss_IOC :  -0.22673078414277542\n",
      "loss_IOC :  -0.2292118813081187\n",
      "loss_IOC :  -0.22771569437818828\n",
      "loss_IOC :  -0.22923902198165153\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999803 -0.9999941   0.9999972 ]]\n",
      "Episode 0: tensor([0.5082])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999986 -0.9999996  0.999992 ]]\n",
      "Episode 1: tensor([0.5084])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999887  0.99999946]]\n",
      "Episode 2: tensor([0.5082])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999875 -0.9999993   0.99999934]]\n",
      "Episode 3: tensor([0.5083])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999964 -0.9999951   0.99999094]]\n",
      "Episode 4: tensor([0.5073])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.999527    0.9999981 ]]\n",
      "Episode 5: tensor([0.5083])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999993 -0.9999971  0.9999997]]\n",
      "Episode 6: tensor([0.5080])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999774 -0.99998766  0.99999976]]\n",
      "Episode 7: tensor([0.5084])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999857 -0.99999905  0.9999985 ]]\n",
      "Episode 8: tensor([0.5079])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999917 -0.99999976  0.9999962 ]]\n",
      "Episode 9: tensor([0.5086])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999845 -0.9999994   0.9999999 ]]\n",
      "Episode 10: tensor([0.5078])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999987  -0.9999912   0.99999994]]\n",
      "Episode 11: tensor([0.5084])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999997  -0.99999994  0.9999998 ]]\n",
      "Episode 12: tensor([0.5085])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999986  -0.9999736   0.99999815]]\n",
      "Episode 13: tensor([0.5086])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999803 -0.99999917  0.9999993 ]]\n",
      "Episode 14: tensor([0.5083])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999977  -0.99999994  1.        ]]\n",
      "Episode 15: tensor([0.5081])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999976  -0.99999905  0.9999996 ]]\n",
      "Episode 16: tensor([0.5081])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999964 -0.99999565  0.99999994]]\n",
      "Episode 17: tensor([0.5084])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999939  -0.999995    0.99999976]]\n",
      "Episode 18: tensor([0.5075])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999976  0.9999999 ]]\n",
      "Episode 19: tensor([0.5077])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999988 -0.9999981  0.9999956]]\n",
      "Episode 20: tensor([0.5087])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999714 -0.9999869   0.99999714]]\n",
      "Episode 21: tensor([0.5080])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999  -0.999997    0.99999934]]\n",
      "Episode 22: tensor([0.5080])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999965 -0.9999977  0.9999458]]\n",
      "Episode 23: tensor([0.5086])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  0.99999994]]\n",
      "Episode 24: tensor([0.5083])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999972  0.9999982]]\n",
      "Episode 25: tensor([0.5082])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999995 -0.9998607  1.       ]]\n",
      "Episode 26: tensor([0.5087])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.999999   -0.9999911   0.99999976]]\n",
      "Episode 27: tensor([0.5080])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999887 -0.99998885  0.9999998 ]]\n",
      "Episode 28: tensor([0.5081])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999994  -0.99997956  0.9999661 ]]\n",
      "Episode 29: tensor([0.5072])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999976  0.99999994]]\n",
      "Episode 30: tensor([0.5089])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999986 -0.9999976  0.9999994]]\n",
      "Episode 31: tensor([0.5084])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999984   0.99999523]]\n",
      "Episode 32: tensor([0.5084])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99998957  0.99999976]]\n",
      "Episode 33: tensor([0.5086])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999996  -0.99999976  0.99999917]]\n",
      "Episode 34: tensor([0.5079])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999917  0.99999964]]\n",
      "Episode 35: tensor([0.5082])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999946 -0.99999976  0.9999991 ]]\n",
      "Episode 36: tensor([0.5082])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999998  -0.99999964  0.9999998 ]]\n",
      "Episode 37: tensor([0.5088])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999756 -0.99999565  0.9999995 ]]\n",
      "Episode 38: tensor([0.5072])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999765 -0.9999902  0.9999999]]\n",
      "Episode 39: tensor([0.5078])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999882  -0.99993384  0.9999998 ]]\n",
      "Episode 40: tensor([0.5081])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999  -0.9999878   0.99994016]]\n",
      "Episode 41: tensor([0.5079])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.999993  -0.9999848  0.9999995]]\n",
      "Episode 42: tensor([0.5077])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999996 -0.9999439  0.9999999]]\n",
      "Episode 43: tensor([0.5082])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999977   0.99999976]]\n",
      "Episode 44: tensor([0.5080])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999477  0.9999989]]\n",
      "Episode 45: tensor([0.5083])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999996  0.9999991]]\n",
      "Episode 46: tensor([0.5088])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999972   0.99999994]]\n",
      "Episode 47: tensor([0.5081])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99993134  0.99999994]]\n",
      "Episode 48: tensor([0.5077])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999976 -0.99999946  0.99999994]]\n",
      "Episode 49: tensor([0.5080])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999943  0.9999998]]\n",
      "Episode 50: tensor([0.5078])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999959  -0.99999875  0.99999994]]\n",
      "Episode 51: tensor([0.5085])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.9999986   0.99999976]]\n",
      "Episode 52: tensor([0.5087])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999 -0.9999993  0.9999999]]\n",
      "Episode 53: tensor([0.5081])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999 -0.9999924  1.       ]]\n",
      "Episode 54: tensor([0.5087])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999 -0.9999998  1.       ]]\n",
      "Episode 55: tensor([0.5075])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999986  -0.99999774  0.99999946]]\n",
      "Episode 56: tensor([0.5083])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999987  -0.9999992   0.99999875]]\n",
      "Episode 57: tensor([0.5085])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999996   0.9999982 ]]\n",
      "Episode 58: tensor([0.5075])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999845  0.9999995 ]]\n",
      "Episode 59: tensor([0.5086])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999 -0.9999923  0.9999998]]\n",
      "Episode 60: tensor([0.5077])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.999994    0.9999996 ]]\n",
      "Episode 61: tensor([0.5086])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999995  -0.9999996   0.99998426]]\n",
      "Episode 62: tensor([0.5084])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999849   0.9999971 ]]\n",
      "Episode 63: tensor([0.5081])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999 -0.9999966  1.       ]]\n",
      "Episode 64: tensor([0.5080])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999934 -0.9999996   1.        ]]\n",
      "Episode 65: tensor([0.5077])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999815 -0.9999998   0.9999942 ]]\n",
      "Episode 66: tensor([0.5082])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99997723 -0.99999976  0.9999989 ]]\n",
      "Episode 67: tensor([0.5076])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999696 -0.99999994  0.99998134]]\n",
      "Episode 68: tensor([0.5084])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999996 -1.         0.9999991]]\n",
      "Episode 69: tensor([0.5078])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999964  0.99999934]]\n",
      "Episode 70: tensor([0.5078])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999998   0.9999997 ]]\n",
      "Episode 71: tensor([0.5075])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999993  -0.9999997   0.99999964]]\n",
      "Episode 72: tensor([0.5076])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999896 -0.9999933  1.       ]]\n",
      "Episode 73: tensor([0.5087])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999997  -0.99999166  0.99999547]]\n",
      "Episode 74: tensor([0.5082])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999946 -0.99997294  0.99999994]]\n",
      "Episode 75: tensor([0.5086])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999996 -0.9999991  0.9999999]]\n",
      "Episode 76: tensor([0.5083])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999875  0.9999998 ]]\n",
      "Episode 77: tensor([0.5080])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99998564 -0.99999917  0.9999994 ]]\n",
      "Episode 78: tensor([0.5081])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999917 -0.9999969   0.9999995 ]]\n",
      "Episode 79: tensor([0.5079])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999971   0.9999993 ]]\n",
      "Episode 80: tensor([0.5078])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999854   0.9999796 ]]\n",
      "Episode 81: tensor([0.5080])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.9999994   0.99999994]]\n",
      "Episode 82: tensor([0.5084])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999997  -0.9999995   0.99999803]]\n",
      "Episode 83: tensor([0.5084])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999934 -0.99999964  0.9999998 ]]\n",
      "Episode 84: tensor([0.5087])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999998   0.99999964]]\n",
      "Episode 85: tensor([0.5080])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999982   1.        ]]\n",
      "Episode 86: tensor([0.5083])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999946 -0.9999823   0.999982  ]]\n",
      "Episode 87: tensor([0.5091])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999995  -0.99999905  0.99999994]]\n",
      "Episode 88: tensor([0.5084])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999976  0.99999976]]\n",
      "Episode 89: tensor([0.5086])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999976 -0.9999984   1.        ]]\n",
      "Episode 90: tensor([0.5074])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999964 -1.          0.9999999 ]]\n",
      "Episode 91: tensor([0.5081])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999975  0.9999999]]\n",
      "Episode 92: tensor([0.5076])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999386 -0.9999923   0.9999999 ]]\n",
      "Episode 93: tensor([0.5078])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999951  -0.9999987   0.99999887]]\n",
      "Episode 94: tensor([0.5075])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999995 -0.999997   1.       ]]\n",
      "Episode 95: tensor([0.5083])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -1.         0.9999999]]\n",
      "Episode 96: tensor([0.5087])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999989  -0.99999386  0.99998975]]\n",
      "Episode 97: tensor([0.5078])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999995 -0.9999999  0.9999995]]\n",
      "Episode 98: tensor([0.5083])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999983  -0.9999988   0.99999994]]\n",
      "Episode 99: tensor([0.5074])\n",
      "Epoch :  5\n",
      "loss_IOC :  -0.2316715126545374\n",
      "loss_IOC :  -0.22653749853111083\n",
      "loss_IOC :  -0.2284629517321236\n",
      "loss_IOC :  -0.22861984823745535\n",
      "loss_IOC :  -0.23071696321736823\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999994  -0.99999994  0.99999976]]\n",
      "Episode 0: tensor([0.5074])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999978 -0.9999996  1.       ]]\n",
      "Episode 1: tensor([0.5064])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999  -0.99999976  0.9999997 ]]\n",
      "Episode 2: tensor([0.5068])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999984  -0.99999845  0.9999998 ]]\n",
      "Episode 3: tensor([0.5065])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999997  -0.9999992   0.99999684]]\n",
      "Episode 4: tensor([0.5064])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999998  -0.99999994  0.99999994]]\n",
      "Episode 5: tensor([0.5065])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          0.99999964]]\n",
      "Episode 6: tensor([0.5072])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999976 -0.99958986  0.9999936 ]]\n",
      "Episode 7: tensor([0.5072])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999  -0.99999994  0.99999833]]\n",
      "Episode 8: tensor([0.5061])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999976 -0.9999997   1.        ]]\n",
      "Episode 9: tensor([0.5075])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999993  0.9999985]]\n",
      "Episode 10: tensor([0.5063])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999994  -0.9999997   0.99999803]]\n",
      "Episode 11: tensor([0.5067])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999993 -0.9999941  0.9999991]]\n",
      "Episode 12: tensor([0.5069])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999996 -0.9999985  0.9999999]]\n",
      "Episode 13: tensor([0.5062])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999998  -0.9999975   0.99999994]]\n",
      "Episode 14: tensor([0.5064])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999996  -0.9999987   0.99999994]]\n",
      "Episode 15: tensor([0.5064])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.5069])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999993   0.99999994]]\n",
      "Episode 17: tensor([0.5063])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999 -0.9999976  1.       ]]\n",
      "Episode 18: tensor([0.5069])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999979   1.        ]]\n",
      "Episode 19: tensor([0.5069])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  0.99999994]]\n",
      "Episode 20: tensor([0.5070])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999974 -1.         0.9999998]]\n",
      "Episode 21: tensor([0.5063])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999997  -0.9999998   0.99999994]]\n",
      "Episode 22: tensor([0.5065])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999978  -0.99999917  0.9999998 ]]\n",
      "Episode 23: tensor([0.5066])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999967  -0.99999946  0.9999982 ]]\n",
      "Episode 24: tensor([0.5067])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999985   0.9999995 ]]\n",
      "Episode 25: tensor([0.5063])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999666 -0.9999998   0.9999964 ]]\n",
      "Episode 26: tensor([0.5068])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999946 -1.          0.99999994]]\n",
      "Episode 27: tensor([0.5065])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999774 -0.99999917  0.9999997 ]]\n",
      "Episode 28: tensor([0.5061])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999997 -0.9999997  1.       ]]\n",
      "Episode 29: tensor([0.5072])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999971  -0.99999845  1.        ]]\n",
      "Episode 30: tensor([0.5063])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999989  -0.99999994  0.9999994 ]]\n",
      "Episode 31: tensor([0.5070])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999998  -0.9999999   0.99999964]]\n",
      "Episode 32: tensor([0.5063])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999  -0.99999976  1.        ]]\n",
      "Episode 33: tensor([0.5066])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999994  1.        ]]\n",
      "Episode 34: tensor([0.5064])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999997 -0.9999997  0.9999988]]\n",
      "Episode 35: tensor([0.5067])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999803 -0.99999994  1.        ]]\n",
      "Episode 36: tensor([0.5066])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999905 -0.99999994  0.9999999 ]]\n",
      "Episode 37: tensor([0.5066])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999976 -0.99999803  0.9999997 ]]\n",
      "Episode 38: tensor([0.5068])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999  -0.9999997   0.99999535]]\n",
      "Episode 39: tensor([0.5073])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 40: tensor([0.5057])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999991 -1.         1.       ]]\n",
      "Episode 41: tensor([0.5070])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999857  1.        ]]\n",
      "Episode 42: tensor([0.5064])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999999   0.99999905]]\n",
      "Episode 43: tensor([0.5069])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999998  -0.99999964  0.9999999 ]]\n",
      "Episode 44: tensor([0.5065])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999785 -0.9999995   0.99999976]]\n",
      "Episode 45: tensor([0.5070])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999994  0.9999978 ]]\n",
      "Episode 46: tensor([0.5066])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999971  -0.99999875  1.        ]]\n",
      "Episode 47: tensor([0.5068])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999964 -1.          0.99999994]]\n",
      "Episode 48: tensor([0.5061])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  0.9999998 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 49: tensor([0.5063])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999996  1.       ]]\n",
      "Episode 50: tensor([0.5062])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999991  0.9999999]]\n",
      "Episode 51: tensor([0.5070])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999994  -0.99999994  0.9999998 ]]\n",
      "Episode 52: tensor([0.5070])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999  -0.99999964  0.9999999 ]]\n",
      "Episode 53: tensor([0.5065])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999  -1.          0.99999994]]\n",
      "Episode 54: tensor([0.5065])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999 -1.         0.9999999]]\n",
      "Episode 55: tensor([0.5069])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          0.9999999 ]]\n",
      "Episode 56: tensor([0.5061])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  0.9999996 ]]\n",
      "Episode 57: tensor([0.5065])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999 -1.         1.       ]]\n",
      "Episode 58: tensor([0.5060])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999971  0.9999998]]\n",
      "Episode 59: tensor([0.5068])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999976  0.999965  ]]\n",
      "Episode 60: tensor([0.5064])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999999   0.99999994]]\n",
      "Episode 61: tensor([0.5078])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999998  1.       ]]\n",
      "Episode 62: tensor([0.5057])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 63: tensor([0.5059])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999833 -0.99999976  1.        ]]\n",
      "Episode 64: tensor([0.5064])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999 -0.9999999  1.       ]]\n",
      "Episode 65: tensor([0.5062])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999 -0.9999998  0.9999998]]\n",
      "Episode 66: tensor([0.5061])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.9999957   0.99999934]]\n",
      "Episode 67: tensor([0.5060])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999875 -0.999998    1.        ]]\n",
      "Episode 68: tensor([0.5065])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999  -0.99999917  1.        ]]\n",
      "Episode 69: tensor([0.5064])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  0.99999946]]\n",
      "Episode 70: tensor([0.5068])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999996 -0.9999992  0.9999999]]\n",
      "Episode 71: tensor([0.5066])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999994  1.        ]]\n",
      "Episode 72: tensor([0.5059])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999964  0.99999946]]\n",
      "Episode 73: tensor([0.5061])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999999   0.99999994]]\n",
      "Episode 74: tensor([0.5062])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999998 -1.         1.       ]]\n",
      "Episode 75: tensor([0.5067])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999997  -0.99999994  0.9999986 ]]\n",
      "Episode 76: tensor([0.5068])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999995   1.        ]]\n",
      "Episode 77: tensor([0.5057])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999875 -0.9999998   0.9999999 ]]\n",
      "Episode 78: tensor([0.5072])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999994 -1.         0.9999999]]\n",
      "Episode 79: tensor([0.5067])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.5068])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999997 -1.         1.       ]]\n",
      "Episode 81: tensor([0.5069])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999997  -0.99999994  0.999998  ]]\n",
      "Episode 82: tensor([0.5059])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999946  0.9999999 ]]\n",
      "Episode 83: tensor([0.5072])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999976 -0.99999976  0.9999999 ]]\n",
      "Episode 84: tensor([0.5071])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999998  1.       ]]\n",
      "Episode 85: tensor([0.5067])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999934 -0.99999994  1.        ]]\n",
      "Episode 86: tensor([0.5062])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999917  0.99999994]]\n",
      "Episode 87: tensor([0.5063])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.5064])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.9999934   0.99999994]]\n",
      "Episode 89: tensor([0.5063])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999994  1.        ]]\n",
      "Episode 90: tensor([0.5057])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999994  1.        ]]\n",
      "Episode 91: tensor([0.5062])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -1.          0.99999994]]\n",
      "Episode 92: tensor([0.5068])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          0.99999994]]\n",
      "Episode 93: tensor([0.5067])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.9999999   0.99999994]]\n",
      "Episode 94: tensor([0.5072])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.9999999   0.99999994]]\n",
      "Episode 95: tensor([0.5074])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999994  1.        ]]\n",
      "Episode 96: tensor([0.5071])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999997  -0.99999946  0.99999976]]\n",
      "Episode 97: tensor([0.5067])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -1.          0.99999994]]\n",
      "Episode 98: tensor([0.5069])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999976  1.        ]]\n",
      "Episode 99: tensor([0.5068])\n",
      "Epoch :  6\n",
      "loss_IOC :  -0.23014774128075804\n",
      "loss_IOC :  -0.23044148086975857\n",
      "loss_IOC :  -0.2280540593956073\n",
      "loss_IOC :  -0.22774657657493869\n",
      "loss_IOC :  -0.226808453773592\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999934  1.        ]]\n",
      "Episode 0: tensor([0.5053])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999995  -0.99999964  0.9999998 ]]\n",
      "Episode 1: tensor([0.5057])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999774  1.        ]]\n",
      "Episode 2: tensor([0.5049])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999982  1.       ]]\n",
      "Episode 3: tensor([0.5050])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999976  1.        ]]\n",
      "Episode 4: tensor([0.5050])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999505  0.9999991 ]]\n",
      "Episode 5: tensor([0.5045])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999  -0.9999998   0.99999905]]\n",
      "Episode 6: tensor([0.5052])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999996  0.9999998]]\n",
      "Episode 7: tensor([0.5054])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999994   0.99999994]]\n",
      "Episode 8: tensor([0.5060])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999976 -0.99999946  1.        ]]\n",
      "Episode 9: tensor([0.5053])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999998  -0.9999996   0.99999994]]\n",
      "Episode 10: tensor([0.5054])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          0.99999994]]\n",
      "Episode 11: tensor([0.5051])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 12: tensor([0.5049])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999998  1.       ]]\n",
      "Episode 13: tensor([0.5063])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999  -0.99999934  1.        ]]\n",
      "Episode 14: tensor([0.5056])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999999  0.9999999]]\n",
      "Episode 15: tensor([0.5048])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.5051])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999964  0.99999994]]\n",
      "Episode 17: tensor([0.5053])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 18: tensor([0.5049])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 19: tensor([0.5048])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999 -0.9999997  1.       ]]\n",
      "Episode 20: tensor([0.5058])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999993 -1.         1.       ]]\n",
      "Episode 21: tensor([0.5048])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  0.99999994]]\n",
      "Episode 22: tensor([0.5052])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999976  0.99999994]]\n",
      "Episode 23: tensor([0.5054])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  0.9999999 ]]\n",
      "Episode 24: tensor([0.5048])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 25: tensor([0.5053])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999994  1.        ]]\n",
      "Episode 26: tensor([0.5057])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999976 -0.9999999   1.        ]]\n",
      "Episode 27: tensor([0.5057])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 28: tensor([0.5049])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999 -0.9999997  0.999999 ]]\n",
      "Episode 29: tensor([0.5050])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999998 -0.9999999  0.9999998]]\n",
      "Episode 30: tensor([0.5060])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 31: tensor([0.5054])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999997  -0.9999999   0.99999994]]\n",
      "Episode 32: tensor([0.5053])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999998   1.        ]]\n",
      "Episode 33: tensor([0.5052])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999964  1.        ]]\n",
      "Episode 34: tensor([0.5057])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999  -0.99999636  1.        ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 35: tensor([0.5056])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999994  1.        ]]\n",
      "Episode 36: tensor([0.5053])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.5046])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 38: tensor([0.5052])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999994  0.9999999 ]]\n",
      "Episode 39: tensor([0.5055])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.9999999   0.99999994]]\n",
      "Episode 40: tensor([0.5057])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.5053])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999998  -0.99999994  0.9999997 ]]\n",
      "Episode 42: tensor([0.5046])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999994  1.        ]]\n",
      "Episode 43: tensor([0.5059])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999998  1.       ]]\n",
      "Episode 44: tensor([0.5055])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999976 -0.9999997   0.9999998 ]]\n",
      "Episode 45: tensor([0.5058])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999975  1.       ]]\n",
      "Episode 46: tensor([0.5052])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.5058])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.9999998   0.99999994]]\n",
      "Episode 48: tensor([0.5059])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.9999997   0.99999994]]\n",
      "Episode 49: tensor([0.5053])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 50: tensor([0.5051])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.5052])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999995  -0.99999976  1.        ]]\n",
      "Episode 52: tensor([0.5050])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999997 -1.         1.       ]]\n",
      "Episode 53: tensor([0.5054])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999995  -0.9999999   0.99999994]]\n",
      "Episode 54: tensor([0.5055])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 55: tensor([0.5048])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999964  1.        ]]\n",
      "Episode 56: tensor([0.5042])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999964 -0.9999992   1.        ]]\n",
      "Episode 57: tensor([0.5054])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999946 -1.          1.        ]]\n",
      "Episode 58: tensor([0.5049])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.9999999   0.99999946]]\n",
      "Episode 59: tensor([0.5050])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999996  -1.          0.99999994]]\n",
      "Episode 60: tensor([0.5050])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999995  -0.99999964  1.        ]]\n",
      "Episode 61: tensor([0.5060])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  0.99999994]]\n",
      "Episode 62: tensor([0.5060])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  0.99999994]]\n",
      "Episode 63: tensor([0.5051])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.5051])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.5062])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.5053])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 67: tensor([0.5052])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 68: tensor([0.5047])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 69: tensor([0.5053])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999 -1.         0.9999998]]\n",
      "Episode 70: tensor([0.5048])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999994  1.        ]]\n",
      "Episode 71: tensor([0.5048])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999994  1.        ]]\n",
      "Episode 72: tensor([0.5054])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999999  1.       ]]\n",
      "Episode 73: tensor([0.5058])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999 -1.         1.       ]]\n",
      "Episode 74: tensor([0.5050])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999964  1.        ]]\n",
      "Episode 75: tensor([0.5050])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999999  1.       ]]\n",
      "Episode 76: tensor([0.5056])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999999   1.        ]]\n",
      "Episode 77: tensor([0.5058])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999999  0.9999996]]\n",
      "Episode 78: tensor([0.5050])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 79: tensor([0.5054])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999998  0.9999999]]\n",
      "Episode 80: tensor([0.5051])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999 -1.         0.9999998]]\n",
      "Episode 81: tensor([0.5059])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999 -1.         1.       ]]\n",
      "Episode 82: tensor([0.5052])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999917  1.        ]]\n",
      "Episode 83: tensor([0.5047])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999845  1.        ]]\n",
      "Episode 84: tensor([0.5056])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.5056])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 86: tensor([0.5045])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -1.         0.9999999]]\n",
      "Episode 87: tensor([0.5044])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999998 -1.         0.9999998]]\n",
      "Episode 88: tensor([0.5056])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.5049])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.5046])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -1.          0.99999994]]\n",
      "Episode 91: tensor([0.5060])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.5048])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999998  1.       ]]\n",
      "Episode 93: tensor([0.5050])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999994  1.        ]]\n",
      "Episode 94: tensor([0.5047])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.5053])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -1.          0.99999994]]\n",
      "Episode 96: tensor([0.5048])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          0.99999994]]\n",
      "Episode 97: tensor([0.5052])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999934  1.        ]]\n",
      "Episode 98: tensor([0.5050])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.5055])\n",
      "Epoch :  7\n",
      "loss_IOC :  -0.23026008884380716\n",
      "loss_IOC :  -0.22711449957554652\n",
      "loss_IOC :  -0.22809002328009337\n",
      "loss_IOC :  -0.22678688247995454\n",
      "loss_IOC :  -0.2273633039047156\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.5041])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999999   1.        ]]\n",
      "Episode 1: tensor([0.5037])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -1.          0.99999994]]\n",
      "Episode 2: tensor([0.5037])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 3: tensor([0.5037])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -1.          0.99999994]]\n",
      "Episode 4: tensor([0.5031])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -1.         0.9999995]]\n",
      "Episode 5: tensor([0.5044])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -1.         0.9999999]]\n",
      "Episode 6: tensor([0.5040])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999999   1.        ]]\n",
      "Episode 7: tensor([0.5036])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999964 -1.          1.        ]]\n",
      "Episode 8: tensor([0.5047])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.5045])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.5038])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.5039])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  0.99999994]]\n",
      "Episode 12: tensor([0.5035])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.5038])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.5043])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -1.          0.99999994]]\n",
      "Episode 15: tensor([0.5034])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 16: tensor([0.5044])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -1.          0.99999994]]\n",
      "Episode 17: tensor([0.5048])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 18: tensor([0.5045])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999976  1.        ]]\n",
      "Episode 19: tensor([0.5031])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -1.          0.99999994]]\n",
      "Episode 20: tensor([0.5040])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.5040])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999999  1.       ]]\n",
      "Episode 22: tensor([0.5044])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.5042])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.5044])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.5039])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.5034])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  0.9999999 ]]\n",
      "Episode 27: tensor([0.5032])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.5040])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 29: tensor([0.5034])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -1.         0.9999998]]\n",
      "Episode 30: tensor([0.5038])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.5045])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.5042])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999999  1.       ]]\n",
      "Episode 33: tensor([0.5037])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.5039])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999995 -1.         1.       ]]\n",
      "Episode 35: tensor([0.5032])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 36: tensor([0.5045])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 37: tensor([0.5038])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.5035])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.5043])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.5041])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.5051])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 42: tensor([0.5048])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.5039])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -1.          0.99999994]]\n",
      "Episode 44: tensor([0.5041])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999976 -0.99999994  1.        ]]\n",
      "Episode 45: tensor([0.5040])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.5032])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.5045])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 48: tensor([0.5030])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.5043])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.5038])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.5036])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.5030])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.5038])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -1.          0.99999994]]\n",
      "Episode 54: tensor([0.5038])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.5031])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.5032])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.5042])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.5041])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 59: tensor([0.5045])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999999  1.       ]]\n",
      "Episode 60: tensor([0.5040])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.5035])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999 -1.         1.       ]]\n",
      "Episode 62: tensor([0.5039])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 63: tensor([0.5036])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.5040])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.5033])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 66: tensor([0.5039])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 67: tensor([0.5034])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.5036])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999997 -1.         1.       ]]\n",
      "Episode 69: tensor([0.5042])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.5036])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.5041])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.5043])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.5035])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.5036])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.5040])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.5036])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.5035])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 78: tensor([0.5041])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.5036])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.5040])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.5033])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.5045])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.5041])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -1.          0.99999994]]\n",
      "Episode 84: tensor([0.5043])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999 -1.         1.       ]]\n",
      "Episode 85: tensor([0.5041])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.5038])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.5038])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.5040])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.5038])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 90: tensor([0.5035])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.5042])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.5051])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.5037])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.5043])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 95: tensor([0.5038])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 96: tensor([0.5039])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.9999999   1.        ]]\n",
      "Episode 97: tensor([0.5038])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999999  1.       ]]\n",
      "Episode 98: tensor([0.5045])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999998  1.       ]]\n",
      "Episode 99: tensor([0.5036])\n",
      "Epoch :  8\n",
      "loss_IOC :  -0.22987970784060036\n",
      "loss_IOC :  -0.22912075167260537\n",
      "loss_IOC :  -0.23286981510644322\n",
      "loss_IOC :  -0.22967538919638397\n",
      "loss_IOC :  -0.22870447234482566\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.5028])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.5019])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 2: tensor([0.5023])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999998  1.       ]]\n",
      "Episode 3: tensor([0.5027])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.5026])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.5026])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -1.          0.99999994]]\n",
      "Episode 6: tensor([0.5032])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.5015])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.5024])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.5015])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 10: tensor([0.5019])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.5031])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.5024])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.5026])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.5021])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999999  1.       ]]\n",
      "Episode 15: tensor([0.5030])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.5022])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.5019])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.5022])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.5019])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.5018])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 21: tensor([0.5014])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.5027])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.5022])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999999 -1.         1.       ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 24: tensor([0.5029])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.5033])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.5028])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 27: tensor([0.5031])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  0.99999994]]\n",
      "Episode 28: tensor([0.5025])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.5026])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.5021])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 31: tensor([0.5035])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.5026])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.5032])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 34: tensor([0.5023])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.9999998 -1.         1.       ]]\n",
      "Episode 35: tensor([0.5024])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.5024])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.5031])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999999  1.       ]]\n",
      "Episode 38: tensor([0.5020])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.5021])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.5021])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.5019])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.5031])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 43: tensor([0.5029])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 44: tensor([0.5024])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 45: tensor([0.5018])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.5017])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.5036])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.5028])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 49: tensor([0.5028])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.5028])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.5025])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.5026])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.5025])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.5025])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -0.99999994  1.        ]]\n",
      "Episode 55: tensor([0.5024])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.5027])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.5024])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.5021])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.5031])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.5029])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 61: tensor([0.5033])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.5027])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.5023])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 64: tensor([0.5023])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.5025])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.5028])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 67: tensor([0.5024])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.5020])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.5027])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.5029])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.5024])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.5026])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.5025])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.5018])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.5029])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.5027])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.5018])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.5039])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.5026])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.5034])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.5025])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.5020])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 83: tensor([0.5033])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.5034])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.5022])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.5028])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.5025])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.5028])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.5026])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.5021])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.5022])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.5038])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.5026])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.5029])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.5019])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999999  1.       ]]\n",
      "Episode 96: tensor([0.5030])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.5021])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.5025])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.5018])\n",
      "Epoch :  9\n",
      "loss_IOC :  -0.2292159336091062\n",
      "loss_IOC :  -0.2322452163112172\n",
      "loss_IOC :  -0.22884300706358218\n",
      "loss_IOC :  -0.22971065445066513\n",
      "loss_IOC :  -0.22139303050006098\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.5008])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.5016])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.5013])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.5013])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.5016])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.5008])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.5002])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.5014])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.5017])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.5010])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.5003])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.5011])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.5007])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.5011])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.5011])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4998])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.5006])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.5000])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.5008])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4999])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.5002])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.5011])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.5003])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.5014])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.5011])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.5011])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.5001])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.5004])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.5011])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.5009])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.5011])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 31: tensor([0.5012])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.5008])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.5011])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 34: tensor([0.5009])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4999])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 36: tensor([0.5017])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.5019])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.5012])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.        -0.9999999  1.       ]]\n",
      "Episode 39: tensor([0.5011])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.5011])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.5009])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.5010])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.5013])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 44: tensor([0.5000])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.5006])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.5002])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4999])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.5014])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.5008])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.5010])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.5003])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.5012])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.5012])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.5005])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.5015])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.5012])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.5019])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.5017])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.5007])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.5007])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.5014])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 62: tensor([0.5007])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.5008])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.5012])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.5009])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.5006])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.5012])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.5006])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.5015])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.5010])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.5015])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.5011])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.5004])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.5015])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.5016])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.5002])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.5015])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.5009])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.5007])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.5002])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4999])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.5002])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -0.99999994  1.        ]]\n",
      "Episode 83: tensor([0.5016])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.5018])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.5017])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.5010])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.5008])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.5004])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4997])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.5010])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.5005])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.5007])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.5010])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.5011])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.5009])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.5007])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.5018])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.5015])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.5010])\n",
      "Epoch :  10\n",
      "loss_IOC :  -0.22966704832429774\n",
      "loss_IOC :  -0.23493411429904465\n",
      "loss_IOC :  -0.23235986347251608\n",
      "loss_IOC :  -0.2279563592017449\n",
      "loss_IOC :  -0.23054589997352948\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 0: tensor([0.5001])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4992])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4994])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 0.99999994 -1.          1.        ]]\n",
      "Episode 3: tensor([0.4996])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.5004])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4996])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4990])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.5004])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4988])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4991])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4991])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.5000])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4990])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4997])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4992])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4997])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4999])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.5005])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4998])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4995])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.5001])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4994])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4997])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.5005])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4995])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4994])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4995])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4987])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.5000])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4995])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4989])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4994])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.5004])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4993])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4998])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4998])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4992])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4996])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4996])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4997])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4991])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4997])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4986])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4990])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4984])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4993])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 46: tensor([0.4993])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4995])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4992])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.5003])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4989])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4998])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4997])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4989])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4994])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4989])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4996])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4991])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4997])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.5000])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4990])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4994])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4996])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4995])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.5001])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4994])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4993])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.5002])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4999])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.5000])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4986])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.5000])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.5003])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.5008])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4993])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4987])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4997])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.5003])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4995])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4997])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4987])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4989])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.5011])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4989])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4999])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.5004])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4986])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.5001])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4991])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4994])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4999])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.5002])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4991])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.5000])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4993])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4999])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4991])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4998])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4996])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4996])\n",
      "Epoch :  11\n",
      "loss_IOC :  -0.23025457505138763\n",
      "loss_IOC :  -0.22404081063330206\n",
      "loss_IOC :  -0.23134711759768795\n",
      "loss_IOC :  -0.23010494849590885\n",
      "loss_IOC :  -0.22466371407580593\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4983])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4980])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4983])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4974])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4981])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4984])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4981])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4976])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4985])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4985])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4980])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4978])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4967])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4979])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4983])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4986])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4982])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4984])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4977])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4978])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4970])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4990])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4987])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4983])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4982])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4985])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4984])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4985])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4982])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4985])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4991])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4986])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4977])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4986])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4984])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4983])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4983])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4980])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4982])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4986])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4984])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4976])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4988])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4978])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4983])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4977])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4983])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4979])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4978])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4980])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4975])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4977])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4981])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4990])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4979])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4991])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4983])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4985])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 58: tensor([0.4983])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4978])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4984])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4979])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4977])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4983])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4985])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4972])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4970])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4977])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4980])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4982])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4974])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4969])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4976])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4982])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4970])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4973])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4986])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4976])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4979])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4998])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4982])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4992])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4984])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4982])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4974])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4982])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4979])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4980])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4976])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4979])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4983])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4985])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4987])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4986])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4990])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4991])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4975])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4990])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4993])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4977])\n",
      "Epoch :  12\n",
      "loss_IOC :  -0.23198070668632004\n",
      "loss_IOC :  -0.2261181052152028\n",
      "loss_IOC :  -0.2294565687940397\n",
      "loss_IOC :  -0.23247636923501486\n",
      "loss_IOC :  -0.22223180531196757\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4970])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4969])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4966])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4963])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4974])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4959])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4965])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4981])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4968])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4981])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4963])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4970])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4964])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4971])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4962])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4971])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4960])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4976])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4969])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4965])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4973])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4983])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4959])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4972])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4971])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4978])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4974])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4970])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4968])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4971])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4972])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4971])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4974])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4970])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4972])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4963])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4970])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4977])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4976])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4959])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4968])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4966])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4972])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4964])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4968])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4965])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4971])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4978])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4968])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4964])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4956])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4965])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4977])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4969])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4976])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4977])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4969])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4975])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4967])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4978])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4964])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4973])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4974])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4981])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4959])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 65: tensor([0.4972])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4967])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4960])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4965])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4974])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4964])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4981])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4973])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4962])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4964])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4965])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4965])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4969])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4966])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4962])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4962])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4965])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4963])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4969])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4965])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4972])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4969])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4976])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4968])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4971])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4966])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4969])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4979])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4980])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4959])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4979])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4969])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4971])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4978])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4966])\n",
      "Epoch :  13\n",
      "loss_IOC :  -0.225786765198232\n",
      "loss_IOC :  -0.2343265365977239\n",
      "loss_IOC :  -0.22754349466578377\n",
      "loss_IOC :  -0.22894524081052625\n",
      "loss_IOC :  -0.22919104508548932\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4944])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4955])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4960])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4954])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4957])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4958])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4959])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4964])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4962])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4952])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4952])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4961])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4960])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4949])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4959])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4951])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4961])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4956])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4947])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4945])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4960])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4952])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4970])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4979])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4961])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4967])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4956])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4965])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4970])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4965])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4960])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4962])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4975])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4958])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4947])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4955])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4969])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4950])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4968])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4950])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4966])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4964])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4956])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4971])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1.         -1.          0.99999994]]\n",
      "Episode 44: tensor([0.4960])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4957])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4953])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4947])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4960])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4959])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4969])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4969])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4953])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4961])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4955])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4949])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4957])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4963])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4957])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4951])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4967])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4959])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4953])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4959])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4960])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4974])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4960])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4956])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4960])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4955])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4957])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4970])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4967])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4957])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4959])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4955])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4949])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4963])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 78: tensor([0.4954])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4960])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4956])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4957])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4958])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4962])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4961])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4952])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4964])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4970])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4965])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4960])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4963])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4955])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4949])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4963])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4958])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4962])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4955])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4953])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4961])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4956])\n",
      "Epoch :  14\n",
      "loss_IOC :  -0.22346430435103515\n",
      "loss_IOC :  -0.22279893250838634\n",
      "loss_IOC :  -0.22515015725687093\n",
      "loss_IOC :  -0.22898713217270084\n",
      "loss_IOC :  -0.23612611222280333\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4943])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4944])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4940])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4938])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4956])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4947])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4953])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4951])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4954])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4953])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4946])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4941])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4939])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4950])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4948])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4949])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4943])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4950])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4951])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4953])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4951])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4938])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4940])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4933])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4937])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4933])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4953])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4934])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4947])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4949])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4944])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4943])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4952])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4930])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4942])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4959])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4958])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4950])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4947])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4944])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4938])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4950])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4943])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4930])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4951])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4942])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4944])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4956])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4938])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4943])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4946])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4937])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4950])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4931])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4937])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4948])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4956])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4936])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4937])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4945])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4953])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4953])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4942])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4940])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4944])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4940])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4951])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4942])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4938])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4934])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4938])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4939])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4939])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4949])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4933])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4953])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4951])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 77: tensor([0.4954])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4940])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4943])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4944])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4940])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4934])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4946])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4938])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4937])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4962])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4947])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4944])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4942])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4953])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4945])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4955])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4935])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4944])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4952])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4940])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4939])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4940])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4948])\n",
      "Epoch :  15\n",
      "loss_IOC :  -0.22235163430274513\n",
      "loss_IOC :  -0.2288231156319398\n",
      "loss_IOC :  -0.22879921882157161\n",
      "loss_IOC :  -0.22546658158394906\n",
      "loss_IOC :  -0.2311684244340439\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4937])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4924])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4930])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4937])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4930])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4931])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4935])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4926])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4944])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4931])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4946])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4924])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4928])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4934])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4927])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4937])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4921])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4939])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4950])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4930])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4935])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4937])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4940])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4930])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4940])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4943])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4932])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4937])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4927])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4933])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4937])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4927])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4930])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4939])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4924])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4930])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4935])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4939])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4935])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4933])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4939])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4942])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4925])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4935])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4927])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4946])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4933])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4937])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4929])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4921])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4940])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4934])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4935])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4944])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4938])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4927])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4925])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4936])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4941])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4924])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4930])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4930])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4922])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4921])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4929])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4941])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4937])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4941])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4927])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4934])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4934])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4954])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4933])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4933])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4924])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4946])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4946])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4947])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4935])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4926])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4923])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4939])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4919])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4931])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4934])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4931])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4931])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4931])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4948])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4935])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4920])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4929])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4931])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4934])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4932])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4930])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 96: tensor([0.4940])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4923])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4932])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4937])\n",
      "Epoch :  16\n",
      "loss_IOC :  -0.22902495786723437\n",
      "loss_IOC :  -0.2330254827578634\n",
      "loss_IOC :  -0.23134779465500394\n",
      "loss_IOC :  -0.22355593299717558\n",
      "loss_IOC :  -0.22558115309052523\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4923])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4907])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4928])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4919])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4921])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4925])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4921])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4918])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4924])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4929])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4925])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4918])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4923])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4923])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4918])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4928])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4907])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4922])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4929])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4917])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4922])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4927])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4920])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4919])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4925])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4915])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4930])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4928])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4925])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4916])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4919])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4925])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4919])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4932])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4916])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4924])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4929])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4916])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4914])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4921])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4909])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4926])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4927])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4921])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4932])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4927])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4923])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4918])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4915])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4932])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4921])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4927])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4915])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4926])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4920])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4931])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4927])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4924])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4923])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4921])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4915])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4930])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4931])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4920])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4922])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4921])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4929])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4929])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4921])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4920])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4921])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4923])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4927])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4926])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4931])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4917])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4926])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4930])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4917])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4913])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4916])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4927])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4922])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4926])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4924])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4909])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4931])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4917])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4923])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4921])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4925])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4933])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4919])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4914])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4914])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4926])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4918])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4929])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4927])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4932])\n",
      "Epoch :  17\n",
      "loss_IOC :  -0.22538287302768434\n",
      "loss_IOC :  -0.22591830224947929\n",
      "loss_IOC :  -0.22966333010651407\n",
      "loss_IOC :  -0.23145806742463743\n",
      "loss_IOC :  -0.2236001992776392\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4915])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4918])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4901])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4925])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4904])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4906])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4917])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4925])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4914])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4926])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4899])\n",
      "state : \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4925])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4901])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4913])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4916])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4909])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4920])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4909])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4898])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4923])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4919])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4918])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4909])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4905])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4912])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4917])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4898])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4908])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4918])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4917])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4913])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4918])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4916])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4910])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4918])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4906])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4905])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4915])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4902])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4916])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4921])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4924])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4904])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4909])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4924])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4913])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4921])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4901])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4909])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4903])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4926])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4908])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4903])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4921])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4908])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4914])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4920])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4924])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4915])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4909])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4914])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4905])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4914])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4920])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4911])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4907])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4911])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4920])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4900])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4917])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4916])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4902])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4917])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4916])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4908])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4930])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4915])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4903])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4899])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4911])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4911])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4908])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4925])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4913])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4899])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4905])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4915])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4913])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4902])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4908])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4904])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4913])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4919])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4919])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4922])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4915])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4919])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4918])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4911])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4904])\n",
      "Epoch :  18\n",
      "loss_IOC :  -0.22906285879815264\n",
      "loss_IOC :  -0.22785258770422856\n",
      "loss_IOC :  -0.22851911425048854\n",
      "loss_IOC :  -0.23078887234205003\n",
      "loss_IOC :  -0.2282329398652546\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4899])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4905])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4902])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4898])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4898])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4916])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4910])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4898])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4906])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4917])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4911])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4914])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4895])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4899])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14: tensor([0.4911])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4907])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4907])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4904])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4898])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4902])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4907])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4897])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4909])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4905])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4898])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4903])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4892])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4908])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4900])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4903])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4905])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4908])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4913])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4897])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4910])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4897])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4906])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4909])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4912])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4907])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4898])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4901])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4901])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4895])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4900])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4913])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4897])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4908])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4920])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4917])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4894])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4906])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4908])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4903])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4908])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4909])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4911])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4890])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4895])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4894])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4889])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4922])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4901])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4896])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4915])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4905])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4897])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4899])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4905])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4909])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4898])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4901])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4905])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4912])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4914])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4912])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4908])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4894])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4910])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4897])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4905])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4891])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4910])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4893])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4899])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4914])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4895])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4917])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4916])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4894])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4896])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4891])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4902])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4909])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4907])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4909])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4895])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4916])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4918])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4901])\n",
      "Epoch :  19\n",
      "loss_IOC :  -0.2278447701549719\n",
      "loss_IOC :  -0.22699491820000606\n",
      "loss_IOC :  -0.2317209591966991\n",
      "loss_IOC :  -0.22190047183480743\n",
      "loss_IOC :  -0.23333404747961023\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4892])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4902])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4903])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4890])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4890])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4891])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4890])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4887])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4888])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4888])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4902])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4884])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4896])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4896])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4885])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4891])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4901])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4876])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4883])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4887])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4892])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4915])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4890])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4881])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4900])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4891])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4887])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4899])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4888])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4894])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4892])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4890])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4887])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 33: tensor([0.4895])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4899])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4899])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4894])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4901])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4893])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4894])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4897])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4898])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4890])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4892])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4898])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4901])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4893])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4901])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4887])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4901])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4885])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4877])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4900])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4884])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4890])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4894])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4897])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4884])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4898])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4886])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4884])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4892])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4883])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4897])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4897])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4889])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4896])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4910])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4890])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4889])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4887])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4904])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4890])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4892])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4898])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4888])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4880])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4891])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4888])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4891])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4894])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4892])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4887])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4889])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4894])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4878])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4890])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4898])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4889])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4897])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4886])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4876])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4904])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4885])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4902])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4882])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4894])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4901])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4888])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4898])\n",
      "Epoch :  20\n",
      "loss_IOC :  -0.2317446632797706\n",
      "loss_IOC :  -0.22929219519414384\n",
      "loss_IOC :  -0.22767754934967982\n",
      "loss_IOC :  -0.229105625247021\n",
      "loss_IOC :  -0.229729550663671\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4886])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4897])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4881])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4885])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4894])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4888])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4877])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4883])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4881])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4885])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4883])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4899])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4887])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4892])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4890])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4880])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4871])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4872])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4882])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4878])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4884])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4896])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4898])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4873])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4882])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4882])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4875])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4894])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4882])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4898])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4887])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4890])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4887])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4895])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4878])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4885])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4894])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4883])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4892])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 39: tensor([0.4894])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4884])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4901])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4893])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4874])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4877])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4882])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4879])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4882])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4878])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4883])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4897])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4891])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4875])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4873])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4885])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4881])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4866])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4876])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4898])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4897])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4870])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4896])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4889])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4888])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4893])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4882])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4876])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4886])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4886])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4888])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4895])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4892])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4875])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4900])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4895])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4884])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4870])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4897])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4887])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4883])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4891])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4889])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4878])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4876])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4883])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4883])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4882])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4891])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4889])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4881])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4878])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4881])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4894])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4875])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4885])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4895])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4889])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4884])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4885])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4897])\n",
      "Epoch :  21\n",
      "loss_IOC :  -0.22403941269495847\n",
      "loss_IOC :  -0.22791193132499443\n",
      "loss_IOC :  -0.22985577825945736\n",
      "loss_IOC :  -0.23450105150991046\n",
      "loss_IOC :  -0.22518688892136385\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4873])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4891])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4884])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4889])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4869])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4884])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4880])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4885])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4862])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4883])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4872])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4872])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4890])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4876])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4886])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4873])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4874])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4859])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4896])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4885])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4879])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4874])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4885])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4875])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4880])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4887])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4871])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4867])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4869])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4882])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4881])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4887])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4885])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4885])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4865])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4878])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4877])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4882])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4873])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4889])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4884])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4872])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4867])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4879])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4898])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4886])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4877])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4870])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4867])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4867])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4873])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4890])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4875])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4887])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4877])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4879])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4867])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4860])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4886])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 59: tensor([0.4866])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4873])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4873])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4881])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4888])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4869])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4867])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4871])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4875])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4876])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4882])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4879])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4863])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4880])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4874])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4880])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4877])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4878])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4871])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4863])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4867])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4878])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4877])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4875])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4879])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4878])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4895])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4872])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4886])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4882])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4870])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4875])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4879])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4880])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4882])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4885])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4876])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4886])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4879])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4877])\n",
      "Epoch :  22\n",
      "loss_IOC :  -0.23399128974082922\n",
      "loss_IOC :  -0.22762537475895978\n",
      "loss_IOC :  -0.23393104435049827\n",
      "loss_IOC :  -0.2368495037077289\n",
      "loss_IOC :  -0.22557814154582206\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4877])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4872])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4861])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4864])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4887])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4863])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4874])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4866])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4879])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4876])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4884])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4885])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4868])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4871])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4875])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4870])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4880])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4878])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4883])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4876])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4871])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4870])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4857])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4869])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4863])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4870])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4874])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4865])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4876])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4871])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4867])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4881])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4874])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4865])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4865])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4864])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4873])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4857])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4878])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4882])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4877])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4878])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4864])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4865])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4876])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4879])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4862])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4872])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4881])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4869])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4866])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4870])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4891])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4876])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4879])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4876])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4866])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4874])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4878])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4872])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4862])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 62: tensor([0.4860])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4863])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4869])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4871])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4875])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4867])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4872])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4865])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4861])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4866])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4876])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4873])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4882])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4874])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4868])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4879])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4872])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4869])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4872])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4883])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4873])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4875])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4870])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4882])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4885])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4880])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4876])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4854])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4864])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4874])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4866])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4875])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4882])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4867])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4863])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4863])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4870])\n",
      "Epoch :  23\n",
      "loss_IOC :  -0.22517175033800146\n",
      "loss_IOC :  -0.22296431015574075\n",
      "loss_IOC :  -0.2354526400360955\n",
      "loss_IOC :  -0.22033880674604012\n",
      "loss_IOC :  -0.21978507315785778\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4851])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4852])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4854])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4890])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4868])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4873])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4863])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4854])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4868])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4865])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4863])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4856])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4863])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4865])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4865])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4880])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4859])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4874])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4873])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4866])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4865])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4851])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4847])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4864])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4867])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4854])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4864])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4855])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4845])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4857])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4870])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4885])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4873])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4879])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4875])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4864])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4865])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4853])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4881])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4847])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4856])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4886])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4870])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4863])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4868])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4862])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4871])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4863])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4871])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4871])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4866])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4861])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4864])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4878])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4880])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4860])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4878])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4875])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4877])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4879])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4860])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4862])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4856])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4863])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4867])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4851])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4860])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4861])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4867])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4853])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4854])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4863])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4869])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4859])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4850])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4880])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4862])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4854])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4867])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 82: tensor([0.4869])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4878])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4872])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4867])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4860])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4863])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4866])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4884])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4855])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4864])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4861])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4868])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4862])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4874])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4864])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4869])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4858])\n",
      "Epoch :  24\n",
      "loss_IOC :  -0.23458010027841447\n",
      "loss_IOC :  -0.22371369882065567\n",
      "loss_IOC :  -0.23425407966921463\n",
      "loss_IOC :  -0.23090545064363588\n",
      "loss_IOC :  -0.24092467494863623\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4850])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4865])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4849])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4842])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4862])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4855])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4868])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4841])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4844])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4850])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4859])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4859])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4855])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4837])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4850])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4861])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4864])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4854])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4860])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4860])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4849])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4850])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4878])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4859])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4836])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4854])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4863])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4851])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4860])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4867])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4859])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4864])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4855])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4856])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4866])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4844])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4851])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4847])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4860])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4863])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4854])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4862])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4856])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4846])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4868])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4840])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4849])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4848])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4860])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4847])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4862])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4855])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4844])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4855])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4865])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4847])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4862])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4865])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4859])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4854])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4869])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4869])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4847])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4846])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4866])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4852])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4861])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4857])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4857])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4866])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4864])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4874])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4866])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4844])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4850])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4847])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4856])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4842])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4866])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4864])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4857])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4857])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4849])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 90: tensor([0.4855])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4866])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4875])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4855])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4842])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4842])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4838])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4842])\n",
      "Epoch :  25\n",
      "loss_IOC :  -0.22043310074539435\n",
      "loss_IOC :  -0.23208748725925815\n",
      "loss_IOC :  -0.2359176127407956\n",
      "loss_IOC :  -0.21915509853277026\n",
      "loss_IOC :  -0.22414765449135055\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4865])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4830])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4860])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4843])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4848])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4838])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4844])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4846])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4837])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4852])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4850])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4847])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4854])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4852])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4844])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4855])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4838])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4853])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4837])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4836])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4826])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4835])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4840])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4860])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4836])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4826])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4847])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4862])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4853])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4832])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4859])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4844])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4861])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4828])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4844])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4852])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4848])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4857])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4861])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4850])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4849])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4859])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4854])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4861])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4837])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4836])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4832])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4847])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4857])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4838])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4837])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4846])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4846])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4852])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4847])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4847])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4843])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4855])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4851])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4855])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4842])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4852])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4846])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4850])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4852])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4849])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4835])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4837])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4845])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4845])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4839])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4841])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4846])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4849])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4838])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4842])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4834])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4849])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4851])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4866])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4844])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4874])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4856])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4846])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4866])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4851])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4854])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4840])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4844])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4850])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4850])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4840])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4852])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4845])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4845])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4845])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4850])\n",
      "Epoch :  26\n",
      "loss_IOC :  -0.22904814707621995\n",
      "loss_IOC :  -0.22331211166881781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_IOC :  -0.2340011856363035\n",
      "loss_IOC :  -0.2234574854695393\n",
      "loss_IOC :  -0.22317340481707476\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4842])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4833])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4822])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4839])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4841])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4828])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4827])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4838])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4838])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4832])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4824])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4842])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4831])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4842])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4839])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4836])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4839])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4842])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4831])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4843])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4839])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4841])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4840])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4840])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4847])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4832])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4837])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4841])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4844])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4836])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4830])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4842])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4833])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4838])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4845])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4825])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4842])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4845])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4834])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4830])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4839])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4840])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4838])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4842])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4836])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4839])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4824])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4835])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4851])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4843])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4831])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4838])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4832])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4842])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4825])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4843])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4841])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4838])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4817])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4852])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4842])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4851])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4838])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4838])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4828])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4841])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4830])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4837])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4822])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4841])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4836])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4829])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4851])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4858])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4837])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4841])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4839])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4834])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4827])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4838])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4836])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4846])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4853])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4848])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4857])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4825])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4823])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4839])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4818])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4841])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4845])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4839])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4833])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4831])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4827])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4836])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4840])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4843])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4863])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4823])\n",
      "Epoch :  27\n",
      "loss_IOC :  -0.23351759483194767\n",
      "loss_IOC :  -0.23235534256942159\n",
      "loss_IOC :  -0.23096710108981072\n",
      "loss_IOC :  -0.2401004043057564\n",
      "loss_IOC :  -0.22943179987898493\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4825])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4839])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4825])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4803])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4823])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4819])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4826])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4829])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4829])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4828])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4842])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4830])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12: tensor([0.4810])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4838])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4836])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4827])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4829])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4824])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4838])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4822])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4821])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4830])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4835])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4831])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4841])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4854])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4817])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4845])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4809])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4828])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4835])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4817])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4827])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4822])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4826])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4829])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4841])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4817])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4839])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4831])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4816])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4825])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4830])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4824])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4823])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4809])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4826])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4825])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4820])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4835])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4821])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4816])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4832])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4829])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4821])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4827])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4836])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4824])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4814])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4821])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4833])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4838])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4842])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4850])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4814])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4834])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4832])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4816])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4836])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4818])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4819])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4831])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4816])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4837])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4804])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4830])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4827])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4823])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4837])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4834])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4836])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4816])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4823])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4829])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4816])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4828])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4822])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4838])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4831])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4831])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4829])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4829])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4820])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4830])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4817])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4841])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4823])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4828])\n",
      "Epoch :  28\n",
      "loss_IOC :  -0.22942067913983777\n",
      "loss_IOC :  -0.22126076620900015\n",
      "loss_IOC :  -0.22463004258210928\n",
      "loss_IOC :  -0.23271992822561038\n",
      "loss_IOC :  -0.2255244942338943\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4815])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4801])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4806])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4821])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4822])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4827])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4805])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4811])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4827])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4811])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4799])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4829])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4804])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4826])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4814])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4820])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4803])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4822])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4818])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4814])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4802])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4820])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 26: tensor([0.4834])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4816])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4831])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4809])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4802])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4826])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4834])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4813])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4838])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4816])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4820])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4795])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4807])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4820])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4813])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4796])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4824])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4825])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4806])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4803])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4820])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4817])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4832])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4813])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4827])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4811])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4813])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4804])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4806])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4803])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4810])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4795])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4828])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4820])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4797])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4795])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4801])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4810])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4801])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4822])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4816])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4823])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4816])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4827])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4819])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4810])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4806])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4825])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4816])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4821])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4817])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4805])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4805])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4816])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4815])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4808])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4823])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4823])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4814])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4824])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4803])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4808])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4815])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4806])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4802])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4804])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4820])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4829])\n",
      "Epoch :  29\n",
      "loss_IOC :  -0.22952716453150512\n",
      "loss_IOC :  -0.23046407203465985\n",
      "loss_IOC :  -0.22158518859038967\n",
      "loss_IOC :  -0.2448955578163725\n",
      "loss_IOC :  -0.2317727050369\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4821])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4790])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4802])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4796])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4833])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4816])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4796])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4814])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4807])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4791])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4793])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4807])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4809])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4788])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4804])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4813])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4814])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4810])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4800])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4802])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4801])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4811])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4807])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4806])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4803])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4828])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4811])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4809])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4797])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4811])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4823])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4790])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4810])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4802])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4808])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4805])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4810])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4818])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4805])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4825])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4819])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4824])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4822])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4790])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4827])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4790])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4793])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4788])\n",
      "state : \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4809])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4813])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4822])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4814])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4806])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4808])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4811])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4813])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4810])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4796])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4823])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4802])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4801])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4811])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4783])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4808])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4808])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4807])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4811])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4800])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4831])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4831])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4810])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4790])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4796])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4797])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4817])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4796])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4804])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4806])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4816])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4814])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4806])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4798])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4796])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4810])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4798])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4824])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4804])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4799])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4803])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4832])\n",
      "Epoch :  30\n",
      "loss_IOC :  -0.22809197272213955\n",
      "loss_IOC :  -0.2228312089389466\n",
      "loss_IOC :  -0.23660260804373373\n",
      "loss_IOC :  -0.22727591989332346\n",
      "loss_IOC :  -0.23187837885575735\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4798])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4802])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4793])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4797])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4804])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4796])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4808])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4792])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4787])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4792])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4807])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4802])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4809])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4793])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4788])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4800])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4793])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4800])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4789])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4831])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4807])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4808])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4806])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4815])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4788])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4804])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4791])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4788])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4810])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4799])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4806])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4791])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4805])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4796])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4810])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4806])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4797])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4789])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4783])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4809])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4800])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4800])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4815])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4808])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4797])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4820])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4804])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4801])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4816])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4806])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4802])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4811])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4797])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4800])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4803])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4791])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4798])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4796])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4822])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4806])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4795])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4785])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4810])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4789])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 70: tensor([0.4800])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4800])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4805])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4791])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4793])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4800])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4814])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4800])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4786])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4786])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4777])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4803])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4809])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4805])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4792])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4807])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4793])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4804])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4835])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4772])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4778])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4800])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4817])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4791])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4813])\n",
      "Epoch :  31\n",
      "loss_IOC :  -0.22572273684919014\n",
      "loss_IOC :  -0.2305663187679277\n",
      "loss_IOC :  -0.23244921032158006\n",
      "loss_IOC :  -0.23218325511937465\n",
      "loss_IOC :  -0.2240862870979412\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4787])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4775])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4792])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4792])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4787])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4799])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4781])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4787])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4795])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4782])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4799])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4781])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4802])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4803])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4781])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4793])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4785])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4789])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4795])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4777])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4801])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4782])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4779])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4807])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4797])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4796])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4790])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4805])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4780])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4777])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4790])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4792])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4795])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4786])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4788])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4809])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4800])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4798])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4786])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4786])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4802])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4780])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4777])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4805])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4804])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4796])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4805])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4802])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4799])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4789])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4793])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4800])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4790])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4805])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4782])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4804])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4802])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4803])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4784])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4793])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4802])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4798])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4812])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4789])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4771])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4769])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4767])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4801])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4800])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4813])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4791])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4775])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4813])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4773])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4804])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4801])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 78: tensor([0.4789])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4808])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4781])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4782])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4770])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4787])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4789])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4792])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4792])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4804])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4800])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4800])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4823])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4784])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4808])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4791])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4793])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4781])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4790])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4803])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4794])\n",
      "Epoch :  32\n",
      "loss_IOC :  -0.23589624974531476\n",
      "loss_IOC :  -0.2297641051413148\n",
      "loss_IOC :  -0.24092225827259994\n",
      "loss_IOC :  -0.22417175753601187\n",
      "loss_IOC :  -0.23033675132510367\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4792])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4789])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4790])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4769])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4809])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4785])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4790])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4766])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4792])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4772])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4790])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4790])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4779])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4784])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4782])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4783])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4759])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4796])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4774])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4787])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4783])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4793])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4774])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4779])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4757])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4774])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4779])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4784])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4782])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4774])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4800])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4782])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4770])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4793])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4774])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4780])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4754])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4783])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4763])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4781])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4774])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4791])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4788])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4787])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4793])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4789])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4789])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4790])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4785])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4768])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4788])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4804])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4766])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4769])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4776])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4789])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4784])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4776])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4771])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4759])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4780])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4777])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4783])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4790])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4775])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4790])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4798])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4784])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4773])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4780])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4763])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4779])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4779])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4802])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4776])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4781])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4786])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4769])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4792])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4797])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4785])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4785])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4772])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4786])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4791])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4786])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4783])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4782])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4791])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4786])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4781])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4779])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4790])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4770])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4788])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4769])\n",
      "Epoch :  33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_IOC :  -0.2398021814880706\n",
      "loss_IOC :  -0.23390214900294398\n",
      "loss_IOC :  -0.2200174386249379\n",
      "loss_IOC :  -0.23610616698723813\n",
      "loss_IOC :  -0.2307094824278249\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4776])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4771])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4782])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4753])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4802])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4769])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4790])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4780])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4756])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4772])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4765])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4787])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4772])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4749])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4786])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4770])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4780])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4774])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4766])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4788])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4782])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4784])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4763])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4766])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4786])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4776])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4776])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4766])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4769])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4763])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4758])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4775])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4785])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4775])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4767])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4762])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4778])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4794])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4769])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4780])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4768])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4779])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4767])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4744])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4778])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4789])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4759])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4771])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4763])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4784])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4793])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4784])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4761])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4760])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4792])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4801])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4755])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4774])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4774])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4802])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4787])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4759])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4760])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4767])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4782])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4766])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4777])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4769])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4770])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4783])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4765])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4778])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4781])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4789])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4743])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4757])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4785])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4774])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4786])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4769])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4761])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4778])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4779])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4775])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4774])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4775])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4791])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4765])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4748])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4761])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4757])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4771])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4780])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4786])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4782])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4780])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4764])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4774])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4788])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4787])\n",
      "Epoch :  34\n",
      "loss_IOC :  -0.228175676771101\n",
      "loss_IOC :  -0.23804655504166683\n",
      "loss_IOC :  -0.23483887206550735\n",
      "loss_IOC :  -0.23243274791837726\n",
      "loss_IOC :  -0.2415584583286034\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4769])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4757])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4775])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4752])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4756])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4753])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4777])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4762])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4784])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4752])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4761])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4764])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4767])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13: tensor([0.4737])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4754])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4772])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4748])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4756])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4769])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4774])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4765])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4780])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4759])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4781])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4744])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4756])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4760])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4758])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4753])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4749])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4744])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4766])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4772])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4774])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4779])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4789])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4791])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4777])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4757])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4768])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4741])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4760])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4749])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4755])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4775])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4761])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4774])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4773])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4772])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4758])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4751])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4774])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4775])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4765])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4792])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4760])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4764])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4784])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4755])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4773])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4752])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4787])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4761])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4775])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4744])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4745])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4763])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4745])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4804])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4779])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4749])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4746])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4773])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4774])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4748])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4765])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4747])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4745])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4750])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4757])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4780])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4770])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4747])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4756])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4759])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4763])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4760])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4778])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4753])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4770])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4758])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4740])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4773])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4770])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4762])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4750])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4754])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4746])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4751])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4737])\n",
      "Epoch :  35\n",
      "loss_IOC :  -0.22665767023478678\n",
      "loss_IOC :  -0.2247498945797488\n",
      "loss_IOC :  -0.22734097956519034\n",
      "loss_IOC :  -0.23882988282938328\n",
      "loss_IOC :  -0.2402673524103427\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4749])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4762])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4754])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4739])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4767])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4750])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4765])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4736])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4746])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4736])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4755])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4761])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4748])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4760])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4781])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4746])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4759])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4717])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4757])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4749])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4758])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4730])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4749])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4746])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4752])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4726])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 26: tensor([0.4737])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4740])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4748])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4750])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4748])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4756])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4745])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4768])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4764])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4757])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4752])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4746])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4753])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4730])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4755])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4748])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4758])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4766])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4759])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4742])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4764])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4760])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4739])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4749])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4754])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4727])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4740])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4759])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4752])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4761])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4744])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4752])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4730])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4735])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4736])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4732])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4760])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4724])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4742])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4752])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4741])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4764])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4722])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4748])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4724])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4766])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4772])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4741])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4742])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4722])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4760])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4751])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4766])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4742])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4744])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4754])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4741])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4760])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4751])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4749])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4758])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4732])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4751])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4769])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4742])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4760])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4752])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4746])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4741])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4752])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4754])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4734])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4739])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4769])\n",
      "Epoch :  36\n",
      "loss_IOC :  -0.22090890654998085\n",
      "loss_IOC :  -0.22477016505440772\n",
      "loss_IOC :  -0.2379279383337694\n",
      "loss_IOC :  -0.23511536570036667\n",
      "loss_IOC :  -0.22915003058447037\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4747])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4744])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4705])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4711])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4736])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4757])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4728])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4722])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4748])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4717])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4748])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4712])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4723])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4746])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4729])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4745])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4765])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4763])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4737])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4755])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4714])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4749])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4716])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 23: tensor([0.4722])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4761])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4739])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4729])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4759])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4759])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4727])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4732])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4746])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4730])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4744])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4758])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4723])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4738])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4730])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4757])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4734])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4768])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4783])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4745])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4717])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4739])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4735])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4740])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4720])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4735])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4720])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4730])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4730])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4731])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4741])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4753])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4745])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4736])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4735])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4752])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4749])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4751])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4725])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4748])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4749])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4746])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4720])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4747])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4727])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4751])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4755])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4748])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4754])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4735])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4725])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4744])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4723])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4750])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4746])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4736])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4764])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4743])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4738])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4767])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4752])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4737])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4737])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4737])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4762])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4724])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4725])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4757])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4713])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4730])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4754])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4725])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4747])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4725])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4717])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4732])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4723])\n",
      "Epoch :  37\n",
      "loss_IOC :  -0.22058036338696313\n",
      "loss_IOC :  -0.24480714861545866\n",
      "loss_IOC :  -0.21648958905002874\n",
      "loss_IOC :  -0.23546348702457998\n",
      "loss_IOC :  -0.23728452156603158\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4719])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4728])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4754])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4739])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4700])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4752])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4735])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4736])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4741])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4710])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4719])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4739])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4710])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4716])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4713])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4728])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4726])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4740])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4711])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4724])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4732])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4721])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4760])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4736])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4727])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25: tensor([0.4727])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4725])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4717])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4690])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4745])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4715])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4717])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4707])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4732])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4744])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4720])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4706])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4736])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4721])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4706])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4718])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4723])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4721])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4744])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4724])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4711])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4696])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4726])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4712])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4702])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4720])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4704])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4723])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4723])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4702])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4716])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4718])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4750])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4731])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4752])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4730])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4730])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4714])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4727])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4728])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4741])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4731])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4729])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4714])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4726])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4716])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4720])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4734])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4721])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4716])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4737])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4727])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4706])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4713])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4731])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4761])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4716])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4737])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4741])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4749])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4709])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4726])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4750])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4752])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4720])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4739])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4715])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4726])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4716])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4713])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4716])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4718])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4737])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4717])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4726])\n",
      "Epoch :  38\n",
      "loss_IOC :  -0.2319807598054593\n",
      "loss_IOC :  -0.23367956165145665\n",
      "loss_IOC :  -0.2317769462985087\n",
      "loss_IOC :  -0.23367220502773894\n",
      "loss_IOC :  -0.22711752223448622\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4693])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4690])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4708])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4723])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4723])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4721])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4716])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4710])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4726])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4725])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4729])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4703])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4684])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4730])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4712])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4721])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4719])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4726])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4719])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4707])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4716])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4709])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4719])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4714])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4720])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4713])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4700])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4711])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4721])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4700])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4697])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 31: tensor([0.4705])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4719])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4694])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4724])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4727])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4705])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4708])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4730])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4714])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4724])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4703])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4704])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4719])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4706])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4725])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4693])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4688])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4713])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4712])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4710])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4709])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4728])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4698])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4706])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4704])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4706])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4733])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4710])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4716])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4708])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4687])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4727])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4730])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4694])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4711])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4705])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4711])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4695])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4704])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4723])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4698])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4719])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4700])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4719])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4699])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4727])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4716])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4708])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4698])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4707])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4725])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4714])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4702])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4685])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4712])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4717])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4723])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4722])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4723])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4719])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4698])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4717])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4751])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4695])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4733])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4699])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4729])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4716])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4704])\n",
      "Epoch :  39\n",
      "loss_IOC :  -0.22927018489110457\n",
      "loss_IOC :  -0.23630811593364148\n",
      "loss_IOC :  -0.22663223647490932\n",
      "loss_IOC :  -0.22829735690507763\n",
      "loss_IOC :  -0.22473220037495\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4698])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4704])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4701])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4710])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4686])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4724])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4700])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4696])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4694])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4724])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4712])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4708])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4676])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4703])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4703])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4708])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4691])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4723])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4716])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4725])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4700])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4720])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4695])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4715])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4686])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4704])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4718])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4684])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4721])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4675])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4720])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4708])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4689])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4724])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4696])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4719])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4692])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 37: tensor([0.4700])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4702])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4709])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4692])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 41: tensor([0.4719])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4703])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4686])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4694])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4718])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4704])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4699])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4732])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4697])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4703])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4713])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4684])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4676])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4692])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4710])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4697])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4678])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4709])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4699])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4686])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4711])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4721])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4708])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4675])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4724])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4705])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4683])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4716])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4691])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4712])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4706])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 72: tensor([0.4730])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 73: tensor([0.4720])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 74: tensor([0.4728])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 75: tensor([0.4703])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 76: tensor([0.4693])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 77: tensor([0.4714])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 78: tensor([0.4711])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 79: tensor([0.4692])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 80: tensor([0.4721])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 81: tensor([0.4696])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 82: tensor([0.4706])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 83: tensor([0.4726])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 84: tensor([0.4700])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 85: tensor([0.4706])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 86: tensor([0.4712])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 87: tensor([0.4695])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 88: tensor([0.4706])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 89: tensor([0.4722])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 90: tensor([0.4699])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 91: tensor([0.4699])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 92: tensor([0.4718])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 93: tensor([0.4709])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 94: tensor([0.4695])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 95: tensor([0.4716])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 96: tensor([0.4679])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 97: tensor([0.4732])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 98: tensor([0.4706])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 99: tensor([0.4699])\n",
      "Epoch :  40\n",
      "loss_IOC :  -0.2242803494068379\n",
      "loss_IOC :  -0.23373291625323844\n",
      "loss_IOC :  -0.23340757658317418\n",
      "loss_IOC :  -0.23949249152750818\n",
      "loss_IOC :  -0.22462654140465088\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 0: tensor([0.4681])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 1: tensor([0.4708])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 2: tensor([0.4690])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 3: tensor([0.4693])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 4: tensor([0.4708])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 5: tensor([0.4674])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 6: tensor([0.4699])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 7: tensor([0.4700])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 8: tensor([0.4677])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 9: tensor([0.4689])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 10: tensor([0.4681])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 11: tensor([0.4700])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 12: tensor([0.4692])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 13: tensor([0.4703])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 14: tensor([0.4691])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 15: tensor([0.4699])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 16: tensor([0.4708])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 17: tensor([0.4662])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 18: tensor([0.4661])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 19: tensor([0.4694])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 20: tensor([0.4695])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 21: tensor([0.4707])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 22: tensor([0.4689])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 23: tensor([0.4693])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 24: tensor([0.4708])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 25: tensor([0.4693])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 26: tensor([0.4688])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 27: tensor([0.4675])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 28: tensor([0.4707])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 29: tensor([0.4712])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 30: tensor([0.4700])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 31: tensor([0.4662])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 32: tensor([0.4710])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 33: tensor([0.4693])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 34: tensor([0.4688])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 35: tensor([0.4687])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 36: tensor([0.4727])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 37: tensor([0.4710])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 38: tensor([0.4705])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 39: tensor([0.4706])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 40: tensor([0.4705])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 41: tensor([0.4677])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 42: tensor([0.4701])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 43: tensor([0.4710])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 44: tensor([0.4709])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 45: tensor([0.4678])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 46: tensor([0.4701])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 47: tensor([0.4676])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 48: tensor([0.4702])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 49: tensor([0.4695])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 50: tensor([0.4688])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 51: tensor([0.4687])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 52: tensor([0.4745])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 53: tensor([0.4679])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 54: tensor([0.4704])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 55: tensor([0.4716])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 56: tensor([0.4679])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 57: tensor([0.4682])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 58: tensor([0.4713])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 59: tensor([0.4697])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 60: tensor([0.4704])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 61: tensor([0.4661])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 62: tensor([0.4701])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 63: tensor([0.4700])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 64: tensor([0.4678])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 65: tensor([0.4707])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 66: tensor([0.4701])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 67: tensor([0.4699])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 68: tensor([0.4694])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 69: tensor([0.4668])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 70: tensor([0.4667])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n",
      "Episode 71: tensor([0.4697])\n",
      "state : \n",
      "agent.get_action(state)[0] :  [[ 1. -1.  1.]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmean(demo_rewards) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.99\u001b[39m:\n\u001b[0;32m     43\u001b[0m         is_early_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m episode_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mmini_batch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 21\u001b[0m, in \u001b[0;36mmini_batch_train\u001b[1;34m(env, agent, max_episodes, max_steps, batch_size)\u001b[0m\n\u001b[0;32m     18\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mreplay_buffer) \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[1;32m---> 21\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;129;01mor\u001b[39;00m step \u001b[38;5;241m==\u001b[39m max_steps\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     24\u001b[0m     episode_rewards\u001b[38;5;241m.\u001b[39mappend(episode_reward)\n",
      "Cell \u001b[1;32mIn[15], line 72\u001b[0m, in \u001b[0;36mDDPGAgent.update\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     71\u001b[0m         q_loss\u001b[38;5;241m.\u001b[39mbackward() \n\u001b[1;32m---> 72\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;66;03m# update actor\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m#         policy_loss = -self.critic.forward(state_batch, self.actor.forward(state_batch)).mean()\u001b[39;00m\n\u001b[0;32m     76\u001b[0m         policy_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mforward(state_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mestimate_action(state_batch)[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 213\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:263\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    262\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m--> 263\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable:\n\u001b[0;32m    266\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(NUM_EPOCHS):\n",
    "    print(\"Epoch : \", i)\n",
    "    \n",
    "    samp_trajs = [generate_session(t_max=max_steps,\n",
    "                                   agent=agent,\n",
    "                                   is_policy_inference=False) for _ in range(EPISODES_TO_PLAY)]\n",
    "    \n",
    "    D_samp = preprocess_traj(traj_list_=samp_trajs,\n",
    "                             step_tensor=D_samp,\n",
    "                             is_Demo=False)\n",
    "    \n",
    "    loss_reward, loss_policy = [], []\n",
    "    samp_reward_values = []\n",
    "    is_early_stop = False\n",
    "    \n",
    "    for _ in range(REWARD_FUNCTION_UPDATE):\n",
    "        \n",
    "        selected_samp = np.random.choice(len(D_samp), DEMO_BATCH, replace=True)\n",
    "        selected_demo = np.random.choice(len(D_demo), DEMO_BATCH, replace=True)\n",
    "        \n",
    "        D_s_samp = D_samp[selected_samp].clone().detach()\n",
    "        D_s_demo = D_demo[selected_demo].clone().detach()\n",
    "        \n",
    "        D_s_samp = torch.cat((D_s_demo, D_s_samp), dim=0)\n",
    "        D_sr_samp = D_s_samp[torch.randperm(int(D_s_samp.size(0)))]\n",
    "        \n",
    "        states_robot, log_probs_robot, actions_robot = D_sr_samp[:, :3], D_sr_samp[:, 3:4], D_sr_samp[:, 4:]\n",
    "        states_expert, actions_expert = D_s_demo[:, :3], D_s_demo[:, 4:]\n",
    "        \n",
    "        samp_rewards = updater_obj.reward_network.estimate_reward(state_action=states_robot.float(), is_inference=False)\n",
    "        demo_rewards = updater_obj.reward_network.estimate_reward(state_action=states_expert.float(), is_inference=False)\n",
    "        \n",
    "        loss_IOC = - torch.mean(demo_rewards) + \\\n",
    "            torch.log(torch.mean(torch.exp(samp_rewards) / (torch.exp(log_probs_robot) + 1e-7)))\n",
    "        \n",
    "        print(\"loss_IOC : \", loss_IOC.detach().numpy())\n",
    "        \n",
    "        updater_obj.run_reward_optimizer(irl_loss=loss_IOC)\n",
    "        \n",
    "        loss_reward.append(loss_IOC.detach().item())\n",
    "        \n",
    "        if torch.mean(demo_rewards) >= 0.99:\n",
    "            is_early_stop = True\n",
    "    \n",
    "    episode_rewards = mini_batch_train(env, agent, max_episodes, max_steps, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b99bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(demo_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39befbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d77ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(samp_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9855cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs_robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326f1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(log_probs_robot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a292b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(log_probs_robot) + 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69548400",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(samp_rewards) / (torch.exp(log_probs_robot) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1cc244",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.6320 / 1.6300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564a2638",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8edd0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs_robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67218177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.mean(torch.exp(samp_rewards) / (torch.exp(log_probs_robot) + 1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af776e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(torch.mean(torch.exp(samp_rewards) / (torch.exp(log_probs_robot) + 1e-7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc891170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4280e6be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
