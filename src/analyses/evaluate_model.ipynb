{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "308cf3d3",
   "metadata": {},
   "source": [
    "# Evaluation Analysis of Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a25ce4a",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb759d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from typing import List, Optional\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# get the current script's directory\n",
    "current_directory = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in locals() else os.getcwd()\n",
    "# get the parent directory\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "# add the parent directory to the sys.path at the beginning\n",
    "sys.path.insert(0, parent_directory)\n",
    "\n",
    "from utils import constants, common\n",
    "from utils.dataset_loader import PolicyDatasetLoader\n",
    "\n",
    "from optimization import functions\n",
    "from optimization.updater import Updater\n",
    "\n",
    "from environment.environment import RobotEnvironment\n",
    "from environment.buffer import ReplayBuffer\n",
    "\n",
    "from models.policy_model import RobotPolicy\n",
    "from models.reward_model import RewardFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d94bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",\n",
    "              None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c6932b",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83399cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_2D_plotting(df: pd.DataFrame,\n",
    "                    column_names: List[str],\n",
    "                    x_axis_name: str,\n",
    "                    y_axis_name: str,\n",
    "                    x_label_name: str,\n",
    "                    title_label_name: str) -> None:\n",
    "    \n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
    "    if not isinstance(column_names, list) or not all(isinstance(name, str) for name in column_names):\n",
    "        raise TypeError(\"Input 'column_names' must be a list of strings.\")\n",
    "    if not all(name in df.columns for name in column_names):\n",
    "        raise ValueError(\"Invalid column name(s) for column_names.\")\n",
    "    if not all(isinstance(label, str) for label in [x_axis_name, y_axis_name, title_label_name]):\n",
    "        raise TypeError(\"Axis labels and title must be strings.\")\n",
    "\n",
    "    for column_name in column_names:\n",
    "        if column_name not in df.columns:\n",
    "            raise ValueError(f\"Invalid column name: {column_name}\")\n",
    "\n",
    "    if len(column_names) not in [1, 2]:\n",
    "        raise ValueError(\"Invalid number of columns to plot. Must be 1 or 2.\")\n",
    "\n",
    "    if not isinstance(x_axis_name, str) or not isinstance(y_axis_name, str):\n",
    "        raise TypeError(\"Axis names must be strings.\")\n",
    "    if not isinstance(x_label_name, str):\n",
    "        raise TypeError(\"Column name for x axis must be string.\")\n",
    "    if not isinstance(title_label_name, str):\n",
    "        raise TypeError(\"Title must be a string.\")\n",
    "    \n",
    "    for column_name in column_names:\n",
    "        plt.scatter(df[x_label_name], df[column_name],\n",
    "                    label=column_name)\n",
    "    \n",
    "    if len(column_names) == 2:\n",
    "        mae_avg = np.mean(np.abs(df[column_names[0]] - df[column_names[1]]))\n",
    "        mae_max = np.max(np.abs(df[column_names[0]] - df[column_names[1]]))\n",
    "        \n",
    "        error_text = f\"Avg. MAE = {mae_avg:.5f} m\\nMax. MAE = {mae_max:.5f} m\"\n",
    "        \n",
    "        plt.text(0.95, 0.05,\n",
    "                 error_text,\n",
    "                 transform=plt.gca().transAxes,\n",
    "                 verticalalignment=\"bottom\",\n",
    "                 horizontalalignment=\"right\",\n",
    "                 bbox=dict(facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    plt.xlabel(x_axis_name)\n",
    "    plt.ylabel(y_axis_name)\n",
    "    plt.title(title_label_name)\n",
    "    \n",
    "    plt.legend(loc=\"upper right\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d0a7f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2D_label_vs_pred_states(axs: List[plt.Axes],\n",
    "                                 colors: List[str],\n",
    "                                 state_number_series: pd.Series,\n",
    "                                 label_data_df: pd.DataFrame,\n",
    "                                 estim_data_df: pd.DataFrame) -> None:\n",
    "    \n",
    "    if not isinstance(axs, list):\n",
    "        raise TypeError(\"Input 'axs' in plot_2D_label_vs_pred_states function must be a list.\")\n",
    "    if not all(isinstance(ax, plt.Axes) for ax in axs):\n",
    "        raise TypeError(\"All elements in 'axs' must be instances of matplotlib.axes._subplots.AxesSubplot.\")\n",
    "    if not isinstance(colors, list):\n",
    "        raise TypeError(\"Input 'colors' in plot_2D_label_vs_pred_states function must be a list.\")\n",
    "    if not all(isinstance(color, str) for color in colors):\n",
    "        raise TypeError(\"All elements in 'colors' must be strings representing colors.\")\n",
    "    if not isinstance(state_number_series, pd.Series):\n",
    "        raise TypeError(\"Input 'state_number_series' in plot_2D_label_vs_pred_states function must be a pandas Series.\")\n",
    "    if not isinstance(label_data_df, pd.DataFrame):\n",
    "        raise TypeError(\"Input 'label_data_df' in plot_2D_label_vs_pred_states function must be a pandas DataFrame.\")\n",
    "    if not isinstance(estim_data_df, pd.DataFrame):\n",
    "        raise TypeError(\"Input 'estim_data_df' in plot_2D_label_vs_pred_states function must be a pandas DataFrame.\")\n",
    "    \n",
    "    label_distance_to_object = label_data_df.iloc[:, 0]\n",
    "    label_distance_to_target = label_data_df.iloc[:, 1]\n",
    "    label_distance_to_ground = label_data_df.iloc[:, 2]\n",
    "    \n",
    "    estim_distance_to_object = estim_data_df.iloc[:, 0]\n",
    "    estim_distance_to_target = estim_data_df.iloc[:, 1]\n",
    "    estim_distance_to_ground = estim_data_df.iloc[:, 2]\n",
    "    \n",
    "    if len(axs) == 1:\n",
    "        \n",
    "        axs[0].plot(state_number_series, label_distance_to_target,\n",
    "                    color=colors[0],\n",
    "                    label=\"Label: Distance to Target\")\n",
    "        axs[0].plot(state_number_series, estim_distance_to_target,\n",
    "                    color=colors[3],\n",
    "                    linestyle=\"--\",\n",
    "                    label=\"Estimation: Distance to Target\")\n",
    "        \n",
    "        axs[0].plot(state_number_series, label_distance_to_object,\n",
    "                    color=colors[1],\n",
    "                    label=\"Label: Distance to Obstacle\")\n",
    "        axs[0].plot(state_number_series, estim_distance_to_object,\n",
    "                    color=colors[4],\n",
    "                    linestyle=\"--\",\n",
    "                    label=\"Estimation: Distance to Obstacle\")\n",
    "        \n",
    "        axs[0].plot(state_number_series, label_distance_to_ground,\n",
    "                    color=colors[2],\n",
    "                    label=\"Distance to Ground\")\n",
    "        axs[0].plot(state_number_series, estim_distance_to_ground,\n",
    "                    color=colors[5],\n",
    "                    linestyle=\"--\",\n",
    "                    label=\"Estimation: Distance to Ground\")\n",
    "    \n",
    "    else:\n",
    "        axs[0].plot(state_number_series, label_distance_to_target,\n",
    "                    color=colors[0])\n",
    "        axs[0].set_title(\"Distance to Target\")\n",
    "        axs[1].plot(state_number_series, label_distance_to_object,\n",
    "                    color=colors[1])\n",
    "        axs[1].set_title(\"Distance to Obstacle\")\n",
    "        axs[2].plot(state_number_series, label_distance_to_ground,\n",
    "                    color=colors[2])\n",
    "        axs[2].set_title(\"Distance to Ground\")\n",
    "        \n",
    "        axs[0].plot(state_number_series, estim_distance_to_target,\n",
    "                    color=colors[3],\n",
    "                    linestyle=\"--\")\n",
    "        axs[1].plot(state_number_series, estim_distance_to_object,\n",
    "                    color=colors[4],\n",
    "                    linestyle=\"--\")\n",
    "        axs[2].plot(state_number_series, estim_distance_to_ground,\n",
    "                    color=colors[5],\n",
    "                    linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7273d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D_vector(ax: Axes3D,\n",
    "                   list_trajectory: List[torch.Tensor],\n",
    "                   color: str,\n",
    "                   traj_s: int,\n",
    "                   start_s: int,\n",
    "                   end_s: int,\n",
    "                   label: str,\n",
    "                   env: Optional[RobotEnvironment] = None) -> None:\n",
    "\n",
    "    if not isinstance(list_trajectory, list):\n",
    "        raise TypeError(\"list_trajectory should be a list of torch.Tensor\")\n",
    "    if not all(isinstance(position, torch.Tensor) for position in list_trajectory):\n",
    "        raise TypeError(\"Each element in list_trajectory should be a torch.Tensor\")\n",
    "    \n",
    "    state_1 = [object_dist[0].item() for object_dist in list_trajectory]\n",
    "    state_2 = [target_dist[1].item() for target_dist in list_trajectory]\n",
    "    state_3 = [ground_dist[2].item() for ground_dist in list_trajectory]\n",
    "    \n",
    "    scatter_traj = ax.scatter(state_1, state_2, state_3,\n",
    "                              c=color, marker=\"o\", s=traj_s, label=label)\n",
    "    \n",
    "    scatter_start = ax.scatter(state_1[0], state_2[0], state_3[0],\n",
    "                               c=\"green\", marker=\"*\", s=start_s, label=\"Start Position\")\n",
    "    scatter_end = ax.scatter(state_1[-1], state_2[-1], state_3[-1],\n",
    "                             c=\"blue\", marker=\"X\", s=end_s, label=\"End Position\")\n",
    "    \n",
    "    # do not specify environment object when the input is state vector\n",
    "    if env is not None:\n",
    "        target_location_normalized = common.normalize_action(action=np.array(constants.TARGET_LOCATION).reshape((1, len(constants.TARGET_LOCATION))),\n",
    "                                                             norm_range_list=env.action_norms[:3])[0]\n",
    "        scatter_target = ax.scatter(target_location_normalized[0], target_location_normalized[1], target_location_normalized[2],\n",
    "                                    c=\"black\", marker=\"+\", s=end_s, label=\"Target Location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d241b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D_vector_comparison(ax: plt.Axes,\n",
    "                              label_trajectory_df: pd.DataFrame,\n",
    "                              estim_trajectory_df: pd.DataFrame,\n",
    "                              object_position: List[float],\n",
    "                              colors: List[str],\n",
    "                              traj_s: int,\n",
    "                              start_s: int,\n",
    "                              end_s: int) -> None:\n",
    "    \n",
    "    if not isinstance(ax, plt.Axes):\n",
    "        raise TypeError(\"Input 'ax' must be an instance of matplotlib Axes.\")\n",
    "    if not isinstance(label_trajectory_df, pd.DataFrame) or not isinstance(estim_trajectory_df, pd.DataFrame):\n",
    "        raise TypeError(\"Input 'label_trajectory_df' and 'estim_trajectory_df' must be pandas DataFrames.\")\n",
    "    if not isinstance(object_position, list) or len(object_position) != 3 or not all(isinstance(pos, (int, float)) for pos in object_position):\n",
    "        raise TypeError(\"Input 'object_position' must be a list of three integers or floats.\")\n",
    "    if not isinstance(colors, list):\n",
    "        raise TypeError(\"Input 'colors' in plot_trajectory function must be a list.\")\n",
    "    if not all(isinstance(color, str) for color in colors):\n",
    "        raise TypeError(\"All elements in 'colors' must be strings representing colors.\")\n",
    "    if not isinstance(traj_s, int) or not isinstance(start_s, int) or not isinstance(end_s, int):\n",
    "        raise TypeError(\"Input 'traj_s', 'start_s', and 'end_s' must be integers.\")\n",
    "    \n",
    "    # Additional checks for DataFrame columns\n",
    "    for col_df, col_name in zip([label_trajectory_df, estim_trajectory_df], ['label_trajectory_df', 'estim_trajectory_df']):\n",
    "        for col in col_df.columns:\n",
    "            if not isinstance(col_df[col], pd.Series):\n",
    "                raise TypeError(f\"Column '{col}' of '{col_name}' must be a pandas Series.\")\n",
    "    \n",
    "    start_index = 0\n",
    "    end_index = -1\n",
    "    \n",
    "    x_label = label_trajectory_df.iloc[:, 0]\n",
    "    y_label = label_trajectory_df.iloc[:, 1]\n",
    "    z_label = label_trajectory_df.iloc[:, 2]\n",
    "    \n",
    "    x_estim = estim_trajectory_df.iloc[:, 0]\n",
    "    y_estim = estim_trajectory_df.iloc[:, 1]\n",
    "    z_estim = estim_trajectory_df.iloc[:, 2]\n",
    "    \n",
    "    if not isinstance(x_label, pd.Series) or not isinstance(y_label, pd.Series) or not isinstance(z_label, pd.Series):\n",
    "        raise TypeError(\"Columns of 'label_trajectory_df' must be pandas Series.\")\n",
    "    if not isinstance(x_estim, pd.Series) or not isinstance(y_estim, pd.Series) or not isinstance(z_estim, pd.Series):\n",
    "        raise TypeError(\"Columns of 'estim_trajectory_df' must be pandas Series.\")\n",
    "    \n",
    "    scatter_traj = ax.scatter(x_label, y_label, z_label,\n",
    "                              c=colors[0], marker=\"o\", s=traj_s, label=\"Human Demonstration\")\n",
    "    scatter_traj = ax.scatter(x_estim, y_estim, z_estim,\n",
    "                              c=colors[1], marker=\"o\", s=traj_s, label=\"Robot Execution\")\n",
    "    \n",
    "    scatter_start = ax.scatter(x_label.iloc[start_index], y_label.iloc[start_index], z_label.iloc[start_index],\n",
    "                               c=\"green\", marker=\"*\", s=start_s, label=\"Start Position\")\n",
    "    scatter_end = ax.scatter(x_label.iloc[end_index], y_label.iloc[end_index], z_label.iloc[end_index],\n",
    "                             c=\"blue\", marker=\"X\", s=end_s, label=\"Demonstration End Position\")\n",
    "    scatter_end = ax.scatter(x_estim.iloc[end_index], y_estim.iloc[end_index], z_estim.iloc[end_index],\n",
    "                             c=\"lightblue\", marker=\"X\", s=end_s, label=\"Robot End Position\")\n",
    "    \n",
    "    ax.plot(x_label, y_label, z_label,\n",
    "            color=colors[0], linestyle=\"--\", linewidth=1)\n",
    "    ax.plot(x_estim, y_estim, z_estim,\n",
    "            color=colors[1], linestyle=\"--\", linewidth=1)\n",
    "    \n",
    "    scatter_object = ax.scatter(object_position[0], object_position[1], object_position[2],\n",
    "                                c=\"red\", marker=\"s\", s=80, label=\"Obstacle Location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40beb4a2",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfe2802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# available training machine\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Training Device: \", device)\n",
    "\n",
    "configs = functions.setup_config(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd6b530",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ec536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and return preliminary base paths\n",
    "json_paths, results_path = functions.get_directories(parent_directory=parent_directory,\n",
    "                                                     data_folder_name=constants.TEST_COLLECTION_DATE)\n",
    "\n",
    "# load test demonstrations dataset\n",
    "all_data = PolicyDatasetLoader(demo_data_json_paths=json_paths)\n",
    "\n",
    "# get all indice numbers where the new trajectory is initialized in the dataset\n",
    "trajectory_indices = functions.find_indices_of_trajectory_changes(dataset=all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59687a66",
   "metadata": {},
   "source": [
    "## Create and Load Pre-Trained Reward and Policy Models and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd4ce157",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_network = RobotPolicy(state_size=configs.state_size,\n",
    "                             hidden_size=configs.hidden_size,\n",
    "                             out_size=configs.action_size,\n",
    "                             log_std_min=configs.policy_log_std_min,\n",
    "                             log_std_max=configs.policy_log_std_max,\n",
    "                             device=configs.device)\n",
    "reward_network = RewardFunction(state_size=configs.state_size,\n",
    "                                action_size=configs.action_size,\n",
    "                                hidden_size=configs.hidden_size,\n",
    "                                out_size=configs.reward_size,\n",
    "                                device=configs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1fb6f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder name where policy model parameters are located (\"results / policy_network_params / loading_folder_name\")\n",
    "policy_loading_folder_name = constants.POLICY_LOADING_FOLDER\n",
    "policy_params_name = constants.POLICY_PARAMS_NAME\n",
    "\n",
    "# folder name where reward model parameters are located (\"results / reward_network_params / loading_folder_name\")\n",
    "reward_loading_folder_name = constants.REWARD_LOADING_FOLDER\n",
    "reward_params_name = constants.REWARD_PARAMS_NAME\n",
    "\n",
    "# load pretrained policy network parameters if the pre-trained model is available\n",
    "policy_network = functions.load_policy_from_path(policy_network=policy_network,\n",
    "                                                 results_path=results_path,\n",
    "                                                 policy_loading_folder_name=policy_loading_folder_name,\n",
    "                                                 policy_params_name=policy_params_name)\n",
    "\n",
    "# load pretrained reward network parameters if the pre-trained model is available\n",
    "reward_network = functions.load_reward_from_path(reward_network=reward_network,\n",
    "                                                 results_path=results_path,\n",
    "                                                 reward_loading_folder_name=reward_loading_folder_name,\n",
    "                                                 reward_params_name=reward_params_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db321ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loss functions\n",
    "updater_obj = Updater(configs=configs,\n",
    "                      policy_network=policy_network,\n",
    "                      reward_network=reward_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3559b79",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30ff6f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct custom environment for reward function training\n",
    "env = RobotEnvironment()\n",
    "env.set_reward_network(reward_network)\n",
    "env.is_reward_inference = True\n",
    "\n",
    "# create a replay buffer class object\n",
    "replay_buffer = ReplayBuffer(capacity=configs.replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce2e06",
   "metadata": {},
   "source": [
    "## Test Trained Models on Demonstration Dataset Samples Individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b59328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty tensors for demonstration and sample trajectories\n",
    "trajectory_dataframes = []\n",
    "data_demo_tensor, data_robo_tensor = torch.tensor([]), torch.tensor([])\n",
    "\n",
    "# get list of demonstration trajectories for training and analysis\n",
    "for traj_start_index in range(len(trajectory_indices)):\n",
    "    traj_df, _, _, _ = functions.get_estimated_rewards(configs=configs,\n",
    "                                                       updater_obj=updater_obj,\n",
    "                                                       data_loader=all_data,\n",
    "                                                       policy_network=policy_network,\n",
    "                                                       reward_network=reward_network,\n",
    "                                                       trajectory_indices=trajectory_indices,\n",
    "                                                       traj_start_index=traj_start_index,\n",
    "                                                       is_inference_reward=True,\n",
    "                                                       is_inference_policy=True,\n",
    "                                                       is_deterministic=True)\n",
    "    trajectory_dataframes.append(traj_df)\n",
    "    del traj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d4169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose one trajectory to analyze out of human demonstrations\n",
    "test_trajectory = trajectory_dataframes[1]\n",
    "test_trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac05c7",
   "metadata": {},
   "source": [
    "## Visualize Human Demonstration vs. Trained Policy Execution Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedae594",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_1_columns = [constants.ACTION_PREDICTION_DENORMALIZED_NAME + \"_1\",\n",
    "                    constants.ACTION_DENORMALIZED_LABEL_NAME + \"_1\"]\n",
    "action_1_title_name = \"Prediction vs. Actual End-Effector X-Axis Position\"\n",
    "action_1_y_axis_name = \"X-Axis Location [m]\"\n",
    "\n",
    "plt_2D_plotting(df=test_trajectory,\n",
    "                column_names=action_1_columns,\n",
    "                x_axis_name=\"State Counter [#]\",\n",
    "                y_axis_name=action_1_y_axis_name,\n",
    "                x_label_name=constants.STATE_NUMBER_COLUMN,\n",
    "                title_label_name=action_1_title_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03afe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_3_columns = [constants.STATE_ESTIMATION_DENORMALIZED_NAME + \"_3\",\n",
    "                    constants.STATE_DENORMALIZED_LABEL_NAME + \"_3\"]\n",
    "state_3_title_name = \"Prediction vs. Actual End-Effector Distance to the Ground Level\"\n",
    "state_3_y_axis_name = \"State-3: Distance to the Ground [m]\"\n",
    "\n",
    "plt_2D_plotting(df=test_trajectory,\n",
    "                column_names=state_3_columns,\n",
    "                x_axis_name=\"State Counter [#]\",\n",
    "                y_axis_name=state_3_y_axis_name,\n",
    "                x_label_name=constants.STATE_NUMBER_COLUMN,\n",
    "                title_label_name=state_3_title_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5d6a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "state_number_series = test_trajectory[constants.STATE_NUMBER_COLUMN]\n",
    "state_label_df = test_trajectory[[\n",
    "    constants.STATE_DENORMALIZED_LABEL_NAME + \"_1\",\n",
    "    constants.STATE_DENORMALIZED_LABEL_NAME + \"_2\",\n",
    "    constants.STATE_DENORMALIZED_LABEL_NAME + \"_3\",\n",
    "]]\n",
    "state_estim_df = test_trajectory[[\n",
    "    constants.STATE_ESTIMATION_DENORMALIZED_NAME + \"_1\",\n",
    "    constants.STATE_ESTIMATION_DENORMALIZED_NAME + \"_2\",\n",
    "    constants.STATE_ESTIMATION_DENORMALIZED_NAME + \"_3\",\n",
    "]]\n",
    "\n",
    "plot_2D_label_vs_pred_states(axs=[ax],\n",
    "                             colors=[\"green\", \"blue\", \"red\", \"lightgreen\", \"lightblue\", \"lightcoral\"],\n",
    "                             state_number_series=state_number_series,\n",
    "                             label_data_df=state_label_df,\n",
    "                             estim_data_df=state_estim_df)\n",
    "\n",
    "ax.set_xlabel(\"State Number [#]\")\n",
    "ax.set_ylabel(\"Distance [m]\")\n",
    "ax.set_title(\"Label vs. Estimation State Vector Over Time\")\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.02, 1),\n",
    "          loc=\"upper left\", borderaxespad=0.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable interactive plots in Jupyter Notebook\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,\n",
    "                     projection=\"3d\")\n",
    "\n",
    "label_position_df = test_trajectory[[\n",
    "    constants.ACTION_DENORMALIZED_LABEL_NAME + \"_1\",\n",
    "    constants.ACTION_DENORMALIZED_LABEL_NAME + \"_2\",\n",
    "    constants.ACTION_DENORMALIZED_LABEL_NAME + \"_3\"\n",
    "]]\n",
    "estim_position_df = test_trajectory[[\n",
    "    constants.ACTION_PREDICTION_DENORMALIZED_NAME + \"_1\",\n",
    "    constants.ACTION_PREDICTION_DENORMALIZED_NAME + \"_2\",\n",
    "    constants.ACTION_PREDICTION_DENORMALIZED_NAME + \"_3\"\n",
    "]]\n",
    "\n",
    "plot_3D_vector_comparison(ax=ax,\n",
    "                          label_trajectory_df=label_position_df,\n",
    "                          estim_trajectory_df=estim_position_df,\n",
    "                          object_position=constants.OBSTACLE_LOCATION,\n",
    "                          colors=[\"black\", \"grey\"],\n",
    "                          traj_s=20,\n",
    "                          start_s=120,\n",
    "                          end_s=85)\n",
    "\n",
    "ax.set_xlabel(\"X Position [m]\")\n",
    "ax.set_ylabel(\"Y Position [m]\")\n",
    "ax.set_zlabel(\"Z Position [m]\")\n",
    "ax.set_title(\"Demonstration vs. Execution End-Effector 3D Positions Over Time\")\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.02, 1),\n",
    "          loc=\"upper left\", borderaxespad=0.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1959fe4",
   "metadata": {},
   "source": [
    "## Test Models on Trained Policy Generated Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0cf38cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_to_run = 10\n",
    "\n",
    "# create episodes of trajectories by running trained policy model\n",
    "# take deterministic actions for behavior cloning by setting is_deterministic=True to use mean actions from distribution\n",
    "robot_trajectories = [functions.generate_session(env=env,\n",
    "                                                 t_max=constants.TRAJECTORY_SIZE,\n",
    "                                                 updater_obj=updater_obj,\n",
    "                                                 replay_buffer=replay_buffer,\n",
    "                                                 policy_network=policy_network,\n",
    "                                                 is_policy_inference=True,\n",
    "                                                 is_policy_gradient_update=False,\n",
    "                                                 is_deterministic=True) for _ in range(episodes_to_run)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "318c07ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose one randomly initialized state robot trajectory to analyze\n",
    "robot_trajectory = robot_trajectories[-1]\n",
    "\n",
    "state_tensor_list = robot_trajectory[0]\n",
    "action_tensor_list = robot_trajectory[1]\n",
    "reward_tensor_list = robot_trajectory[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3010bce7",
   "metadata": {},
   "source": [
    "## Visualize Randomly Initialized State Trajectory Execution of Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b271b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable interactive plots in Jupyter Notebook\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,\n",
    "                     projection=\"3d\")\n",
    "\n",
    "plot_3D_vector(ax=ax,\n",
    "               list_trajectory=state_tensor_list,\n",
    "               color=\"black\",\n",
    "               traj_s=20,\n",
    "               start_s=120,\n",
    "               end_s=85,\n",
    "               label=\"Robot Trajectory\")\n",
    "\n",
    "ax.set_xlabel(\"Distance to Obstacle [x 3m]\")\n",
    "ax.set_ylabel(\"Distance to Target [x 3m]\")\n",
    "ax.set_zlabel(\"Distance to Ground [x 3m]\")\n",
    "ax.set_title(\"Estimated End-Effector 3D States Over Time\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18242a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable interactive plots in Jupyter Notebook\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,\n",
    "                     projection=\"3d\")\n",
    "\n",
    "plot_3D_vector(env=env,\n",
    "               ax=ax,\n",
    "               list_trajectory=action_tensor_list,\n",
    "               color=\"red\",\n",
    "               traj_s=20,\n",
    "               start_s=120,\n",
    "               end_s=85,\n",
    "               label=\"Robot Trajectory\")\n",
    "\n",
    "ax.set_xlabel(\"Normalized X-Axis Location [x 2m]\")\n",
    "ax.set_ylabel(\"Normalized Y-Axis Location [x 2m]\")\n",
    "ax.set_zlabel(\"Normalized Z-Axis Location [x 2m]\")\n",
    "ax.set_title(\"Estimated End-Effector 3D Positions Over Time\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6961613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [reward_tensor[0].item() for reward_tensor in reward_tensor_list]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(rewards, c=\"darkblue\")\n",
    "plt.xlabel(\"State Index\")\n",
    "plt.ylabel(\"Reward Prediction\")\n",
    "plt.title(\"Predicted Reward Value During Robot Trajectory Over Time\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a4ac1",
   "metadata": {},
   "source": [
    "## Reset the Model Parameters and Examine the Output of Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f60f9705",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_policy_network = RobotPolicy(state_size=configs.state_size,\n",
    "                                   hidden_size=configs.hidden_size,\n",
    "                                   out_size=configs.action_size,\n",
    "                                   log_std_min=configs.policy_log_std_min,\n",
    "                                   log_std_max=configs.policy_log_std_max,\n",
    "                                   device=configs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1018b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlearned_robot_trajectory = [functions.generate_session(env=env,\n",
    "                                                         t_max=constants.TRAJECTORY_SIZE,\n",
    "                                                         updater_obj=updater_obj,\n",
    "                                                         replay_buffer=replay_buffer,\n",
    "                                                         policy_network=dummy_policy_network,\n",
    "                                                         is_policy_inference=True,\n",
    "                                                         is_policy_gradient_update=False,\n",
    "                                                         is_deterministic=True) for _ in range(episodes_to_run)][0]\n",
    "dummy_state_tensor_list = unlearned_robot_trajectory[0]\n",
    "dummy_action_tensor_list = unlearned_robot_trajectory[1]\n",
    "dummy_reward_tensor_list = unlearned_robot_trajectory[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8301ac3",
   "metadata": {},
   "source": [
    "## Visualize Randomly Initialized State Trajectory Execution of Not Trained Policy (Dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265079db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable interactive plots in Jupyter Notebook\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,\n",
    "                     projection=\"3d\")\n",
    "\n",
    "plot_3D_vector(ax=ax,\n",
    "               list_trajectory=dummy_state_tensor_list,\n",
    "               color=\"black\",\n",
    "               traj_s=20,\n",
    "               start_s=120,\n",
    "               end_s=85,\n",
    "               label=\"Robot Trajectory\")\n",
    "\n",
    "ax.set_xlabel(\"Distance to Obstacle [x 3m]\")\n",
    "ax.set_ylabel(\"Distance to Target [x 3m]\")\n",
    "ax.set_zlabel(\"Distance to Ground [x 3m]\")\n",
    "ax.set_title(\"Dummy Robot Estimated End-Effector 3D States Over Time\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e414ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable interactive plots in Jupyter Notebook\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,\n",
    "                     projection=\"3d\")\n",
    "\n",
    "plot_3D_vector(env=env,\n",
    "               ax=ax,\n",
    "               list_trajectory=dummy_action_tensor_list,\n",
    "               color=\"red\",\n",
    "               traj_s=20,\n",
    "               start_s=120,\n",
    "               end_s=85,\n",
    "               label=\"Robot Trajectory\")\n",
    "\n",
    "ax.set_xlabel(\"Normalized X-Axis Location [x 2m]\")\n",
    "ax.set_ylabel(\"Normalized Y-Axis Location [x 2m]\")\n",
    "ax.set_zlabel(\"Normalized Z-Axis Location [x 2m]\")\n",
    "ax.set_title(\"Dummy Robot Estimated End-Effector 3D Positions Over Time\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a64e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_rewards = [dummy_reward_tensor[0].item() for dummy_reward_tensor in dummy_reward_tensor_list]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(dummy_rewards, c=\"darkblue\")\n",
    "plt.xlabel(\"State Index\")\n",
    "plt.ylabel(\"Reward Prediction\")\n",
    "plt.title(\"Predicted Reward Value During Dummy Robot Trajectory Over Time\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc1828b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
