{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d1d047",
   "metadata": {},
   "source": [
    "# Run Beta Distribution Based Trust Model Along with Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bd1b79",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d5db43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from scipy import stats\n",
    "from IPython import display\n",
    "\n",
    "# get the current script's directory\n",
    "current_directory = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in locals() else os.getcwd()\n",
    "# get the parent directory\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "# add the parent directory to the sys.path at the beginning\n",
    "sys.path.insert(0, parent_directory)\n",
    "\n",
    "from utils import constants\n",
    "\n",
    "from optimization import functions\n",
    "from optimization.updater import Updater\n",
    "\n",
    "from environment.environment import RobotEnvironment\n",
    "from environment.buffer import ReplayBuffer\n",
    "\n",
    "from models.policy_model import RobotPolicy\n",
    "from models.reward_model import RewardFunction\n",
    "\n",
    "from trusts.model_dynamics import TrustDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9194c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",\n",
    "              None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c091da",
   "metadata": {},
   "source": [
    "## Functions for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "869a0bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_beta_distribution(axi: plt,\n",
    "                           distribution_object: TrustDistribution,\n",
    "                           title_label: str) -> None:\n",
    "    \n",
    "    # enable interactive plots in Jupyter Notebook\n",
    "    %matplotlib notebook\n",
    "    \n",
    "    if not isinstance(distribution_object, TrustDistribution):\n",
    "        raise ValueError(\"Input 'distribution_object' must be an instance of TrustDistribution class.\")\n",
    "    if not isinstance(title_label, str):\n",
    "        raise ValueError(\"Input 'title_label' must be a string.\")\n",
    "    \n",
    "    # generate random samples from the beta distribution\n",
    "    sample_size = 1000\n",
    "    samples = stats.beta.rvs(distribution_object.alpha,\n",
    "                             distribution_object.beta,\n",
    "                             size=sample_size)\n",
    "\n",
    "    # plot the histogram of the generated samples\n",
    "    axi.hist(samples,\n",
    "             bins=30,\n",
    "             density=True,\n",
    "             alpha=0.5,\n",
    "             color=\"darkblue\",\n",
    "             label=\"Human Trust Model Histogram\")\n",
    "\n",
    "    # plot the probability density function (PDF) of the beta distribution\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    pdf = stats.beta.pdf(x,\n",
    "                         distribution_object.alpha,\n",
    "                         distribution_object.beta)\n",
    "    axi.plot(x,\n",
    "             pdf,\n",
    "             color=\"darkred\",\n",
    "             linestyle=\"-\",\n",
    "             linewidth=2,\n",
    "             label=\"Probability Density Function\")\n",
    "\n",
    "    # plot the mean value as a horizontal line\n",
    "    mean_value = distribution_object.get_beta_distribution_mean()\n",
    "    axi.axvline(x=mean_value,\n",
    "                color=\"darkgreen\",\n",
    "                linestyle=\"--\",\n",
    "                linewidth=2,\n",
    "                label=\"Mean Value [Trust Prediction]\")\n",
    "    \n",
    "    axi.text(mean_value - 0.03,\n",
    "             0.03,\n",
    "             f\"Trust Prediction: %{100 * mean_value:.2f}\",\n",
    "             color=\"darkgreen\",\n",
    "             rotation=90)\n",
    "\n",
    "    axi.set_title(title_label)\n",
    "    axi.set_xlabel(\"Probabilistic Trust Distribution Estimation\")\n",
    "    axi.set_ylabel(\"Probability Density\")\n",
    "    axi.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14e56cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dynamic_trust_reward(performance_array_list: list,\n",
    "                              trust_obj: TrustDistribution) -> None:\n",
    "    \n",
    "    %matplotlib inline\n",
    "\n",
    "    # enable interactive mode\n",
    "    plt.ion()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    for step_num in range(1, len(performance_array_list) + 1):\n",
    "\n",
    "        try:\n",
    "            performance = performance_array_list[step_num - 1]\n",
    "\n",
    "            # update distribution parameters\n",
    "            trust_obj.update_parameters(performance)\n",
    "\n",
    "            title_label = f\"Trust Modeling via Beta Distribution (step={step_num}; alpha={trust_obj.alpha:.2f}; beta={trust_obj.beta:.2f})\"\n",
    "\n",
    "            ax1.clear()\n",
    "            plot_beta_distribution(axi=ax1,\n",
    "                                   distribution_object=trust_obj,\n",
    "                                   title_label=title_label)\n",
    "\n",
    "            ax2.clear()\n",
    "            ax2.plot(performance_array_list[: step_num - 1],\n",
    "                     color=\"darkorange\",\n",
    "                     linestyle=\"-\",\n",
    "                     linewidth=2,\n",
    "                     label=\"Performance (Reward) Values\")\n",
    "\n",
    "            ax2.set_xlabel(\"Time Steps [#]\")\n",
    "            ax2.set_ylabel(\"Performance Value [from -1 to 1]\")\n",
    "            ax2.grid()\n",
    "            ax2.legend()\n",
    "\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "            if step_num != len(performance_array_list):\n",
    "                display.clear_output(wait=True)\n",
    "                time.sleep(0.5)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "    # turn off interactive mode\n",
    "    plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca917b41",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f35961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# available training machine\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Training Device: \", device)\n",
    "\n",
    "configs = functions.setup_config(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1313cfa1",
   "metadata": {},
   "source": [
    "## Create and Load Pre-Trained Reward and Policy Models and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8ef4779",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_network = RobotPolicy(state_size=configs.state_size,\n",
    "                             hidden_size=configs.hidden_size,\n",
    "                             out_size=configs.action_size,\n",
    "                             log_std_min=configs.policy_log_std_min,\n",
    "                             log_std_max=configs.policy_log_std_max,\n",
    "                             device=configs.device)\n",
    "reward_network = RewardFunction(state_size=configs.state_size,\n",
    "                                action_size=configs.action_size,\n",
    "                                hidden_size=configs.hidden_size,\n",
    "                                out_size=configs.reward_size,\n",
    "                                device=configs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e43bda6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and return preliminary base paths\n",
    "_, results_path = functions.get_directories(parent_directory=parent_directory,\n",
    "                                            data_folder_name=constants.TEST_COLLECTION_DATE)\n",
    "\n",
    "# folder name where policy model parameters are located (\"results / policy_network_params / loading_folder_name\")\n",
    "policy_loading_folder_name = constants.POLICY_LOADING_FOLDER\n",
    "policy_params_name = constants.POLICY_PARAMS_NAME\n",
    "\n",
    "# folder name where reward model parameters are located (\"results / reward_network_params / loading_folder_name\")\n",
    "reward_loading_folder_name = constants.REWARD_LOADING_FOLDER\n",
    "reward_params_name = constants.REWARD_PARAMS_NAME\n",
    "\n",
    "# load pretrained policy network parameters if the pre-trained model is available\n",
    "policy_network = functions.load_policy_from_path(policy_network=policy_network,\n",
    "                                                 results_path=results_path,\n",
    "                                                 policy_loading_folder_name=policy_loading_folder_name,\n",
    "                                                 policy_params_name=policy_params_name)\n",
    "\n",
    "# load pretrained reward network parameters if the pre-trained model is available\n",
    "reward_network = functions.load_reward_from_path(reward_network=reward_network,\n",
    "                                                 results_path=results_path,\n",
    "                                                 reward_loading_folder_name=reward_loading_folder_name,\n",
    "                                                 reward_params_name=reward_params_name)\n",
    "\n",
    "# model loss functions\n",
    "updater_obj = Updater(configs=configs,\n",
    "                      policy_network=policy_network,\n",
    "                      reward_network=reward_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa3424c",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb0e9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct custom environment for reward function training\n",
    "env = RobotEnvironment()\n",
    "env.set_reward_network(reward_network)\n",
    "env.is_reward_inference = True\n",
    "\n",
    "# create a replay buffer class object\n",
    "replay_buffer = ReplayBuffer(capacity=configs.replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c31789",
   "metadata": {},
   "source": [
    "## Create beta Distribution Object to Resembe Trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e811791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trust_obj = TrustDistribution(initial_alpha=configs.initial_alpha,\n",
    "                              initial_beta=configs.initial_beta,\n",
    "                              initial_w_success=configs.initial_w_success,\n",
    "                              initial_w_failure=configs.initial_w_failure,\n",
    "                              gamma=configs.gamma,\n",
    "                              epsilon_reward=configs.epsilon_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdacbc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_beta_distribution(axi=ax,\n",
    "                       distribution_object=trust_obj,\n",
    "                       title_label=\"Initial Human Trust Modeling via Beta Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403b824f",
   "metadata": {},
   "source": [
    "## Simulation Environment by Running Trained Policy and Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b76698ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_to_run = 10\n",
    "\n",
    "# create episodes of trajectories by running trained policy model\n",
    "# take deterministic actions for behavior cloning by setting is_deterministic=True to use mean actions from distribution\n",
    "robot_trajectories = [functions.generate_session(env=env,\n",
    "                                                 t_max=constants.TRAJECTORY_SIZE,\n",
    "                                                 updater_obj=updater_obj,\n",
    "                                                 replay_buffer=replay_buffer,\n",
    "                                                 policy_network=policy_network,\n",
    "                                                 is_policy_inference=True,\n",
    "                                                 is_policy_gradient_update=False,\n",
    "                                                 is_deterministic=True) for _ in range(episodes_to_run)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "044c55c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose one randomly initialized state robot trajectory to analyze\n",
    "robot_trajectory = robot_trajectories[-1]\n",
    "\n",
    "state_array_list = np.array([state.numpy() for state in robot_trajectory[0]])\n",
    "action_array_list = np.array([action.numpy() for action in robot_trajectory[1]])\n",
    "reward_array_list = np.array([reward.item() for reward in robot_trajectory[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a20e9ed",
   "metadata": {},
   "source": [
    "## Dynamically Update Estimated Trust Distribution while Executing Robot Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75622f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dynamic_trust_reward(performance_array_list=reward_array_list,\n",
    "                          trust_obj=trust_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d96abb",
   "metadata": {},
   "source": [
    "## Load Dummy (Not Optimized) Policy and Visualize Trust Estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b2bbe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_policy_network = RobotPolicy(state_size=configs.state_size,\n",
    "                                   hidden_size=configs.hidden_size,\n",
    "                                   out_size=configs.action_size,\n",
    "                                   log_std_min=configs.policy_log_std_min,\n",
    "                                   log_std_max=configs.policy_log_std_max,\n",
    "                                   device=configs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "729fb0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlearned_robot_trajectory = [functions.generate_session(env=env,\n",
    "                                                         t_max=constants.TRAJECTORY_SIZE,\n",
    "                                                         updater_obj=updater_obj,\n",
    "                                                         replay_buffer=replay_buffer,\n",
    "                                                         policy_network=dummy_policy_network,\n",
    "                                                         is_policy_inference=True,\n",
    "                                                         is_policy_gradient_update=False,\n",
    "                                                         is_deterministic=True) for _ in range(episodes_to_run)][0]\n",
    "\n",
    "dummy_state_array_list = np.array([state.numpy() for state in unlearned_robot_trajectory[0]])\n",
    "dummy_action_array_list = np.array([action.numpy() for action in unlearned_robot_trajectory[1]])\n",
    "dummy_reward_array_list = np.array([reward.item() for reward in unlearned_robot_trajectory[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5121dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dynamic_trust_reward(performance_array_list=dummy_reward_array_list,\n",
    "                          trust_obj=trust_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda7f75",
   "metadata": {},
   "source": [
    "## Reset Human Trust Estimation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f28398ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "trust_obj = TrustDistribution(initial_alpha=configs.initial_alpha,\n",
    "                              initial_beta=configs.initial_beta,\n",
    "                              initial_w_success=configs.initial_w_success,\n",
    "                              initial_w_failure=configs.initial_w_failure,\n",
    "                              gamma=configs.gamma,\n",
    "                              epsilon_reward=configs.epsilon_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b1030",
   "metadata": {},
   "source": [
    "## Load Early Stages of Training Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034e0552",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_policy_network = RobotPolicy(state_size=configs.state_size,\n",
    "                                   hidden_size=configs.hidden_size,\n",
    "                                   out_size=configs.action_size,\n",
    "                                   log_std_min=configs.policy_log_std_min,\n",
    "                                   log_std_max=configs.policy_log_std_max,\n",
    "                                   device=configs.device)\n",
    "\n",
    "# load pretrained policy network parameters if the pre-trained model is available (early stages of the training)\n",
    "early_policy_network = functions.load_policy_from_path(policy_network=early_policy_network,\n",
    "                                                       results_path=results_path,\n",
    "                                                       policy_loading_folder_name=\"Feb_21_2024-22_47_07\",\n",
    "                                                       policy_params_name=\"policy_network_epoch_21_loss_101_83953348795573.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3d58e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "lesslearned_robot_trajectory = [functions.generate_session(env=env,\n",
    "                                                           t_max=constants.TRAJECTORY_SIZE,\n",
    "                                                           updater_obj=updater_obj,\n",
    "                                                           replay_buffer=replay_buffer,\n",
    "                                                           policy_network=early_policy_network,\n",
    "                                                           is_policy_inference=True,\n",
    "                                                           is_policy_gradient_update=False,\n",
    "                                                           is_deterministic=True) for _ in range(episodes_to_run)][0]\n",
    "\n",
    "early_state_array_list = np.array([state.numpy() for state in lesslearned_robot_trajectory[0]])\n",
    "early_action_array_list = np.array([action.numpy() for action in lesslearned_robot_trajectory[1]])\n",
    "early_reward_array_list = np.array([reward.item() for reward in lesslearned_robot_trajectory[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960d9b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dynamic_trust_reward(performance_array_list=early_reward_array_list,\n",
    "                          trust_obj=trust_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40703e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
